---
layout: post
title: "ğŸ“ƒ Are Sixteen Heads Really Better than One? ë¦¬ë·°"
tags:
  - paper
  - nlp
---

Multi head attentionì´ í‘œí˜„ë ¥ì´ ì¢‹ê³  ë§ì€ ì •ë³´ë¥¼ ë‹´ì„ ìˆ˜ ìˆë‹¤ì§€ë§Œ, ëª¨ë“  headê°€ í•„ìš”í•œ ê²ƒì€ ì•„ë‹ˆë‹¤. ì´ì— ê´€í•œ ë…¼ë¬¸ì´ Are Sixteen Heads Really Better Than One? (Michel et al., 2019)ì´ê³ , arxiv ë§í¬ëŠ” [https://arxiv.org/abs/1905.10650](https://arxiv.org/abs/1905.10650)ì´ë‹¤.

## Abstract

* MultiHeadë¡œ í•™ìŠµì´ ë˜ì—ˆë”ë¼ë„ Test Timeì—ëŠ” ë§ì€ headë¥¼ ì œê±°í•´ë„ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì¡´í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•¨.
* íŠ¹íˆ ëª‡ëª‡ ë ˆì´ì–´ëŠ” single headì—¬ë„ ì„±ëŠ¥í•˜ë½ì´ ì—†ì—ˆë‹¤.

## 1. Introduction

* greedy í•˜ê³  iterativeí•œ attention head pruning ë°©ë²• ì œì‹œ
* inference timeì„ 17.5% ë†’ì˜€ë‹¤.
* MTëŠ” pruningì— íŠ¹íˆ ë¯¼ê°í–ˆëŠ”ë°, ì´ë¥¼ ìì„¸íˆ ì‚´í´ë´„

## 2. Background: Attention, Multi-headed Attention, and Masking

* ê±°ì˜ ë‹¤ íŒ¨ìŠ¤
* Multi Head Attention Maskingí•˜ëŠ” ê²ƒì€ mask variableë¡œ ê³„ì‚°í•¨
* íŠ¹ì • headì˜ ê²°ê³¼ê°’ì„ 0ìœ¼ë¡œ ì§€ì •

## 3. Are All Attention Heads Important?

* WMTì—ì„œ í…ŒìŠ¤íŠ¸

### 3.2. Ablating One Head

* í•˜ë‚˜ì˜ Headë§Œ ì œê±°í•˜ëŠ” í…ŒìŠ¤íŠ¸

{%include image.html url="/images/2020-05-13-sixteen-heads/fig1.png" class='noshadow' %}


### 3.3. Ablating All Heads but One

### 3.4. Are Important Heads the Same Across Datasets?

## 4. Iterative Pruning of Attention Heads

### 4.1. Head Importance Score for Pruning

### 4.2. Effect of Pruning on BLEU/Accuracy

### 4.3. Effect of Pruning on Efficiency

## 5. When Are More Heads Important? The Case of Machine Translation

## 6. Dynamics of Head Importance during Training

## 7. Related work

## 8. Conclusion
