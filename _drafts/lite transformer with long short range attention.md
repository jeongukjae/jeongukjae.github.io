---
layout: post
title: "ğŸ“ƒ Lite Transformer with Long-Short Range Attention ë¦¬ë·°"
tags:
  - paper
---

ICLR 2020 ë³´ë©´ì„œ ì œì¼ ì¬ë°Œì—ˆë˜ ë…¼ë¬¸ ëª‡í¸ë„ ì•ìœ¼ë¡œ ëª‡ì¼ê°„ ë¦¬ë·°ë¥¼ ì˜¬ë ¤ë³´ë„ë¡ í•˜ê² ë‹¤. ì´ ë…¼ë¬¸ì€ í•œì¤„ë¡œ ë§í•˜ìë©´ transformerì˜ ì—°ì‚°ì„ (íŠ¹íˆ FFN + Attentionì„) ê°„ë‹¨í•˜ê²Œ ë§Œë“¤ì–´ë³´ëŠ” ë…¼ë¬¸ì´ë‹¤. ë…¼ë¬¸/ìŠ¬ë¼ì´ë“œ/ë°œí‘œ ì˜ìƒì€ [https://iclr.cc/virtual_2020/poster_ByeMPlHKPH.html](https://iclr.cc/virtual_2020/poster_ByeMPlHKPH.html)ì—ì„œ ë³¼ ìˆ˜ ìˆë‹¤.

## Abstract

* Key Primitive: Long Short Range Attention (LSRA)
  1. Convolution Layer -> Local Context Modeling
  2. Atttention -> Long Distance Modeling
* machine translation, abstractive summarization, language modelingì—ì„œ improvementë¥¼ ë³´ì—¬ì¤Œ
  * NMTì—ì„œëŠ” 0.3 BLEU Score degradationì„ ìˆ˜ìš©í•  ê²½ìš° 2.5xê¹Œì§€ computationì„ ì¤„ì¼ ìˆ˜ ìˆìŒ
  * Quantization + Pruningê¹Œì§€ ì ìš©í•  ê²½ìš° 18.2xê¹Œì§€ ì••ì¶•ê°€ëŠ¥
  * LMì€ 500MACs(Mul + Add) ê·¼ì²˜ì—ì„œ 1.8ì •ë„ perplexity ë‚®ì•„ì§

## 1 Introduction

* Mobile NLPì— ì ìš©ê°€ëŠ¥í•œ ëª¨ë¸
* Main Contribution
  1. Transformerì˜ ì—°ì‚° bottleneckì„ ì°¾ê³  í•´ê²°
  2. ê¸°ì¡´ transformerë¥¼ multi brnach feature extractorë¥¼ ì‚¬ìš©í•´ ìˆ˜ì •í•¨
  3. mobile computation resource constraintsì—ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ
  4. AutoML ë² ì•„ìŠ¤ì¸ Evolved Transformerì™€ë„ ë¹„êµí•´ë³´ì•˜ë”ë‹ˆ 0.5ì •ë„ ë†’ì€ BLEU scoreë¥¼ ê°€ì ¸ê°€ë©´ì„œ 20,000x ì •ë„ì˜ CO2 ë°°ì¶œëŸ‰ì„ ì ˆì•½í•¨
* main contribution-4ëŠ” AutoMLì´ë¼ ê·¸ëŸ°ê±° ì•„ë‹Œê°€...?

## 2 Related Work

íŒ¨ìŠ¤

## 3 Is bottelneck effective for 1-D attention?
