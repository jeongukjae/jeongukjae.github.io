---
layout: post
title: "ğŸ“ƒ DynaBERT: Dynamic BERT with Adaptive Width and Depth ë¦¬ë·°"
tags:
  - paper
  - nlp
---

ì´ ë…¼ë¬¸ì—ì„œëŠ” BERT, RoBERTaê°€ ë§¤ìš° ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, memory, computing powerê°€ ë„ˆë¬´ ë§ì´ í•„ìš”í•˜ë¯€ë¡œ ê·¸ë¥¼ ì••ì¶•í•´ë³´ëŠ” ë°©ë²•ì„ ì œì•ˆí•œë‹¤. ì•„ì§ WIPì¸ ë…¼ë¬¸ì´ê³ , [https://arxiv.org/abs/2004.04037](https://arxiv.org/abs/2004.04037)ê°€ ë§í¬ì´ë‹¤.

## Abstract

* dynamic BERT Model ì œì•ˆ, width, depth ë°©í–¥ìœ¼ë¡œ dynamicí•¨
* Knowledge Distillation ë°©ì‹ìœ¼ë¡œ full BERT ëª¨ë¸ì„ width adaptive BERTë¡œ í•™ìŠµí•œ ë’¤, width, depth ëª¨ë‘ adaptiveí•˜ê²Œ í•™ìŠµí•¨

## 1. Introduction

* ê¸°ì¡´ì˜ Transformer-based modelì„ ì••ì¶•í•˜ê±°ë‚˜, ì¶”ë¡  ê°€ì†í™”ë¥¼ ì‹œë„í•œ ë°©ë²•ë¡ ë“¤:
  * low-rank approximation
    * [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
    * [A Tensorized Transformer for Language Modeling](https://arxiv.org/abs/1906.09777)
  * weight sharing
    * [Universal transformers](https://arxiv.org/abs/1807.03819)
    * [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
  * knowledge distillation
    * [Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)
    * [Patient knowledge distillation for bert model compression](https://arxiv.org/abs/1908.09355)
    * [Tinybert: Distilling bert for natural language understanding](https://arxiv.org/abs/1909.10351)
  * quantization
    * [Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model](https://arxiv.org/abs/1906.00532)
    * [Q8bert: Quantized 8bit bert](https://arxiv.org/abs/1910.06188)
    * [Q-bert: Hessian based ultra low precision quantization of bert](https://arxiv.org/abs/1909.05840)
  * pruning
    * [Pruning Convolutional Neural Networks for Resource Efficient Inference](https://arxiv.org/abs/1611.06440)
    * [Pruning a BERT-based Question Answering Model](https://deepai.org/publication/pruning-a-bert-based-question-answering-model)
    * [Are sixteen heads really better than one?](https://arxiv.org/abs/1905.10650)
    * [Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned](https://arxiv.org/abs/1905.09418)
    * [Fine-tune BERT with sparse self-attention mechanism](https://www.semanticscholar.org/paper/Fine-tune-BERT-with-Sparse-Self-Attention-Mechanism-Cui-Li/a3ef6ee560e93e6f58be2b28f27aed0eb86dc463)
* ëª‡ëª‡ ë¦¬ì„œì¹˜ì—ì„œ depth adaptive modelsë„ ì¶©ë¶„í•œ ì˜ë¯¸ê°€ ìˆìŒì„ ì¦ëª…
* ìµœê·¼ì˜ ë¦¬ì„œì¹˜ëŠ” width directionë„ ì¶©ë¶„íˆ redundantí•¨ì„ ë§í•˜ê³  ìˆìŒ
  * ex> Attention Headë¥¼ pruningí•´ë„ ì¶©ë¶„íˆ ì„±ëŠ¥ì´ ì¢‹ìŒ
* CNNì—ì„œ width, depth - adaptiveí•˜ê²Œ ëª¨ë¸ì„ ë§Œë“¤ì–´ë‚¸ ì‹œë„ê°€ ìˆì—ˆì§€ë§Œ, BERTì— ì ìš©í•˜ê¸´ í˜ë“¤ë‹¤.
  * Transformer ë ˆì´ì–´ ì•ˆì˜ Multi Head Attentionê³¼ Position wise Feed Forward Network ë•Œë¬¸
* Training ë°©ë²•
  * width adaptive BERT í•™ìŠµ : attention headsë‘ neuron ì¤‘ ì¤‘ìš”í•œ ê²ƒë“¤ë§Œ rewireí•œ ë’¤ distillation ì§„í–‰
  * adaptive BERT í•™ìŠµ : width adaptive BERTì—ì„œ initializeí•œ ë’¤ì— width, depth ë‘˜ ë‹¤ distillation

## 2. Related Work

### 2.1. Transformer Layer

ì´ê±°ëŠ” ê·¸ëƒ¥ Transformer ì„¤ëª…ì„

### 2.2. Compression for Transformer/BERT

* Low Rank Approximation
  * weight matrixë¥¼ ë‘ lower rank matrixì˜ ê³±ìœ¼ë¡œ ê·¼ì‚¬í•œë‹¤.
  * ALBERTëŠ” embedding layerë¥¼ ê·¼ì‚¬
  * Tensorized TransformerëŠ” MHA ê²°ê³¼ê°€ orthonormal base vectorsë¡œ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤ê³  í•¨ + multi-linear attention ì‚¬ìš©
* weight sharing
  * Universal TransformerëŠ” layerê°„ weight sharing
  * Deep Equilibrium Modelì€ íŠ¹ì • ë ˆì´ì–´ì˜ input, outputì´ ê°™ì•„ì§€ê²Œ í•¨ -> ??? ëª¨ë¥´ê² ë‹¤ ì°¾ì•„ë³´ì
  * ALBERTëŠ” ë ˆì´ì–´ê°„ parameter sharingì´ network parameterë¥¼ ì•ˆì •ì ìœ¼ë¡œ ë§Œë“¤ê²Œ í•´ì£¼ê³  ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ëŠ”ë‹¤ê³  í•¨
  * ê·¼ë° model sizeëŠ” ì¤„ì–´ë„ inferenceëŠ” ì•ˆë¹ ë¦„
* Distillation
  * DistilBertëŠ” soft logitì´ë‘ hidden states distillation ì‹œí‚´
  * BERT PKDëŠ” intermediate layerì— ë¡œìŠ¤ ì¤Œ
  * Tiny BERTëŠ” general distillation, task-specific distillationìœ¼ë¡œ ë‚˜ëˆ ì„œ ì§„í–‰í•¨
* Quantizaiton
  * QBERTëŠ” second order informationì„ í™œìš©í•´ ê° ë ˆì´ì–´ë³„ë¡œ ëª‡ ë¹„íŠ¸ë¥¼ í• ë‹¹í•  ì§€ ì •í•¨
    * steeper curvatureì—ëŠ” ë” ë§ì€ bit í• ë‹¹
  * Fully Quantized TransformerëŠ” uniform min max quantizationì„ ì”€
  * Q8BERTëŠ” quantization aware training + symmetric 8 bit linear quantizatio í™œìš©í•¨
* Pruing
  * "Fine-tune BERT with sparse self-attention mechanism"ì´ë€ ë…¼ë¬¸ì—ì„œ sparse self attentionì„ ì‚¬ìš©
  * "Compressing bert: Studying the effects of weight pruning on transfer learning"ì´ë€ ë…¼ë¬¸ì—ì„œ magnitude-based pruning ì‚¬ìš©
  * LayerDropì—ì„œëŠ” transformer layerë“¤ì˜ ì¶”ë¡ ì„ ìœ„í•´ structed dropoutì„ ì ìš©í•¨
* ê·¼ë° ì´ ë°©ë²•ë“¤ ëŒ€ë¶€ë¶„ì´ ì••ì¶•ê³¼ ê´€ë ¨ëœ ê±°ê³  Universal Transformerë‚˜ LayerDrop, Depth-adaptive transformerë„ ì••ì¶•ì´ë‘ ê°€ì†ì— ì‹ ê²½ì“°ê¸°ëŠ” í•˜ë‚˜ depth directionë¿ì´ë‹¤.

## 3. Method

### 3.1. Training DynaBERT_w with Adaptive Width

* CNNê³¼ ë¹„êµí•´ BERTëŠ” Transformers Layerê°€ ìŒ“ì—¬ìˆëŠ” í˜•íƒœë¼ ë” ë³µì¡
* MHAì—ëŠ” linear transformationê³¼  key, query, valueì˜ ê³±ì´ ì¡´ì¬í•¨.

### 3.1.1. Using Attention heads and Intermediate Neurons in FFN to Adapt the Width

* MHAë¥¼ ê° Attention ì—°ì‚°ìœ¼ë¡œ ë¶„ë¦¬í•œ ë‹¤ìŒ ì¤‘ìš”í•œ attention headsë§Œì„ ì·¨í•œë‹¤.
* ê°€ì¥ ì¤‘ìš”í•œ ìˆœìœ¼ë¡œ Headì™€ Neuronì„ ì™¼ìª½ìœ¼ë¡œ ëª°ì•„ë„£ëŠ”ë‹¤.

### 3.1.2. Network Rewiring

* "Pruning convolutional neural networks for resource efficient inference"ì™€ "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned" ì— ë”°ë¼ì„œ importance scoreë¥¼ êµ¬í•¨.
* ê·¸ëŸ° ë‹¤ìŒ ì•„ë˜ì²˜ëŸ¼ ì¬êµ¬ì„±í•¨

{% include image.html url="/images/dynabert/fig1.png" %}

### 3.1.3. Training with Adaptive Width
