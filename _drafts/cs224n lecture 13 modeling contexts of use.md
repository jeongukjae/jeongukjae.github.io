---
layout: post
title: "ğŸ“• CS224n Lecture 13 Modeling contexts of use: Contextual Representations and Pretraining"
tags:
  - nlp
  - cs224n
  - machine learning
---

13ê°• ì •ë¦¬! 11ê°•ë¶€í„°ì¸ê°€? ê·¸ë•Œë¶€í„° ëŒ€ë¶€ë¶„ ì†Œê°œê°€ ë˜ì–´ê°€ê³  ìˆì–´ì„œ ì¢‹ì€ ë§í¬ ì •ë¦¬ ì •ë„ë§Œ í•˜ê³  ìˆëŠ” ê²ƒ ê°™ë‹¤.

## Reflections on word representation

ì§€ê¸ˆê¹Œì§€ëŠ” word embeddingì„ ì‹œì‘ë¶€í„° í–ˆëŠ”ë°, pretrained modelë¥¼ ì‚¬ìš©í•˜ìëŠ” ë§. ê·¸ ì´ìœ ëŠ” ë” ë§ì€ ë‹¨ì–´ì™€ ë” ë§ì€ ë°ì´í„°ì— ëŒ€í•´ í•™ìŠµì´ ê°€ëŠ¥í•´ì§„ë‹¤ëŠ” ì´ìœ ì´ë‹¤. ì‹¤ì œë¡œ ì„±ëŠ¥ë„ ë” ë†’ì€ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.

ê·¼ë° unknown wordsì— ëŒ€í•´ì„œëŠ” ì–´ë–»ê²Œ ëŒ€ì‘í•  ê²ƒì¸ê°€? `UNK`ìœ¼ë¡œ ë§¤í•‘í•´ì„œ ì–´ì©Œêµ¬ì €ì©Œêµ¬ë¥¼ í•˜ì§€ë§Œ ê²°ë¡ ì€ char-level modelì„ ì‚¬ìš©í•˜ì! ë˜ëŠ” í…ŒìŠ¤íŠ¸ë•Œ `<UNK>`ê°€ unsupervised word embeddingì— ì¡´ì¬í•œë‹¤ë©´ ê·¸ê±¸ ê³„ì† ì“°ê³ , ê·¸ëƒ¥ ì•„ì˜ˆ ëª¨ë¥´ëŠ” ê²ƒì€ random vectorë¡œ ë§Œë“ ë‹¤ìŒì— vocabì— ì¶”ê°€í•˜ëŠ” ë°©ë²•ë„ ê³ ë ¤í•´ë³´ë¼ê³  í•œë‹¤. [^Dhingra2017]

[^Dhingra2017]: [A Comparative Study of Word Embeddings for Reading Comprehension](https://arxiv.org/abs/1703.00993) í—¤ë”© ë…¼ë¬¸

ì–´ì°Œë˜ì—ˆë“  word embeddingì„ ì‹œì‘ë¶€í„° í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì€ ë‘ê°€ì§€ í° ë¬¸ì œê°€ ìˆëŠ”ë°, í•˜ë‚˜ì˜ ë‹¨ì–´ì— ëŒ€í•´ context ìƒê´€ì—†ì´ ë‹¤ ê°™ì€ representationì„ ê°€ì ¸ì˜¨ë‹¤ëŠ” ì ì´ë‹¤.

ì´ ë¬¸ì œì— ëŒ€í•œ í•´ê²°ë²•ì€ ë¬´ì—‡ì¼ê¹Œ? NLMì—ì„œ LSTM Layerë“¤ì€ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ ëª¨ë¸ì´ë‹¤. ê·¸ ë§ì€ context-specific word representationì„ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” ë§ì´ ì•„ë‹ê¹Œ?

{% include image.html url="/images/cs224n/13-1.png" description="LSTM Layer in NLM" %}

## Pre-ELMo and ELMO

TagLM[^TagLM]ì´ë€ ë…¼ë¬¸ì€ ELMoê°€ ë‚˜ì˜¤ê¸° ì „ì— ë‚˜ì˜¨ ë…¼ë¬¸ì´ë‹¤. ë©”ì¸ ì•„ì´ë””ì–´ëŠ” word representationì„ contextì•ˆì—ì„œ í•´ë‚´ê³  ì‹¶ì§€ë§Œ, ê·¸ë ‡ê²Œ ê¸°ì¡´ì˜ í•™ìŠµë°©ì‹ê³¼ ë‹¤ë¥´ì§€ ì•Šê²Œ í•˜ê³  ì‹¶ë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ semi-supervised approach ë°©ì‹ì„ ì°¨ìš©í•˜ì˜€ë‹¤. ì´ê²Œ Pre-ELMo

[^TagLM]: [Semi-supervised sequence tagging with bidirectional language models](https://arxiv.org/abs/1705.00108) taglm ë…¼ë¬¸

{% include image.html url="/images/cs224n/13-2.png" description="TagLM" %}

[CoVe](https://arxiv.org/pdf/1708.00107.pdf)ë¼ëŠ” ëª¨ë¸ë„ ìˆì—ˆëŠ”ë°, ì´ê±´ ê·¸ëƒ¥ ê°•ì˜ì—ì„œ ë„˜ì–´ê°

[ELMO](https://arxiv.org/abs/1802.05365)ëŠ” Deep Contextualized word representationsë¼ëŠ” ì œëª©ì„ ê°€ì§„ ë…¼ë¬¸ì˜ ëª¨ë¸ì´ë‹¤. word token vectorì™€ contextual word vectorì˜ breakout versionì´ë‹¤. word token vectorë¥¼ long contextë¡œë¶€í„° ë°°ìš´ë‹¤. (ë‹¤ë¥¸ ëª¨ë¸ë“¤ì€ fixed window contextë¡œë¶€í„° ë°°ìš°ë‚˜..?)

bi-drectional LMì„ ì‚¬ìš©í•˜ì§€ë§Œ, ì„±ëŠ¥ë•Œë¬¸ì— ì´ìƒí•  ì •ë„ë¡œ í° LMì„ ì‚¬ìš©í•˜ì§„ ì•ŠëŠ”ë‹¤. ë‘ê°œì˜ biLSTM layerë¡œ êµ¬í˜„í–ˆë‹¤ê³  í•œë‹¤. ë˜í•œ initial word representationì„ ìœ„í•´ character CNNì„ ì‚¬ìš©í–ˆë‹¤ê³  í•˜ê³ , redisual connectionë„ ì‚¬ìš©í–ˆë‹¤ê³  í•œë‹¤. ìì„¸í•œ ì‚¬í•­ì€ ë…¼ë¬¸ì„ ì½ì–´ë³´ì•„ì•¼ ì•Œ ìˆ˜ ìˆì„ ê²ƒ ê°™ë‹¤.

## ULMfit and onward

ULMfit: Universal Language Model [^ULM]

ì–´ë–»ê²Œ NLM Knowledgeë¥¼ ê³µìœ í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ê°€ í•µì‹¬. text classificationì„ ì˜ˆì‹œë¡œ ê°•ì˜ì—ì„œëŠ” ì„¤ëª…í•œë‹¤. ULMfitì€ reasonable-sizeì—¬ì„œ ë” ì•Œë ¤ì§„ ì ë„ ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. 1GPUë¡œ í•™ìŠµì´ ê°€ëŠ¥í–ˆë‹¤ê³  í•œë‹¤. transfer learning ê°™ì€ í‚¤ì›Œë“œë¥¼ ê°™ì´ ì°¾ì•„ë³´ì.

[^ULM]: [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146) ì—¬ê¸° ë…¼ë¬¸ì„ ì°¸ê³ í•˜ì

ULMfit ì´í›„ë¡œ ì ì  ê³„ì† í°ëª¨ë¸ì´ ë§ì´ ë‚˜ì˜¨ë‹¤. OpenAIì—ì„œ ë§Œë“  2048ê°œì˜ TPUë¥¼ ì‚¬ìš©í•˜ëŠ” GPT-2 ëª¨ë¸ì€ ê½¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤ê³ .

{% include image.html url="/images/cs224n/13-3.png" %}

ê·¼ë° í° ëª¨ë¸ë“¤ì€ ì „ë¶€ Transformerë‹¤.

## Transformer architecture

### Motivation

ìš°ë¦° RNN ì—°ì‚°ì„ parallelizationí•˜ê³  ì‹¶ë‹¤. ê·¼ë° long range dependenciesëŠ” ê·¸ëŒ€ë¡œ í•„ìš”í•˜ë‹¤. ê·¸ë¥¼ ìœ„í•´ Recurrant Modelì—ì„œëŠ” Attentionì„ ì‚¬ìš©í•˜ê³  ìˆì—ˆê³ , attentionì´ í•´ë‹¹ dependencyë¥¼ ì•Œë ¤ì£¼ë‹ˆ, ê·¸ëƒ¥ Recurrant Modelì„ ì“°ì§€ ì•Šê³  attentionë§Œ ì¨ë³´ëŠ” ê²ƒì€ ì–´ë–¤ê°€?

### Overview

Attention is all you need ë…¼ë¬¸ì„ ë³´ì. ê·¸ì™€ ê°™ì´ ë³´ê¸°ë¥¼ ì¶”ì²œí•˜ëŠ” ë¦¬ì†ŒìŠ¤ë“¤ì€ ì•„ë˜ì •ë„ì´ë‹¤.

* [http://nlp.seas.harvard.edu/2018/04/03/attention.html](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

ë…¼ë¬¸ì—ì„œëŠ” Dot Product Attention, Scaled Dot Product Attention, Multi-head attentionì„ í•˜ëŠ”ë°, ë…¼ë¬¸ ì½ê³ ë„ ì˜ ì´í•´ ì•ˆë˜ì—ˆìœ¼ë‹ˆê¹Œ ë…¼ë¬¸ ì •ë¦¬í• ë•Œ ë‹¤ì‹œ ë³´ì.

ì•„ë˜ì™€ ê°™ì€ í‚¤ì›Œë“œ/ë…¼ë¬¸ì„ ì°¾ì•„ë³´ì

* byte-pair encoding
* checkpoint averaging
* adam optimizer
* dropout
* label smoothing
* auto-regressive decoding with beam search and length penalties

## BERT

ë…¼ë¬¸ì€ [ì—¬ê¸°](https://arxiv.org/abs/1810.04805)ë¥¼ ë³´ë©´ ëœë‹¤.

BERTì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ì–¸ì–´ëŠ” ì–‘ë°©í–¥ìœ¼ë¡œ ì´í•´í•´ì•¼ í•˜ëŠ”ë°, ì™œ í•œìª½ë§Œ ë³¼ê¹Œ?ë¼ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ Bidrectional contextë¥¼ êµ¬ì„±í–ˆë‹¤. í•™ìŠµì€ k%ì˜ ë‹¨ì–´ë¥¼ ê°€ë¦¬ê³  ê·¸ ë‹¨ì–´ë“¤ì— ëŒ€í•œ predictionì„ í†µí•´ í•˜ê²Œ ë˜ì—ˆë‹¤. í•­ìƒ 15%ë¥¼ ì‚¬ìš©í–ˆë‹¤ê³  í•˜ëŠ”ë°, kê°€ ë†’ìœ¼ë©´ contextê°€ ì¶©ë¶„í•˜ì§€ ì•Šê³ , kê°€ ë„ˆë¬´ ì ìœ¼ë©´ í•™ìŠµí•˜ê¸°ì—ëŠ” ë„ˆë¬´ costê°€ ë†’ë‹¤.
