---
layout: post
title: "ğŸ“• CS224n Lecture 13 Modeling contexts of use: Contextual Representations and Pretraining"
tags:
  - nlp
  - cs224n
  - machine learning
---

13ê°• ì •ë¦¬! 11ê°•ë¶€í„°ì¸ê°€? ê·¸ë•Œë¶€í„° ëŒ€ë¶€ë¶„ ì†Œê°œê°€ ë˜ì–´ê°€ê³  ìˆì–´ì„œ ì¢‹ì€ ë§í¬ ì •ë¦¬ ì •ë„ë§Œ í•˜ê³  ìˆëŠ” ê²ƒ ê°™ë‹¤.

## Reflections on word representation

ì§€ê¸ˆê¹Œì§€ëŠ” word embeddingì„ ì‹œì‘ë¶€í„° í–ˆëŠ”ë°, pretrained modelë¥¼ ì‚¬ìš©í•˜ìëŠ” ë§. ê·¸ ì´ìœ ëŠ” ë” ë§ì€ ë‹¨ì–´ì™€ ë” ë§ì€ ë°ì´í„°ì— ëŒ€í•´ í•™ìŠµì´ ê°€ëŠ¥í•´ì§„ë‹¤ëŠ” ì´ìœ ì´ë‹¤. ì‹¤ì œë¡œ ì„±ëŠ¥ë„ ë” ë†’ì€ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.

ê·¼ë° unknown wordsì— ëŒ€í•´ì„œëŠ” ì–´ë–»ê²Œ ëŒ€ì‘í•  ê²ƒì¸ê°€? `UNK`ìœ¼ë¡œ ë§¤í•‘í•´ì„œ ì–´ì©Œêµ¬ì €ì©Œêµ¬ë¥¼ í•˜ì§€ë§Œ ê²°ë¡ ì€ char-level modelì„ ì‚¬ìš©í•˜ì! ë˜ëŠ” í…ŒìŠ¤íŠ¸ë•Œ `<UNK>`ê°€ unsupervised word embeddingì— ì¡´ì¬í•œë‹¤ë©´ ê·¸ê±¸ ê³„ì† ì“°ê³ , ê·¸ëƒ¥ ì•„ì˜ˆ ëª¨ë¥´ëŠ” ê²ƒì€ random vectorë¡œ ë§Œë“ ë‹¤ìŒì— vocabì— ì¶”ê°€í•˜ëŠ” ë°©ë²•ë„ ê³ ë ¤í•´ë³´ë¼ê³  í•œë‹¤. [^Dhingra2017]

[^Dhingra2017]: [A Comparative Study of Word Embeddings for Reading Comprehension](https://arxiv.org/abs/1703.00993) í—¤ë”© ë…¼ë¬¸

ì–´ì°Œë˜ì—ˆë“  word embeddingì„ ì‹œì‘ë¶€í„° í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì€ ë‘ê°€ì§€ í° ë¬¸ì œê°€ ìˆëŠ”ë°, í•˜ë‚˜ì˜ ë‹¨ì–´ì— ëŒ€í•´ context ìƒê´€ì—†ì´ ë‹¤ ê°™ì€ representationì„ ê°€ì ¸ì˜¨ë‹¤ëŠ” ì ì´ë‹¤.

## Pre-ELMo and ELMO

## ULMfit and onward

## Transformer architecture

## BERT
