---
layout: post
title: "ğŸ“ƒ Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
tags:
  - nlp
  - paper
---

MeCabì„ í•œêµ­ì–´ë¥¼ ìœ„í•´ ìƒˆë¡œ ì‘ì„±í•´ë³´ê³  ì‹¶ì–´ì„œ [ê·¸ ë…¼ë¬¸(Applying Conditional Random Fields to Japanese Morphological Analysis)](https://www.aclweb.org/anthology/W04-3230.pdf)ì„ ì°¾ì•„ë³´ì•˜ë”ë‹ˆ, CRF based segmentationì´ë¼ê³  ë¶ˆëŸ¬ì„œ CRFì— ëŒ€í•´ ìš°ì„  ì •ë¦¬í•œë‹¤. ~~(í”„ë¡œ ì•¼í¬ ì‰ì´ë²„)~~

ëŒ€ì¶© MeCab ê´€ë ¨ ê¸€ë“¤ì„ ë³´ë‹ˆ [ì´ ë…¼ë¬¸(Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data)](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)ì´ ì œì¼ ì—°ê´€ìˆì–´ ë³´ì—¬ì„œ ì´ê±°ë¡œ ì •ë¦¬í•´ë³¸ë‹¤.

---

## Abstract

CRFsëŠ” Hidden Markov modelì— ë¹„í•´ì„œë„ ëª‡ëª‡ ì´ì ë“¤ì´ ìˆê³ , CRFsëŠ” MEMMs(Maximum entropy Markov models)ì˜ ê¸°ë³¸ì ì¸ í•œê³„ë¥¼ ê·¹ë³µí•  ìˆ˜ë„ ìˆê³ ,ë‹¤ë¥¸ directed graphical modelë“¤ì˜ ê¸°ë°˜ì¸ discriminative Markov modlë“¤ì˜ í•œê³„ë„ ê·¹ë³µí•  ìˆ˜ ìˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œ CRFsë¥¼ ìœ„í•œ iterative parameter estimation algorithmì„ ì—¬ê¸°ì„œ ì œì‹œí•œë‹¤. ê·¸ë¦¬ê³  ê·¸ ê²°ê³¼ë¥¼ HMMsì™€ MEMMsê³¼ ë¹„êµí•œë‹¤.

## Introduction

* HMMs and stochastic grammars are generative models
  * To define a joint probability over observation and label sequences, a generative model needs to enumerate all possible ob- servation sequences, typically requiring a representation in which observations are task-appropriate atomic entities, such as words or nucleotides
  * This difficulty is one of the main motivations for looking at conditional models as an alternative.
* A conditional model specifies the probabilities of possible label sequences given an observation sequence
  * Therefore, it does not expend modeling effort on the observations, which at test time are fixed anyway.
  * Maximum entropy Markov models (MEMMs) are condi- tional probabilistic sequence models
* In MEMMs, each source state1 has a exponential model that takes the observation features as input, and outputs a distribution over possible next states
  * MEMMs and other non-generative finite-state models based on next-state classifiers, such as discriminative Markov models (Bottou, 1991), share a weakness we call here the label bias problem
  * label bias problem: the transitions leaving a given state compete only against each other, rather than against all other transitions in the model
