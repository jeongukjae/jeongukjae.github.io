---
layout: post
title: "ğŸ“ƒ Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
tags:
  - nlp
  - paper
---

MeCabì„ í•œêµ­ì–´ë¥¼ ìœ„í•´ ìƒˆë¡œ ì‘ì„±í•´ë³´ê³  ì‹¶ì–´ì„œ [ê·¸ ë…¼ë¬¸(Applying Conditional Random Fields to Japanese Morphological Analysis)](https://www.aclweb.org/anthology/W04-3230.pdf)ì„ ì°¾ì•„ë³´ì•˜ë”ë‹ˆ, CRF based segmentationì´ë¼ê³  ë¶ˆëŸ¬ì„œ CRFì— ëŒ€í•´ ìš°ì„  ì •ë¦¬í•œë‹¤. ~~(í”„ë¡œ ì•¼í¬ ì‰ì´ë²„)~~

ëŒ€ì¶© MeCab ê´€ë ¨ ê¸€ë“¤ì„ ë³´ë‹ˆ [ì´ ë…¼ë¬¸(Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data)](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)ì´ ì œì¼ ì—°ê´€ìˆì–´ ë³´ì—¬ì„œ ì´ê±°ë¡œ ì •ë¦¬í•´ë³¸ë‹¤.

---

## Abstract

CRFsëŠ” Hidden Markov modelì— ë¹„í•´ì„œë„ ëª‡ëª‡ ì´ì ë“¤ì´ ìˆê³ , CRFsëŠ” MEMMs(Maximum entropy Markov models)ì˜ ê¸°ë³¸ì ì¸ í•œê³„ë¥¼ ê·¹ë³µí•  ìˆ˜ë„ ìˆê³ ,ë‹¤ë¥¸ directed graphical modelë“¤ì˜ ê¸°ë°˜ì¸ discriminative Markov modlë“¤ì˜ í•œê³„ë„ ê·¹ë³µí•  ìˆ˜ ìˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œ CRFsë¥¼ ìœ„í•œ iterative parameter estimation algorithmì„ ì—¬ê¸°ì„œ ì œì‹œí•œë‹¤. ê·¸ë¦¬ê³  ê·¸ ê²°ê³¼ë¥¼ HMMsì™€ MEMMsê³¼ ë¹„êµí•œë‹¤.

## 1. Introduction

HMMsì´ë‘ stochastic grammersëŠ” generative modelì¸ë°, ì´ê²Œ joint probì„ observationê³¼ label sequenceì— ëŒ€í•´ ì •ì˜í•˜ê¸° ìœ„í•´ì„œ ëª¨ë“  ê°€ëŠ¥í•œ observation sequenceì— ëŒ€í•´ì„œ ê³„ì‚°ì„ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ì¡°ê¸ˆ.. ê·¸ë ‡ë‹¤. ê·¸ë˜ì„œ ì´ëŸ° ë³µì¡ë„ê°€ conditional fieldë¥¼ ì‚¬ìš©í•˜ê²Œ ëœ ì›ì¸ì´ë¼ê³  í•œë‹¤.

conditional modelì€ ì˜¤ë¡œì§€ given observation sequenceì— ëŒ€í•´ ê°€ëŠ¥í•œ label sequenceì˜ probì„ ê³„ì‚°í•œë‹¤. ì´ê²Œ observation ì°¾ìœ¼ë ¤ê³  ì• ì“°ì§€ ì•Šì•„ë„ ë˜ì–´ì„œ ì¢‹ë‹¤ê³  í•œë‹¤. MEMMsì´ conditional probabilistic sequence modelì´ë¼ê³ .

MEMMsì€ ê°ê°ì˜ source stateê°€ exponential modelì¸ë°, observation featuresë¥¼ ì…ë ¥ìœ¼ë¡œ distribution over possible next statesë¥¼ ì¶œë ¥ìœ¼ë¡œ ë‚¸ëŠ” ëª¨ë¸ì´ë‹¤. ê·¼ë° MEMMsì´ë‚˜ non-generative, finite-state, next-state classifiers basedì¸ ëª¨ë¸(discriminative Markov modelê°™ì€)ì€ label bias problemì´ ìˆë‹¤. ì¼ë‹¨ transition scoreëŠ” ì´ì „ì˜ stateì—ì„œ ì´ stateë¡œ ì˜¬ í™•ë¥ ì¸ë°, ì´ë¯¸ì§€ë¥¼ ë”°ì˜¤ê¸° ê·€ì°®ì•„ì„œ ê¸€ë¡œë§Œ ì„¤ëª…í•˜ìë©´, í•˜ë‚˜ì˜ stateì—ì„œ ë‹¤ë¥¸ stateë¡œ ì´ë™í•  í™•ë¥ ì˜ ì´í•©ì´ 1ì´ë‹¤. ê·¸ë˜ì„œ pathê°€ ê³„ì† ëŠ˜ì–´ë‚  ìˆ˜ë¡ ì´ scoreëŠ” ê³„ì† ë‚®ì•„ì§€ëŠ”ë°, ê·¸ë˜ì„œ ì§„ì§œì§„ì§œ ì•ˆì¢‹ì€ ê²½ìš°ì—ëŠ” single outgoing pathê°€ ë˜ê²Œ ê¸¸ì§€ë§Œ ì¢‹ì€ pathë¥¼ ì´ê¸¸ ìˆ˜ë„ ìˆë‹¤.

ì—¬ê¸°ì„œ ì†Œê°œí•˜ëŠ” CRFsëŠ” sequence modeling frameowrkì¸ë°, MEMMsì˜ ì´ì ì„ ê°€ì ¸ì˜¤ë©´ì„œ label bais problemì„ í‘¼ë‹¤. ê°€ì¥ ì¤‘ìš”í•œ ì°¨ì´ì ì€ MEMMì€ per-state exponential modelì¸ë°, CRFëŠ” single exponential modelì´ë‹¤.

CRFëŠ” unnormalzied transition probabilitesë¥¼ ê°€ì§€ëŠ” finite state modelì´ë‹¤. í•˜ì§€ë§Œ ë‹¤ë¥¸ weighted fininte-state approachesì™€ëŠ” ë‹¤ë¥´ê²Œ CRFëŠ” possible labelingì— ëŒ€í•´ well-defined probability distributionì„ ë°°ì •í•œë‹¤. ê·¸ë¦¬ê³  loss functionì´ convex ì´ê³  global optimumìœ¼ë¡œ convergenceë¥¼ ë³´ì¥í•œë‹¤.

ê·¸ë˜ì„œ ì´ ëª¨ë¸ì˜ ë‘ê°€ì§€ í•™ìŠµ ë°©ë²•ì„ ì†Œê°œí•˜ê³ , convergenceì˜ ì¦ëª…ì„ ì†Œê°œí•œë‹¤ê³  í•œë‹¤.

## 2. The Label Bias Problem

ì¼ë‹¨ ë„˜ê¸°ê³  ë‚˜ì¤‘ì— í•„ìš”í•˜ë©´ ë³¼ë˜ìš”.

## 3. Conditional Random Fields

* $$X$$ : random variable over data sequence to be labeled
* $$Y$$ : random variable over corresponding label sequence
* $$\mathcal {Y}$$ : range over a finite label

ê·¸ëƒ¥ $$X$$ê°€ natural language sentenceì´ê³ , $$Y$$ê°€ POS tagging í•œê±°ë¼ ìƒê°í•˜ë©´ ë ê²ƒ ê°™ì•„ìš”. $$\mathcal Y$$ëŠ” ê°€ëŠ¥í•œ POS íƒœê·¸ ì „ë¶€..?

CRFëŠ” random field globally conditioned on the observation $$X$$ì´ê³ , $$Y$$ì™€ $$X$$ëŠ” jointly distributedì´ë‹¤. ê·¸ë˜ì„œ jointly didstribution over the label sequence $$Y$$ given $$X$$ëŠ” ì•„ë˜ì™€ ê°™ì•„ì§„ë‹¤.

$$p_\theta (y|x) \propto \exp(\sum_{e \in E, k} \lambda_k f_k (e, y|_e, x) + \sum_{v\in V, k} \mu_kg_k(v, y|_v, x))$$

$$x$$ëŠ” data sequenceì´ê³ , $$y$$ëŠ” label sequenceì´ë‹¤. feature functionì¸ $$f_k$$ì™€ $$g_k$$ëŠ” ê³ ì •ë˜ì–´ìˆë‹¤ê³  í•œë‹¤. $$Y$$ì˜ graph $$G = (V, E)$$ëŠ” tree í˜•íƒœì´ë‹¤.

parameter estimationì€ $$\theta = (\lambda_1, \lambda_2, ... ; \mu_1, \mu_2, ...)$$ì„ training data $$ \mathcal D = \{(x^i, y^i)\}^N_{i=1} $$ë¡œë¶€í„° ê²°ì •í•˜ëŠ” problemì´ë‹¤.

Objective functionì€ ì•„ë˜ì™€ ê°™ë‹¤.

$$\mathcal O (\theta) = \sum^N_{i=1} \log p_{\theta} (y^i | x^i) \propto \sum_{x, y} \tilde p (x, y) \log p_{\theta} (y|x)$$

{% include image.html url="/images/crf/fig2.png" description="HMMs, MEMMs, chain-structed case of CRFsë¥¼ ìˆœì„œëŒ€ë¡œ ë‚˜íƒ€ë‚¸ graphical structures. open circleì€ ëª¨ë¸ì—ì„œ ë§Œë“¤ì–´ì§„ ê²Œ ì•„ë‹ˆë‹¤." %}

ê·¼ë° ìœ„ ì´ë¯¸ì§€ì—ì„œ chain-structed case of CRFsë¼ê³  ì ì–´ë†“ì•˜ëŠ”ë°, ì´ ë…¼ë¬¸ì—ì„œ ì£¼ë¡œ ë‹¤ë£¨ëŠ” ê²ƒì€ chain-structed case of CRFsì´ë‹¤.

í‘œí˜„ì„ ë‹¨ìˆœí•˜ê²Œ í•˜ê¸° ìœ„í•´ì„œ start, stop stateë¥¼ ë„£ì–´ì¤€ë‹¤ê³  í•œë‹¤. BOS, EOS ê°™ì€ í† í°ì¸ê°€ë³´ë‹¤. $$Y_0 = \text{start}$$, $$Y_{n+1} = \text{stop}$$ì´ë‹¤.

chain structureì—ì„œ labelì˜ conditional probì€ matrix formìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ì´ê±´ parameter estimationì´ë‘ inferenceì—ì„œ ì—„ì²­ ìœ ìš©í•˜ë‹¤ëŠ”ë° ì´ê±´ ë˜ ë‹¤ìŒ ì„¹ì…˜ì—ì„œ..

$$p_\theta (Y\rvert X)$$ê°€ CRFë¼ê³  í•  ë•Œ, observation sequence $$x$$ì˜ ê° position $$i$$ì— ëŒ€í•´ $$\rvert \mathcal Y \rvert \times \rvert \mathcal Y \rvert$$ matrix random variable $$M_i (x) = [M_i(y', y\rvert x)]$$ë¥¼ ì •ì˜í•œë‹¤.

$$M_i (y', y\rvert x) = \exp (\Lambda_i (y', y \rvert x))$$

$$\Lambda_i (y', y \rvert x) = \sum_k \lambda_k f_k (e_i, Y\rvert_{e_i} = (y', y), x) + \sum_k \mu_k g_k (v_i, Y \rvert_{v_i} = y, x)$$

$$e_i$$ëŠ” edge with labels $$(Y_{i-1}, Y_i)$$ì´ê³  $$v_i$$ëŠ” vertex with label $$Y_i$$ì´ë‹¤.

normalization (partition function) $$Z_\theta(x)$$ëŠ” ìœ„ì˜ ëª¨ë“  matricesë“¤ì˜ ê³±ì˜ $$(\text{start}, \text{stop})$$ entryì´ë‹¤.

$$Z_\theta (x) = (\prod_{i=1}^{n+1} M_i (x))_{\text{start, stop}} $$

ìœ„ë¥¼ ì´ìš©í•˜ë©´ $$y_0 = \text{start}$$, $$y_{n+1} = \text{stop}$$ì¼ ë•Œ ì•„ë˜ì²˜ëŸ¼ ë‹¤ì‹œ ì“¸ ìˆ˜ ìˆë‹¤.

$$p_\theta(y \rvert x) = \frac {\prod^{n+1}_{i=1} M_i(y_{i-1}, y_i \rvert x)} {Z_\theta (x)}$$

## 4. Parameter Estimation for CRFs

ë‘ê°œì˜ iterative scaling algorithmì„ ì†Œê°œí•˜ëŠ”ë°, ë‘˜ ë‹¤ improved iterative scaling algorithm of Deela Pietra et al.(1997)ì— ê¸°ë°˜í•œë‹¤.

Iterative scaling algorithmì€ weightsë¥¼ $$\lambda_k \leftarrow \lambda_k + \delta \lambda_k$$ë‘ $$\mu_k \leftarrow \mu_k + \delta \mu_k$$ë¡œ í•œë‹¤.

---

ì´ ì´í›„ë¡œëŠ” Viterbi algorithmì²˜ëŸ¼ inferenceì— í•„ìš”í•œ ì•Œê³ ë¦¬ì¦˜ì„ ë¨¼ì € ì•Œì•„ì•¼ í•  ê²ƒ ê°™ì•„ì„œ ì¼ë‹¨ Pending..
