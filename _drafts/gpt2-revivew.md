---
layout: post
title: ğŸ“ƒ GPT2 ë¦¬ë·°
tags:
  - nlp
  - paper
---

GPTì— ì´ì–´ì„œ [GPT2 ë…¼ë¬¸ (Language Models are Unsupervised Multitask Learners)](https://openai.com/blog/better-language-models/)ë„ ê°„ë‹¨í•˜ê²Œ ì½ì–´ë³´ì•˜ë‹¤. ì—­ì‹œ ì •ë¦¬í•˜ê¸°ì— ê·€ì°®ì€ ë¶€ë¶„ì€ ë‹¤ ê±´ë„ˆë›´ë‹¤.

## Abstract

ì›ë˜ NLPë¥¼ ìˆ˜í–‰í•  ë•Œ task specific datasetì—ë‹¤ê°€ supervised learningì„ í•˜ëŠ”ë°, GPT2ëŠ” ì´ëŸ° supervision ì—†ì´ LMë§Œìœ¼ë¡œ í’€ì–´ë³´ìê³  í•˜ëŠ” ëª¨ë¸ì´ë‹¤. ê°€ì¥ í° ëª¨ë¸ GPT-2ëŠ” 1.5Bê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ëŠ” Transformerë¡œ WebTextì— ëŒ€í•´ ë‹¤ í•™ìŠµì´ ì•ˆë˜ì—ˆì–´ë„ í…ŒìŠ¤íŠ¸í•œ 8ê°œì˜ ë¶„ì•¼ ì¤‘ 7ê°œì—ì„œ sotaë¥¼ ë‹¬ì„±í–ˆë‹¤ê³  í•œë‹¤.

## 1. Introduction

Language taskë“¤ì—ì„œ ìµœê·¼ ì¢‹ì€ ì„±ëŠ¥ì„ ëª¨ë¸ì€ pre-trainingê³¼ superivsed fine-tuningì˜ ì¡°í•©ìœ¼ë¡œ êµ¬ì„±ë˜ëŠ” ëª¨ë¸ì¸ë°, ì´ëŸ° ì ‘ê·¼ë²•ì´ ë” ìœ ì—°í•œ ë°©ì‹ìœ¼ë¡œ í•™ìŠµëœ ì •ë³´ë¥¼ transferí•˜ëŠ” ê²ƒì´ë¼ê³  í•œë‹¤. ì²«ë²ˆì§¸ëŠ” Word2Vecì²˜ëŸ¼ word vectorë¥¼ í•™ìŠµí•´ì„œ task-specific architectureì˜ inputìœ¼ë¡œ ë„£ì–´ì£¼ë‹¤ê°€, recurrent networkì˜ contextual representationì„ ì‚¬ìš©í•˜ê²Œ ë˜ê³ , ì´ì œëŠ” task-specificí•œ architectureì—†ì´ ê·¸ëƒ¥ self-attention blockì„ ê³„ì† ì´ì–´ì„œ ì‚¬ìš©í•˜ê²Œ ë˜ì—ˆë‹¤. í•˜ì§€ë§Œ ë¬¸ì œì ì€ ì´ëŸ° ë°©ì‹ì€ ì—¬ì „íˆ supervised trainingì„ ìš”êµ¬í•œë‹¤ëŠ” ê²ƒì´ë‹¤. supervised trainingì„ í•  ìˆ˜ ì—†ì„ ë•Œ, ì¦‰, supervised dataê°€ ì—†ê±°ë‚˜ ë§¤ìš° ì ì„ ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²•ìœ¼ë¡œ LMì„ íŠ¹ì •í•œ íƒœìŠ¤í¬ë“¤ì„ ìœ„í•´ ë™ì‘í•˜ê²Œ ë§Œë“œëŠ” ë°©ë²•ì´ ìˆë‹¤.

ê·¸ë˜ì„œ GPT-2ëŠ” ì´ ë°©ë²•ì„ í•©ì³ì„œ LMìœ¼ë¡œ ì—„ì²­ë‚˜ê²Œ í•™ìŠµì‹œì¼œì„œ down-stream taskë“¤ì„ parameter ìˆ˜ì •ì´ë‚˜ architecture modification ì—†ì´ ìˆ˜í–‰í•˜ê²Œ ë§Œë“ ë‹¤ëŠ” ê²ƒì´ë‹¤.

## 2. Approach

ì¼ë‹¨ í•µì‹¬ì€ LMì´ë‹¤. ê·¸ë¦¬ê³  ì–¸ì–´ëŠ” natural sequential orderingì´ ìˆìœ¼ë‹ˆ joint probabilitiesë¥¼ conditional probabilitiesì˜ ê³±ìœ¼ë¡œ factorizeí•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.

$$ p(x) = \prod^n_{i=1} p(s_n\rvert s_1, ..., s_{n-1})$$

(ê·¼ë° ì´ê±° $$s_n$$ì´ ì•„ë‹ˆë¼ $$s_i$$ì•„ë‹ê¹Œ...?) ì—¬ê¸°ì„œ conditional probabilityê°€ ë‚˜ì™”ìœ¼ë‹ˆê¹Œ ì´ ê²ƒë“¤ì„ ì˜ í‘œí˜„í•  ìˆ˜ ìˆëŠ” self-attention arhictectureë¡œ ì˜ ê³„ì‚°í•œë‹¤.

ê·¼ë° general systemì€ ë§ì€ íƒœìŠ¤í¬ë“¤ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì–´ì•¼ í•˜ëŠ”ë°, ìœ„ í˜•íƒœëŠ” $$p(output \rvert input)$$ ë°–ì— ìˆ˜í–‰ì„ ëª»í•œë‹¤. ê·¸ë˜ì„œ $$p(output \rvert input, task)$$ì™€ ê°™ì€ í˜•íƒœë¡œ ëª¨ë¸ë§ì„ í•œë‹¤ê³  í•œë‹¤. task conditioningì€ ë³´í†µ architectrure levelì—ì„œ êµ¬í˜„í•˜ëŠ” ê²ƒì€ task specific encoders and decoders(Kaiser et al., 2017)ì™€ ê°™ì€ ê²ƒì„ ì‚´í´ë³´ë©´ ë  ê²ƒ ê°™ë‹¤. ê·¸ì™€ ë°˜ëŒ€ë¡œ ì•Œê³ ë¦¬ì¦˜ ë ˆë²¨ì—ì„œ êµ¬í˜„í•˜ëŠ” ê²ƒì€ the inner and outer loop optimization framework of MAML (Finn et al., 2017)ê°™ì€ ê²ƒì„ ì‚´í´ë³´ë©´ ë  ê²ƒ ê°™ë‹¤.

## ë” ì½ì–´ë³´ê³  ì‹¶ì€ ë¦¬ìŠ¤íŠ¸

* Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137â€“1155, 2003.
* Kaiser, L., Gomez, A. N., Shazeer, N., Vaswani, A., Parmar, N., Jones, L., and Uszkoreit, J. One model to learn them all. arXiv preprint arXiv:1706.05137, 2017.
