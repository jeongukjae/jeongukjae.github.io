---
layout: post
title: "ğŸ“ƒ BERT ë¦¬ë·°"
tags:
  - nlp
  - paper
---

ìµœê·¼ì— [ğŸ¤—/transformers](https://github.com/huggingface/transformers)ì—ì„œ ë‹¤ì–‘í•œ transformer based modelì„ ì†Œê°œí–ˆëŠ”ë°, ê·¸ë˜ì„œ transformer ê¸°ë°˜ì˜ ì—¬ëŸ¬ ëª¨ë¸ë“¤ì„ ì •ë¦¬í•´ë³´ë ¤ í•œë‹¤. ì²«ë²ˆì§¸ë¡œ Googleì˜ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)ë¥¼ ì •ë¦¬í•´ë³¸ë‹¤.

BERTëŠ” 2018ë…„ 10ì›”ì— ë‚˜ì˜¨ ëª¨ë¸ë¡œ í˜„ì¬ ì˜¤í”ˆì†ŒìŠ¤í™” ë˜ì–´ìˆë‹¤. ([google-research/bert](https://github.com/google-research/bert))

## Abstract

ELMoê°™ì€ ê·¸ ë‹¹ì‹œì˜ language representation modelê³¼ëŠ” ë‹¤ë¥´ê²Œ BERTëŠ” unlabeled textì—ì„œ right, left contextë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ê²Œ ë§Œë“œëŠ” deep bidirectional representationsì„ pretrainí•˜ê¸° ìœ„í•´ì„œ ì„¤ê³„ë˜ì—ˆë‹¤. ê·¸ë¥¼ í†µí•´ pre-trained BERT modelì„ fine tuning + í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ ì–¹ëŠ” ê²ƒ ë§Œìœ¼ë¡œë„ sotaëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆì—ˆë‹¤ê³  í•œë‹¤.

## 1. Introduction

LMì„ pretrainí•˜ëŠ” ê²ƒì€ ê·¸ ë‹¹ì‹œì—ë„ ì¢‹ë‹¤ê³  ì—¬ê²¨ì¡ŒëŠ”ë°, (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018)ë“±ì„ í™•ì¸í•´ë³´ê³  ì•Œ ìˆ˜ ìˆë‹¤.

ê·¸ëŸ° pre-trained language representationsë¥¼ downstream taskë“¤ì— ì ìš©í•˜ëŠ” ë°©ë²•ì—ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì´ ìˆë‹¤. feature-basedì™€ fine-tuningì´ë‹¤. feature-basedëŠ” ELMoì™€ ê°™ì€ ëª¨ë¸ì´ ì†í•˜ê³  pre-trained representationì„ ì¶”ê°€ì ì¸ featureì •ë„ë¡œ í™œìš©ì„ í•˜ê³  task-specific architectureë¥¼ ê°€ì ¸ê°€ëŠ” ë°©ì‹ì´ë‹¤. ê·¸ì™€ ë‹¤ë¥´ê²Œ fine-tuningë°©ë²•ì€ GPTì™€ ê°™ì€ ëª¨ë¸ì´ ì†í•˜ê³  task-specificí•œ parametersë¥¼ ìµœëŒ€í•œ ì¤„ì´ê³ , pretrained parametersë¥¼ fine-tuningí•˜ê¸°ë§Œ í•˜ëŠ” ë°©ë²•ì´ë‹¤.

êµ¬ê¸€ì€ ì—¬ê¸°ì„œ ë‘˜ ë‹¤ ê°™ì€ objective functionì„ ì‚¬ìš©í•˜ê³  general language representationì„ ìœ„í•´ unidirectional LMì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— pretrained reprsentationì˜ ê°•ë ¥í•¨ì„ ì „ë¶€ ëŒì–´ì™€ì„œ ì“°ì§€ ëª»í•œë‹¤ê³  íŒë‹¨í•œ ê²ƒ ê°™ë‹¤. íŠ¹íˆ fine-tuning approachì—ì„œ. OpenAIì˜ GPTì™€ ê°™ì€ ê²½ìš°ì—ëŠ” left-to-right architectureì´ê¸° ë•Œë¬¸ì— Transformerì˜ self-attention layerì—ì„œ ëª¨ë“  tokenì´ ì´ì „ì˜ tokenë“¤ë§Œ í™œìš©í•  ìˆ˜ ìˆë‹¤.

ì—¬ê¸°ì„œëŠ” BERTë¥¼ ì†Œê°œí•˜ë©´ì„œ unidirectionality constraintë¥¼ masked LMì„ ì‚¬ìš©í•˜ë©´ì„œ í•´ê²°í–ˆë‹¤ê³  í•œë‹¤. masked MLì€ ì…ë ¥ ë¬¸ì¥ì—ì„œ ì„ì˜ë¡œ íŠ¹ì • í† í°ì„ ê°€ë¦¬ê³  í•´ë‹¹ ë¬¸ì¥ë§Œì„ ì£¼ê³  ê°€ë ¤ì§„ í† í°ë¥¼ ì¶”ì¸¡í•˜ê²Œ ë§Œë“œëŠ” ë°©ë²•ì´ë‹¤. masked LMì€ ê¸°ì¡´ì˜ LMê³¼ëŠ” ë‹¤ë¥´ê²Œ ì–‘ìª½ì˜ contextë¥¼ ì „ë¶€ í™œìš©í•  ìˆ˜ ìˆë‹¤. masked LM íƒœìŠ¤í¬ì™€ next sentence prediction taskë„ í’€ê²Œ í–ˆë‹¤ê³  í•œë‹¤.

## 2. Related Work

### 2.1. Unsupervised Featrue-based Approaches

ë‹¤ì–‘í•˜ê²Œ ì ìš©í•  ìˆ˜ ìˆëŠ” representationì„ í•™ìŠµí•˜ëŠ” ê²ƒì€ ìƒê°ë³´ë‹¤ ì˜¤ë«ë™ì•ˆ ì—°êµ¬ë˜ì–´ ì™”ëŠ”ë° word embeddingì„ pretrainí•˜ëŠ” ê²ƒì´ modern NLP Systemì´ë‹¤. ë³´í†µ ìµœê·¼ì—ëŠ” left-to-right langauge modeling objectivesê°€ ì‚¬ìš©ë˜ì–´ ì™”ë‹¤.

ELMoë¥¼ ë¹„ë¡¯í•œ ë‹¤ì–‘í•œ ëª¨ë¸ì˜ ì„¤ëª…ì´ ë‚˜ì˜¤ëŠ”ë° ë‚˜ì¤‘ì— ì½ì–´ë³´ëŠ” ê²Œ ê·¸ëƒ¥ ë” ì¢‹ì„ ê²ƒ ê°™ë‹¤.

* ELMo and its predecessor (Peters et al., 2017, 2018a)
* sentence embedding (Kiros et al., 2015; Logeswaran and Lee 2018)
* paragraph embedding (Le and Mikolov, 2014)
* Learning widely applicable representations
  * non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006)
  * neural (Mikolov et al., 2013; Pennington et al., 2014)

### 2.2. Unsupervised Fine-tuning Approaches

word embeddingì„ unlabeled textë¡œë¶€í„° pretrainedí•˜ëŠ” ë°©ë²•ì´ Collobert and Westn, 2008ë¶€í„° ì‹œì‘ë˜ì—ˆë‹¤. ë” ìµœê·¼ì—ëŠ” contextual token representaionì„ ë§Œë“¤ì–´ë‚´ëŠ” sentence, document encoderë¥¼ unlabeled textë¡œë¶€í„° pretrainì‹œì¼œì„œ downstream taskë¡œ fine tuningì„ ì‹œí‚¨ë‹¤. (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018)

### 2.3. Transfer Learning from Supervised Learning

supervised learningìœ¼ë¡œë¶€í„° íš¨ê³¼ì ì¸ transferë¥¼ ë³´ì—¬ì£¼ëŠ” ê²ƒë“¤ì´ ìˆì—ˆë‹¤. (NLIì—ì„œ Conneau et al., 2017, MTì—ì„œ McCann et al., 2017) Computer Visionì—ì„œëŠ” large pretrained modelì—ì„œì˜ transfer learningì˜ ì¤‘ìš”ì„±ì„ ë³´ì—¬ì£¼ê¸°ë„ í—€ë‹¤.

## 3. BERT

ì—¬íŠ¼ BERT ëª¨ë¸ê¹Œì§€ ì™”ë‹¤..

{% include image.html url="/images/bert/fig1.png" description="BERTì˜ ì „ì²´ì ì¸ ê·¸ë¦¼" %}

2ê°€ì§€ ìŠ¤í…ì´ ìˆëŠ”ë°, pre-trainingê³¼ fine-tuningì´ë‹¤. pre trainingì—ì„œ unlabeled dataìœ¼ë¡œ í•™ìŠµí•˜ê³  fine tuningì—ì„œ pre-trainingì—ì„œ í•™ìŠµí•œ ëª¨ë¸ parameterë¡œ ì´ˆê¸°í™”í•œ ë‹¤ìŒì— labeled dataë¡œ ë‹¤ì‹œ í•™ìŠµí•œë‹¤.

### Model Architecture

BERTê°€ ë‹¤ë¥¸ ëª¨ë¸ê³¼ í™•ì‹¤í•˜ê²Œ êµ¬ë³„ë˜ëŠ” ì ì€ ë‹¤ë¥¸ íƒœìŠ¤í¬ì—ì„œë„ ê°™ì€ ì•„í‚¤í…ì³ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ì ì´ë‹¤. ëª¨ë¸ ì•„í‚¤í…ì³ëŠ” multi layer bidirectional Transformer encoderì¸ë°, Vaswani et al. (2017)ì—ì„œ ì„¤ëª…í•˜ëŠ” ê²ƒê³¼ tensor2tensorì—ì„œ êµ¬í˜„í•´ë†“ì€ ê²ƒì— ê¸°ë°˜í–ˆë‹¤. [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)ì™€ Vaswani et al. (2017)ì„ ì°¸ê³ í•´ë³´ì.

BERT baseì™€ BERT large ëª¨ë¸ì„ ë§Œë“¤ì–´ì„œ í…ŒìŠ¤íŠ¸í–ˆë‹¤ê³  í•˜ëŠ”ë°, ê·¸ ì´ìœ ëŠ” BERT baseëŠ” OpenAI GPTì™€ ê°™ì€ ëª¨ë¸ ì‚¬ì´ì¦ˆë¡œ ë‘ê³  ë¹„êµí•´ë³´ê¸° ìœ„í•´ì„œë¼ê³  í•œë‹¤. ì¼ë‹¨ ë‘ ëª¨ë¸ì˜ ì£¼ìš”í•œ ì°¨ì´ì ì€ BERTëŠ” bidirectioanl self-attentionì´ ì—„ì²­ ìŒ“ì—¬ìˆëŠ”ë° GPTëŠ” self attentionì„ ìê¸° ì™¼ìª½ì˜ í† í°ë“¤ë§Œ ë³¼ ìˆ˜ ìˆê²Œ ë§Œë“¤ì—ˆë‹¤ê³ .

### Input Output Representation

Input/Output Representationì„ downstream taskë¥¼ ë‹¤ì–‘í•˜ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë§Œë“œë ¤ê³  single sentenceì™€ pair of sentenceë¥¼ ì¡°ê¸ˆ ëª¨í˜¸í•˜ê²Œ í•˜ë‚˜ì˜ token sequenceì—ì„œ í‘œí˜„í•  ìˆ˜ ìˆê²Œ í–ˆë‹¤. ì—¬ê¸°ì„œ sentenceëŠ” ì‹¤ì œ ì–¸ì–´ì˜ sentenceê°€ ì•„ë‹Œ ì„ì˜ì˜ ì—°ì†ì ì¸ í…ìŠ¤íŠ¸ì´ë‹¤. sequenceëŠ” BERTì— ë“¤ì–´ê°€ëŠ” input token sequenceë¥¼ ê°€ë¦¬í‚¨ë‹¤. (single sentenceë‚˜ two sentenceê°€ í•¨ê»˜ ë“¤ì–´ê°ˆ ìˆ˜ ìˆë‹¤)

BERTì—ì„œëŠ” 30,000ê°œì˜ ë‹¨ì–´ë¡œ WordPiece embeddingì„ ì‚¬ìš©í–ˆë‹¤. (Wu et al., 2016) í•­ìƒ ì²«ë²ˆì§¸ í† í°ì€ `[CLS]`ë¥¼ ì‚¬ìš©í–ˆë‹¤. ì´ í† í°ì— í•´ë‹¹í•˜ëŠ” final hidden stateëŠ” classification taskì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. sentence pairëŠ” ì—¬ê¸°ì„œ single sequenceë¡œ ë“¤ì–´ê°ˆ ìˆ˜ ìˆëŠ”ë°, special tokenìœ¼ë¡œ ë¶„ë¦¬ë¥¼ í•œë‹¤. (`[SEP]`)

### 3.1. Pre-training BERT

BERTë¥¼ ë‘ê°€ì§€ unsupervised taskë¡œ í•™ìŠµì„ ì‹œí‚¤ëŠ”ë° ì²«ë²ˆì§¸ê°€ Masked LMì´ê³  ë‘ë²ˆì§¸ê°€ Next Sentence Predictionì´ë‹¤.

#### Masked LM

LMì€ ë³´í†µ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ë³´ëŠ”ë°, ì •ë§ ê·¸ë ‡ê²Œ í•´ì•¼í•´ì„œì´ì§€ë§Œ, ê·¸ë˜ë„ deep bidirectional representaionì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ì¼ì • í™•ë¥ ë¡œ ëœë¤í•˜ê²Œ input tokenì„ maskingí–ˆë‹¤. ê·¸ë¦¬ê³  Masked tokenì„ ë„£ì—ˆë‹¤. ì´ ë•Œ final hidden vectorê°€ ê·¸ëƒ¥ LMì²˜ëŸ¼ vocabì— ëŒ€í•œ output softmaxë¡œ ì „ë‹¬ëœë‹¤ê³  í•œë‹¤. ê° sequenceì—ì„œ 15% ì •ë„ì˜ í™•ë¥ ë¡œ WordPiece tokenì„ ëœë¤í•˜ê²Œ ë§ˆìŠ¤í‚¹í–ˆê³ , denoising auto-encoders (Vincent et al., 2008)ê³¼ëŠ” ë‹¤ë¥´ê²Œ ì „ì²´ inputì„ reconstructingí•˜ê¸°ë³´ë‹¤ ê·¸ëƒ¥ masked wordsë§Œ predictí–ˆë‹¤.

ê·¼ë° ì´ê²Œ downstream taskì—ì„œëŠ” ì¢€ ì•ˆë§ëŠ”ê²Œ pretrainingë™ì•ˆë§Œ `[MASK]`ê°€ ë‚˜íƒ€ë‚˜ê³  fine-tuningí•  ë•ŒëŠ” ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ”ë°, ì´ê±¸ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ `[MASK]`ë¡œ ì¹˜í™˜í•˜ì§€ ì•Šì•˜ë‹¤. $$i$$-th tokenì´ ì„ íƒë˜ë©´ 80%ë§Œ `[MASK]`ë¡œë§Œ ì¹˜í™˜í•˜ê³  10%ëŠ” ëœë¤ìœ¼ë¡œ ì¹˜í™˜í•˜ê³  10%ëŠ” ë°”ê¾¸ì§€ ì•Šê³  ë†”ë‘”ë‹¤.
