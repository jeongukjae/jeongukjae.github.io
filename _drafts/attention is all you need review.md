---
title: ğŸ“ƒ Attention Is All You Need Review
layout: post
tags:
  - nlp
  - paper
  - cs224n
---

Transformerë¥¼ ì†Œê°œí•˜ëŠ” ë…¼ë¬¸ìœ¼ë¡œ, CS224nê°•ì˜ì˜ suggested readings ëª©ë¡ì— ìˆì–´ì„œ ì½ì–´ë³¸ ë…¼ë¬¸ì´ë‹¤. í•œêµ­ì–´ ë¦¬ë·°ë„ ì—„ì²­ ë§ì„ ì •ë„ë¡œ ìœ ëª…í•œ ë…¼ë¬¸ì´ë‹¤. í•´ë‹¹ ë…¼ë¬¸ì„ ì½ê³ , ê°„ëµí•œ ì •ë¦¬ë¥¼ í•´ë³´ì•˜ë‹¤. ë…¼ë¬¸ì€ [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)ì— ìˆë‹¤.

## Abtract

TransformerëŠ” ê¸°ì¡´ê³¼ ë‹¤ë¥´ê²Œ ì™„ì „íˆ attentionë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ êµ¬ì¡°ì´ë‹¤. 2014 WMT English-to-German translation taskì—ì„œ sotaë¥¼ ì°ì€ ëª¨ë¸ì´ë¼ê³  í•œë‹¤.

## Introduction & Background

Recurrent Modelì€ ìˆœì„œê°€ ì¤‘ìš”í•˜ë‹¤ëŠ” íŠ¹ì„±ìƒ ë³‘ë ¬í™”í•˜ê¸°ê°€ ì–´ë µë‹¤. í•˜ì§€ë§Œ ì´ transformerë¼ëŠ” Attentionì— ê¸°ë°˜í•œ ëª¨ë¸ì€ inputê³¼ outputì˜ global dependencyë¥¼ ë°”ë¡œ ë½‘ì•„ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë³‘ë ¬í™”í•˜ê¸° ì¢‹ë‹¤. ë”°ë¼ì„œ sotaì¸ ëª¨ë¸ì„ P100 8ëŒ€ë¡œ 12ì‹œê°„ë§Œì— ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆì—ˆë‹¤. sequence-aligned RNNì—†ì´ ì™„ì „íˆ self-attention (intra attention)ë§Œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì´ë‹¤.

## Model Architecture

{% include image.html url="/images/2019-06-09-transformer/1.png" description="Transformer architecture" %}

ìš°ì„  Encoder-decoder structureë¥¼ ê°€ì§€ê³  ìˆë‹¤. í•˜ì§€ë§Œ ë‹¤ë¥¸ encoder-decoder êµ¬ì¡°ì™€ ë‹¤ë¥¸ ì ì€ stacked self-attentionì„ ì‚¬ìš©í•œë‹¤ëŠ” ì ê³¼ point-wise feed forward networkë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ì ì´ë‹¤.

### Encoder and Decoder Stacks
