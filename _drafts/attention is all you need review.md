---
title: "ğŸ“ƒ Review of \"Attention Is All You Need\""
layout: post
tags:
  - nlp
  - paper
---

Transformerë¥¼ ì†Œê°œí•˜ëŠ” ë…¼ë¬¸ìœ¼ë¡œ, CS224nê°•ì˜ì˜ suggested readings ëª©ë¡ì— ìˆì–´ì„œ ì½ì–´ë³¸ ë…¼ë¬¸ì´ë‹¤. í•œêµ­ì–´ ë¦¬ë·°ë„ ì—„ì²­ ë§ì„ ì •ë„ë¡œ ìœ ëª…í•œ ë…¼ë¬¸ì´ë‹¤. í•´ë‹¹ ë…¼ë¬¸ì„ ì½ê³ , ê°„ëµí•œ ì •ë¦¬ë¥¼ í•´ë³´ì•˜ë‹¤. ë…¼ë¬¸ì€ [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)ì— ìˆë‹¤.

## Abtract

TransformerëŠ” ê¸°ì¡´ê³¼ ë‹¤ë¥´ê²Œ ì™„ì „íˆ attentionë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ êµ¬ì¡°ì´ë‹¤. 2014 WMT English-to-German translation taskì—ì„œ sotaë¥¼ ì°ì€ ëª¨ë¸ì´ë¼ê³  í•œë‹¤.

## 1. Introduction & 2. Background

Recurrent Modelì€ ìˆœì„œê°€ ì¤‘ìš”í•˜ë‹¤ëŠ” íŠ¹ì„±ìƒ ë³‘ë ¬í™”í•˜ê¸°ê°€ ì–´ë µë‹¤. í•˜ì§€ë§Œ ì´ transformerë¼ëŠ” Attentionì— ê¸°ë°˜í•œ ëª¨ë¸ì€ inputê³¼ outputì˜ global dependencyë¥¼ ë°”ë¡œ ë½‘ì•„ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë³‘ë ¬í™”í•˜ê¸° ì¢‹ë‹¤. ë”°ë¼ì„œ sotaì¸ ëª¨ë¸ì„ P100 8ëŒ€ë¡œ 12ì‹œê°„ë§Œì— ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆì—ˆë‹¤. sequence-aligned RNNì—†ì´ ì™„ì „íˆ self-attention (intra attention)ë§Œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì´ë‹¤.

## 3. Model Architecture

{% include image.html url="/images/2019-06-09-transformer/1.png" description="Transformer architecture" %}

ìš°ì„  Encoder-decoder structureë¥¼ ê°€ì§€ê³  ìˆë‹¤. í•˜ì§€ë§Œ stacked self-attentionì„ ì‚¬ìš©í•˜ê³ , point-wise feed forward networkë¥¼ ì‚¬ìš©í•œë‹¤.

### Encoder

Encoderì˜ Layer í•˜ë‚˜ëŠ” ë‘ ê°œì˜ sublayerë¡œ ë˜ì–´ ìˆìœ¼ë©°, ì²«ë²ˆì§¸ëŠ” multi-head self-attention mechanismì„ ê°€ì§€ê³  ìˆë‹¤. ë‘ë²ˆì§¸ëŠ” position-wise fully-connected feed-forward networkë¥¼ ì‚¬ìš©í•œë‹¤. residual connectionì„ ì‚¬ìš©í•œ ê²ƒì„ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë‹¤. ë…¼ë¬¸ì—ì„œ ì„¤ëª…í•˜ê¸¸ í•˜ë‚˜ì˜ sublayerë¥¼ $$\text{LayerNorm}(x + \text{SubLayer}(x))$$ë¡œ ë³´ë¼ê³  í•œë‹¤. ì´ëŸ° layer í•˜ë‚˜ë¥¼ 6ê°œë¥¼ ìŒ“ì•˜ë‹¤.

### Decoder

ì—¬ê¸°ë‹¤ê°€ ë‘ê°œì˜ sublayerê°€ ë” ìˆë‹¤.
