---
layout: post
title: "ğŸ“ƒ TinyBERT: Distilling BERT For Natual Language Understanding ë¦¬ë·°"
tags:
  - paper
  - nlp
---

TinyBERTëŠ” Under Review ìƒíƒœì¸ ë…¼ë¬¸ì´ê³ , í™”ì›¨ì´ Noah's Ark Labì—ì„œ ë‚˜ì˜¨ ë…¼ë¬¸ì´ë‹¤. ì½”ë“œëŠ” [GitHub huawei-noah/Pretrained-Language-Model/TinyBERT](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT)ì— ìˆë‹¤. arxiv ë§í¬ëŠ” [https://arxiv.org/abs/1909.10351](https://arxiv.org/abs/1909.10351)ì´ë‹¤.

ì¶”ë¡  ê°€ì†í™”ì™€ model size ì••ì¶•ì„ ìœ„í•´ KDë¥¼ ì ìš©í•œ ë…¼ë¬¸ìœ¼ë¡œ teacher BERT ëª¨ë¸ì—ì„œ stduent BERT ëª¨ë¸ìœ¼ë¡œ Distillationì„ ì‹œë„í•˜ëŠ” ë…¼ë¬¸ì´ë‹¤. 96% ì´ìƒì˜ ì„±ëŠ¥ì„ ë³´ì¡´í•¨ê³¼ ë™ì‹œì—, 7.5x ì‘ê³  ì¶”ë¡ ì´ 9.4x ë¹ ë¥¸ ëª¨ë¸ì´ë‹¤.

## 1 Introduction

* ë³´í†µ Compressionì€ quantization, pruning, knowledge distillation ë“±ì„ ì‹œë„
* ê·¸ ì¤‘ì—ì„œë„ KDì— ì§‘ì¤‘í•œ ë…¼ë¬¸ì´ë©°, lossë¥¼ ì–´ë–»ê²Œ ë””ìì¸í•˜ëŠëƒì— ì‹ ê²½ì„ ë§ì´ ì¼ë‹¤ê³  í•¨
* ì•„ë˜ representationsì„ lossë¥¼ êµ¬í•˜ê¸° ìœ„í•´ ì´ìš©
  * output of embedding layer
  * hidden states and attention matrices
  * logit output of prediction layer
* main contributions
  * transformer distillation ë°©ì‹ì„ ìƒˆë¡­ê²Œ ì œì•ˆ
  * two-stage distillation ë°©ì‹ì„ ì‚¬ìš©
  * teacher modelì˜ 96%ì˜ ì„±ëŠ¥ì„ ë³´ì¡´

## 2 Preliminaries

Backgroundë¼ íŒ¨ìŠ¤

## 3 Method

### 3.1 Transformer Distillation

{%include image.html url="/images/2020-05-01-tiny-bert/fig1.png" class='noshadow' %}

#### Problem Formulation

* Teacherê°€ Nê°œì˜ Transformer Layer
* Studentê°€ Mê°œì˜ Transformer Layer
* embedding layerì˜ outputì€ layer 0ìœ¼ë¡œ ê°„ì£¼
* Distillationì€ ì•„ë˜ì‹ì„ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ê°„ë‹¤.

  $$l_{model} = \sum^{M+ 1}_{m = 0} \lambda_{m}l_{layer}(S_m, T_{g(m)})$$

#### Transformer-layer distillation

* Loss for attention mechanism

  $$l_{attn} = \frac 1 h sum^h_{i = 1} \text{MSE}(\textbf A_i^S, \textbf A_i^T )$$

* íŠ¹ì´í•œ ì ì€ softmaxë¥¼ íƒ€ê¸° ì „ì˜ matrixë¥¼ loss functionì˜ inputìœ¼ë¡œ ë„£ëŠ” ê²ƒì¸ë°, ì´ê²Œ ìˆ˜ë ´ì´ ë” ë¹ ë¥´ê²Œ ë˜ì—ˆë‹¤ê³  í•œë‹¤.
  * ì´ ì ì— ëŒ€í•´ì„œ ì˜ê²¬ì„ ë¶™ì´ìë©´, softmaxë¥¼ íƒ€ê³  ë‚˜ë©´ ë‚®ì€ attention ê°’ë“¤ì— ëŒ€í•œ ì •ë³´ê°€ ë§ì´ ì†ì‹¤ë˜ëŠ” í¸ì´ë¼ ê·¸ë ‡ì§€ ì•Šë‚˜ ì‹¶ë‹¤.
* Loss for hidden states

  $$l_{hidn} = \text{MSE} (\textbf{H} ^S \cdot \textbf W_h, \textbf{H}^T)$$

* $$\textbf W_h$$ëŠ” learnable linear transformationì´ë‹¤. tinyBERTì—ì„œëŠ” hidden sizeë„ ì¤„ì´ê³  ì‹¶ê¸° ë–„ë¬¸ì— ì‚¬ìš©í•˜ëŠ” ê°’ì´ë‹¤.

#### Embedding Layer Distillation

* Loss for embedding layer

  $$l_{embd} = \text{MSE} (\textbf{E} ^S \cdot \textbf W_e, \textbf{E}^T)$$

#### Prediction Layer Distillation

* Loss for Prediction Layer

  $$l_{pred} = -\text{softmax}(z^T) \cdot \text{log_softmax}(\frac {Z^S} t)$$

* prediction layerì—ì„œëŠ” student modelì˜ logitì— íŒ¨ë„í‹°ë¥¼ ì£¼ì—ˆì§€ë§Œ, ì‹¤ì œ ì‹¤í—˜ì—ì„œëŠ” t = 1ì´ ì œì¼ ì¢‹ì•˜ë‹¤ê³  í•œë‹¤.

### 3.1 TinyBERT Learning

{%include image.html url="/images/2020-05-01-tiny-bert/fig2.png" class='noshadow' %}

* general-distillationê³¼ task specific distillationì„ ì‹œë„í•´ë´„
* general distillationì€ fine-tuningí•˜ì§€ ì•Šì€ ì› Bert ëª¨ë¸ì— ëŒ€í•´ì„œ Prediction Layer Distillationì„ ì œì™¸í•˜ê³  ì§„í–‰í•˜ëŠ” Stageì´ë‹¤.
* task specific istillationì€ data augmentationì„ ì ìš©í•˜ì—¬ distillationì„ ìˆ˜í–‰í•œë‹¤. data augmentationì˜ ìƒì„¸í•œ ì ˆì°¨ëŠ” ë…¼ë¬¸ Appendix Aì— ìˆë‹¤.

## 4 Experiments

* tiny bert modelì„ M=4, d=312, intermeidate_size=1200, h=12ë¡œ ì¡ì•„ì„œ ì§„í–‰í–ˆë‹¤.

### 4.2 Experimental Results on GLUE

{%include image.html url="/images/2020-05-01-tiny-bert/fig3.png" class='noshadow' %}

* ì¼ë‹¨ BERT small ë³´ë‹¤ëŠ” í›¨ì”¬ ì˜í•¨
* ê¸°ì¡´ KDë³´ë‹¤ë„ ì˜í•¨
* í•˜ì§€ë§Œ CoLAì™€ ê°™ì€ íƒœìŠ¤í¬ëŠ” ë§ì´ ì–´ë ¤ì›€..

### 4.3 Effects of Model Size

{%include image.html url="/images/2020-05-01-tiny-bert/fig4.png" class='noshadow' %}

* ê·¸ë˜ë„ ë” í° ëª¨ë¸ì´ ì˜í•œë‹¤ëŠ” ê°™ë‹¤
* ê·¸ë˜ë„ 4layerê°€ 6layerë³´ë‹¤ ë” ì˜í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì€ ë†€ë¼ìš´ ì 

### 4.4 Ablation Studies

{%include image.html url="/images/2020-05-01-tiny-bert/fig5.png" class='noshadow' %}

ì™œ 4ê°œ íƒœìŠ¤í¬ì—ë§Œ í–ˆì§€...?

* DA=Data Augmentation, TD=Task-specific Distillation, GD=General Distillation
* ê·¸ë˜ë„ ì¼ë‹¨ 4ê°œì˜ íƒœìŠ¤í¬ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” ì ì€ ìƒê°ë³´ë‹¤ DAê°€ ì˜í–¥ì´ í¬ê³ , TDëŠ” ë‹¹ì—°íˆ ì˜í–¥ì´ í¬ë‹¤.

### 4.5 Effect of Mapping Function

* Mapping Function = tinyBERT hidden layer lossë¨¹ì¼ ë•Œ ì–´ë–¤ ë ˆì´ì–´ë¥¼ ì–´ë–¤ ë ˆì´ì–´ì™€ loss ê³„ì‚°í•  ì§€
* uniform strategyê°€ ê½¤ í° ê²©ì°¨ë¡œ ì´ê¸´ë‹¤.

## 5 Conclusion

* Large ëª¨ë¸ë“¤ì— ëŒ€í•´ì„œë„ í•´ë³´ê³  ì‹¶ë‹¤
* joint learning of distillation, quantization/pruningë„ ë‹¤ë¥¸ ë°©ë²•ì´ ë  ìˆ˜ ìˆë‹¤.

---

squadê°€ ì•ˆë˜ì–´ ìˆê¸¸ë˜ ì°¾ì•„ë³´ë‹ˆ Appendixì— ìˆë‹¤. ê½¤ ì˜ëœ ê²ƒ ê°™ì€ë° ì™œ ì•ˆë„£ì–´ë†“ì•˜ì„ê¹Œ??

{%include image.html url="/images/2020-05-01-tiny-bert/fig6.png" class='noshadow' %}
