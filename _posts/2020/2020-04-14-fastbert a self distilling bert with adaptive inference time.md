---
layout: post
title: "ğŸ“ƒ FastBERT: a Self-distilling BERT with Adaptive Inference Time ë¦¬ë·°"
tags:
  - paper
  - nlp
---

ì´ ë…¼ë¬¸ ì—­ì‹œ BERTê°€ ë„ˆë¬´ ì„œë¹™í•˜ê¸° í° ëª¨ë¸ì´ë¼ì„œ fine tuning ì‹œì— self distillationì„ ì ìš©í•´ë³¸ ê²ƒì´ë‹¤. 2019 Tencent Rhino-Bird Elite Training Programì—ì„œ í€ë”©ë°›ì•„ì„œ ì‘ì„±í•œ ê²ƒì´ë‹¤. arxiv ë§í¬ëŠ” [https://arxiv.org/abs/2004.02178](https://arxiv.org/abs/2004.02178)ì´ë‹¤.

## 1. Introduction

* ìµœê·¼ 2ë…„ì •ë„ì— BERT, GPT, XLNETì´ ë‚˜ì™”ì§€ë§Œ, accëŠ” ì˜¬ë ¤ë„ ë„ˆë¬´ ëŠë¦¬ë‹¤.
* ê·¸ë˜ì„œ speed-accuracy balanceë¥¼ ë§ì¶”ê¸° ìœ„í•´ quantization, weight pruning, knowledge distillationë“¤ì´ ì ìš©ë˜ê³  ìˆëŠ” ì¤‘ì´ë‹¤.
* sample wise adaptive mechanismì„ ì ìš©í•œ FastBERTë¥¼ ì œì•ˆ
  * ì‹¤í—˜ì€ Chinese, English NLP Taskì— ëŒ€í•´ ê°ê° 6ê°œì”© ì‹¤í–‰í–ˆë‹¤.
  * main contribution
    * speed tunable BERT model ì œì•ˆ.
    * sample-wise adaptive mechanism, self distillation mechanismì„ ì œì•ˆ
    * ì½”ë“œëŠ” ë…¼ë¬¸ publishëœ ë’¤ì— [autoliuweijie/FastBERT](https://github.com/autoliuweijie/FastBERT)ì— ê³µê°œ (ì•„ë§ˆ ì—‘ì…‰ë˜ê³ ..?)

## 2. Related Work

* BERT-baseëŠ” 110M íŒŒë¼ë¯¸í„° & Transformer block 12ê°œ ìŒ“ì€ êµ¬ì¡°
  * ë„ˆë¬´ ëŠë¦¬ëŒ€ìš”
* Knowledge Distillation
  * PKD-BERT - incremental extraction process
  * TinyBERT - two stage learning
  * DistilBert - triple loss ë¡œ distillation
* Adaptive inference
  * [(Graves, 2016)](https://arxiv.org/abs/1603.08983) - token-wise, patch-wiseë¡œ ê°ê°ì˜ í† í°ì— recurrent step ë„ì…
  * [(Figurnov et al.)](http://openaccess.thecvf.com/content_cvpr_2017/papers/Figurnov_Spatially_Adaptive_Computation_CVPR_2017_paper.pdf) - CVì—ì„œ ë‹¤ì´ë‚˜ë¯¹í•˜ê²Œ ê³„ì‚°í•˜ëŠ” ë ˆì´ì–´ ì¡°ì •

## 3. Methodology

### 3.1. Model Architecture

{% include image.html class="noshadow" url="/images/2020-04-14-fastbert/fig1.png" %}

* Backbone + Branchë¡œ êµ¬ì„±
  * Backboneì€ ê·¸ëƒ¥ BERT
  * BranchëŠ” ê° transformer encoder block ê²°ê³¼ì—ì„œ ë”°ì˜¤ëŠ” ë ˆì´ì–´

#### 3.1.1. Backbone

* ì„¸ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±
  * Embedding Layer
  * Transformer Encoder Layer
  * Teacher Classifier

#### 3.1.2. Branches

* ì—¬ëŸ¬ê°œì˜ Branchê°€ ì¡´ì¬
* ê° Transformer Layerê°€ 0ë¶€í„° ì‹œì‘í•˜ëŠ” ìˆ«ìë¡œ ë²ˆí˜¸ê°€ ë¶€ì—¬ë˜ê³ , ê·¸ ë ˆì•„ì–´ì˜ outputì´ $$h_i$$ë¼ í•  ë•Œ ië²ˆì§¸ Studentì˜ classification ê²°ê³¼:
  * $$p_{s_i} = \text{Student_Classifier}_i(h_i)$$

### 3.2. Model training

* ì„¸ê°€ì§€ë¡œ êµ¬ì„±
  * Major backbone pre-training
  * Entire backbone fine-tuning
  * Self distillation for student classifiers

#### 3.2.1. Pre-training

* ê·¸ëƒ¥ Pretrainí•œë‹¤.
  * BERT-likeí•œê±´ ì“¸ ìˆ˜ ìˆì„ ë“¯?
    * BERT-WWM, RoBERTa, ERNIE

#### 3.2.2. Fine-tuning for backbone

* Backbone + teacher classifier í•™ìŠµ

#### 3.2.3. Self-distillation for branch

* ì´ì œ Student classifier í•™ìŠµ
* ê° $$p_s$$ - $$p_t$$ (teacher) ì‚¬ì´ì˜ lossë¥¼ ì•„ë˜ì²˜ëŸ¼ ê³„ì‚° (KLL Divergence)

  $$ D_{KL}(p_s, p_t) = \sum^N_{i=1} p_s(i) \log \frac {p_s(i)} {p_t(j)} $$

* Layerê°€ Lê°œ ìˆìœ¼ë©´ L-1ê°œë§Œí¼ student classifierê°€ ìˆê¸° ë–„ë¬¸ì— total lossëŠ” ì•„ë˜ì²˜ëŸ¼ ê³„ì‚°

  $$ \text{Loss} (p_{s_0}, ..., p_{s_{L - 2}, p_t}) = \sum ^ {L-2} _ {i = 0} D_{KL} (p_{s_i}, p_t)$$

### 3.3. Adaptive inference

* ê°€ì„¤ 1. Uncertaintyê°€ ë‚®ì„ìˆ˜ë¡ Accuracyê°€ ë†’ë‹¤.
* ì •ì˜ 1. Speed: high, low uncertaintyë¥¼ êµ¬ë³„í•˜ëŠ” threshold
  * uncertainty thresholdê°€ ë‚®ì„ìˆ˜ë¡ low uncertaintyë¥¼ ìš”êµ¬í•˜ê¸° ë•Œë¬¸ì— ë” ëŠë¦° ëª¨ë¸ì´ ë‚˜ì˜¨ë‹¤.
* ê°ê°ì˜ Uncertaintyë¥¼ ê³„ì‚° (N -> # of labeled classes)

  $$\text{Uncertainty} = \frac {\sum^N_{i=1} p_s(i) \log p_s(i) } {\log \frac 1 N}$$

* ì ì ˆí•œ uncertaintyë¥¼ ì–»ì„ ë•Œê¹Œì§€ higher layerë¡œ ê°„ë‹¤.
* Transformer í•œë²ˆ ë” ì—°ì‚°í•˜ëŠ” ê²ƒë³´ë‹¤ classifier + uncertainty ê³„ì‚°ì´ ë” ë¹ ë¥´ë‹ˆ ê·¸ê±° ê³„ì‚°í•´ì„œ ë¯¸ë¦¬ ê²°ê³¼ ë‚´ê² ë‹¤ëŠ” ì „ëµ

## 4. Experimental results

{% include image.html class="noshadow" url="/images/2020-04-14-fastbert/fig2.png" %}

### 4.1. FLOPs analysis

* ìƒê°ë³´ë‹¤ ì¢‹ì€ ê²°ê³¼
* íŠ¹íˆ Dbpediaê°™ì´ ì‰¬ìš´ íƒœìŠ¤í¬ì˜ ê²½ìš°ì—ëŠ” speedë¥¼ 0.1ë§Œ ì¤˜ë„ êµ‰ì¥íˆ ë¹ ë¥¸ ì†ë„ë¥¼ ë‚´ë©´ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.
* ì„±ëŠ¥ì„ ì„ì˜ë¡œ ì¡°ì •í•˜ë©´ì„œ ì†ë„-ì •í™•ë„ë¥¼ ê°€ëŠ í•  ìˆ˜ ìˆëŠ” ê²ƒì´ ì¢‹ì€ ì 
* í‘œì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ BERT, DistilBERTë¥¼ baselineìœ¼ë¡œ ì¡ìŒ

### 4.3. Performance comparison

{% include image.html class="noshadow" url="/images/2020-04-14-fastbert/fig3.png" %}

* Speed - Acc ê·¸ë˜í”„ë¥¼ ë³´ë©´ ì ì •í•œ ì„ê³„ì¹˜ë§Œ ì˜ ì¡ìœ¼ë©´ ì›¬ë§Œí•œ fine tuningë‹¤ ê´œì°®ì„ ë“¯

### 4.4. LUHA hypothesis verification

* ì•„ê¹Œ ê°€ì„¤ ê²€ì¦í•˜ëŠ” ê²ƒì¸ë°, ì¶©ë¶„íˆ ì‹¤í—˜ì ìœ¼ë¡œ ì˜ ì¦ëª…ëœ ê²ƒìœ¼ë¡œ ë³´ì„

{% include image.html class="noshadow" url="/images/2020-04-14-fastbert/fig4.png" %}

* ê·¼ë° ê·¸ëŸ¼ Studentë§Œ ì“°ëŠ” ê²ƒìœ¼ë¡œ Teacherë³´ë‹¤ ê´œì°®ì€ ì„±ëŠ¥ ë‚¼ ìˆ˜ ìˆì§€ ì•Šë‚˜? ì‹¶ì€ë° ì•„ë‹˜. ì•„ë˜ë¥¼ ì˜ ë³´ì

{% include image.html class="noshadow" url="/images/2020-04-14-fastbert/fig5.png" %}

### 4.5. In-depth study

* ì•„ë˜ ì„¸ê°œ in-depth analysis ì§„í–‰í•¨
  * the distribution of exit layer
  * the distribution of sample uncertainty
  * the convergence during self-distillation

#### 4.5.1. Layer distribution

{% include image.html class="noshadow" url="/images/2020-04-14-fastbert/fig6.png" %}

* ìœ„ í‘œë¥¼ ë³´ë©´ Book review datasetì—ì„œ speed 0.8, 0.5, 0.3ì„ ì„¤ì •í–ˆì„ ë•Œì˜ ê°ê° exit layerì˜ distribution
  * 0.8ì„ ì„¤ì •í•  ê²½ìš° í‰ê·  1.92ê°œì˜ ë ˆì´ì–´ë§Œ íƒ€ê³ ë„ ì˜ ë™ì‘í•œë‹¤.
  * ë˜í•œ 61% ì •ë„ëŠ” í•˜ë‚˜ì˜ ë ˆì´ì–´ë§Œ íƒ€ë„ ì˜ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤.

#### 4.5.2. Uncertainty distribution

{% include image.html class="noshadow" url="/images/2020-04-14-fastbert/fig5.png" %}

* ë‹¤ì‹œ ìœ„ ì‚¬ì§„ì„ ë³´ë©´ Uncertainty distributionì„ ë³¼ ìˆ˜ ìˆë‹¤
  * high-layerê°€ low-layerë³´ë‹¤ decisive(ê²°ì •ì ??)ì´ë‹¤.

#### 4.5.3. Convergence of self-distillation

{% include image.html class="noshadow" url="/images/2020-04-14-fastbert/fig7.png" %}

* ìœ„ ì‚¬ì§„ì„ ë³´ë©´ accëŠ” fine-tuning ë™ì•ˆ ì˜¬ë¼ê°€ê³  self distillation ë™ì•ˆ ë§ì´ ë–¨ì–´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

### 4.6. Ablation study

{% include image.html class="noshadow" url="/images/2020-04-14-fastbert/fig8.png" %}

* ìœ„ ì‚¬ì§„ì˜ ablation studyì˜ ê²°ê³¼ì´ë‹¤.
  * ì•„ë§ˆ Yelp.Pì˜ FastBERT - speed=0.2ì˜ FLOPSëŠ” ì˜¤íƒ€ì¼ë“¯...
  * self-distillation + adaptive inferenceê°€ ì œì¼ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ì—ˆë‹¤.
  * **ë…¼ë¬¸ì´ë‘ ì¡°ê¸ˆ ë‹¤ë¥¸ ì˜ê²¬ì´ê¸´ í•˜ì§€ë§Œ, ë„ˆë¬´ ì‰¬ìš´ íƒœìŠ¤í¬ì˜ ê²½ìš°ì—ëŠ” self distillation ì•ˆí•´ë„ ë˜ì§€ ì•Šì„ê¹Œ??**
    * Yelp.Pì—ì„œ without self-distillationì´ í•©ë¦¬ì ì¸ acc - speed up ë°¸ëŸ°ìŠ¤ì¸ë“¯ í•˜ë‹¤.

## 5. Conclusion

ê·¸ëƒ¥ ì •ë¦¬í•œ ì„¹ì…˜

## 6. Future work

* linearizing the Speed-Speedup curve
* extending this approach to other pre-training architectures (such as XLNET, ELMO)
* applying FastBERT on a wider range of NLP tasks
