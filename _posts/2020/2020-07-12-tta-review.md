---
layout: post
title: ğŸ“ƒ Fast and Accurate Deep Bidirectional Language Representation for Unsupervised Learning ë¦¬ë·°
tags:
  - paper
---

ì–¼ë§ˆ ì „ [TensorFlow Koreaì— ì €ì ë¶„ì´ ì§ì ‘ ì„¤ëª…ì„ ê°„ëµí•˜ê²Œ ë‹¬ì•„ì£¼ì…”ì„œ](https://www.facebook.com/groups/TensorFlowKR/permalink/1239207109753678/) ì½ì–´ë³¸ ë…¼ë¬¸ì´ë‹¤. ACL 2020 ë°œí‘œëœ ë…¼ë¬¸ì´ê³ , Abstractì— Similarity taskì—ëŠ” BERT-based ëª¨ë¸ì— ë¹„í•´ 12ë°°ì •ë„ ë¹ ë¥¸ ì†ë„ë¥¼ ê°€ì§€ë©´ì„œë„ ê´œì°®ì€ ì„±ëŠ¥ì„ ê°€ì§„ë‹¤ê³  í•œë‹¤. ìµœê·¼ Similarity Taskê°€ í•„ìš”í•´ì§„ ì¼ì´ ìˆì–´ì„œ ë¦¬ë·°í•´ë³´ì•˜ë‹¤.

ê°„ë‹¨í•˜ê²Œ ë…¼ë¬¸ì—ì„œ í•„ìš”í–ˆë˜ ì ë§Œ ì ì–´ë³¸ë‹¤.

## Introduction

* "Can we construct a deep bidirectional language model with a minimal inference time while maintaining the accuracy of BERT?" -> ì´ ë…¼ë¬¸ì—ì„œ ì£¼ëª©í•˜ê³  ì‹¶ì—ˆë˜ ì£¼ì œ
* ê·¸ë˜ì„œ transformerë¥¼ ì‚¬ìš©í•´ì„œ auto encodingì„ í–ˆëŠ”ë°, ì´ ë•Œ inputì„ ë³µì‚¬í•´ì„œ ê·¸ëƒ¥ outputì— ë‚¼ ìˆ˜ ìˆìœ¼ë‹ˆ ë‘ê°€ì§€ ë°©ë²•ì„ ì œì‹œí–ˆë‹¤.
  * diagonal masking
  * input isolation

## Related works

íŒ¨ìŠ¤

## Language Model Baselines

* speed baseline: Unidirectional language model
* performance baseline: bidirectional language model

## Proposed Method

### Transformer based Text Autoencoder

#### Diagonal Masking

* scaled dot productëŠ” self-unknownì´ ì˜ ì•ˆë¨
* transformer layer outputì˜ ê° positionì˜ ê°’ì€ $$Q$$ì™€ $$K$$ì—ì„œ ë‚˜ì˜¨ attention weightì™€ $$V$$ì˜ ë‹¤ë¥¸ í¬ì§€ì…˜ì˜ ê°€ì¤‘í•©ì´ ë˜ë„ë¡í•œë‹¤.
  * ì¸ë° ê·¸ëƒ¥ attention maskingí•  ë•Œ Identity Matrixë¥¼ ì¶”ê°€í•´ì„œ maskingí•˜ëŠ” ê²ƒìœ¼ë¡œ ì´í•´í•˜ë©´ ë  ê²ƒ ê°™ë‹¤.

{% include image.html url="/images/2020/07-12-tta/fig1.png" class='noshadow' width=70 %}

* ê·¼ë° ì´ê±° í•´ë„ residual connection ìˆìœ¼ë©´ ì†Œìš©ì—†ìŒ

#### Input isolation

* K, V ì™€ Që¥¼ ë¶„ë¦¬í•´ì„œ ë„£ì–´ì¤€ë‹¤.

{% include image.html url="/images/2020/07-12-tta/fig2.png" class='noshadow' width=70  %}

## Experiments

ë‹¤ë¥¸ ê²°ê³¼ë³´ë‹¤ Semantic Textual Similarityë¥¼ ìœ„ì£¼ë¡œ ë´„

* BERT Finetunining ì—†ì´ í•œ ê²ƒ ê°™ì€ë°, STS-B ê¸°ì¤€ìœ¼ë¡œ BERTë³´ë‹¤ ë†’ê²Œ ë‚˜ì˜¨ë‹¤.
  * í•˜ì§€ë§Œ Sentence BERT (Reimers and Gurevych, 2019)ë…¼ë¬¸ì— ê¸°ìˆ ëœ ìŠ¤ì½”ì–´ì™€ ë§ì´ ì°¨ì´ë‚˜ëŠ” ê²ƒì„ ë³´ë©´ ì‹¤ì œ BERTë¥¼ ì˜ ì´ìš©í•œ ê²ƒê³¼ëŠ” ì°¨ì´ê°€ ìˆì–´ ë³´ì¸ë‹¤.
  * Transformerë¥¼ ìˆ˜ì •í–ˆë‹¤ê³  í•˜ë”ë¼ë„ 3ë ˆì´ì–´ë§Œ ì‚¬ìš©í–ˆê¸° ë–„ë¬¸ì— ë‹¹ì—°í•œ ê²ƒìœ¼ë¡œ ë³´ì´ê¸°ë„ í•œë‹¤.

{% include image.html url="/images/2020/07-12-tta/fig3.png" class='noshadow' width=50 %}

{% include image.html url="/images/2020/07-12-tta/fig4.png" class='noshadow' width=50 %}

---

ê·¸ë˜ë„ ë¹ ë¥´ê²Œ ë½‘ì•„ë‚´ê³  ì‹¶ì€ ê²½ìš°ì—ëŠ” ê´œì°®ì€ê°€?? ì‹¶ê¸°ë„ í•˜ë‹¤.
