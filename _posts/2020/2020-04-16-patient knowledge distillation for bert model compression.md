---
layout: post
title: "ğŸ“ƒ Patient Knowledge Distillation for BERT Model Compression ë¦¬ë·°"
tags:
  - paper
  - nlp
---

EMNLP 2019ì— Acceptëœ ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ì—ì„œ ë‚˜ì˜¨ PKD (Patient Knowledge Distillation) ë°©ì‹ì˜ Model Compression ë…¼ë¬¸ì´ë‹¤. arxiv ë§í¬ëŠ” [https://arxiv.org/abs/1908.09355](https://arxiv.org/abs/1908.09355)ì´ê³  ì½”ë“œëŠ” [GitHub - intersun/PKD-for-BERT-Model-Compression](https://github.com/intersun/PKD-for-BERT-Model-Compression)ì— ìˆë‹¤.

PKDëŠ” ë‘ ì „ëµì„ ì·¨í•˜ëŠ”ë°, 1. PKD-Last: learning from last K layers, 2: PKD-Skip: learning from every K layersì´ë‹¤. ìœ„ì˜ ì „ëµì„ ì·¨í•´ì„œ model accuracy í•˜ë½ ì—†ì´ Trainingì„ êµ‰ì¥íˆ íš¨ìœ¨ì ìœ¼ë¡œ í•  ìˆ˜ ìˆì—ˆë‹¤ê³  í•œë‹¤.

## 1. Introduction

* teacherë¥¼ lightweight studnet modelë¡œ PKD ë°©ì‹ì„ ì´ìš©í•´ì„œ ë§Œë“¤ì–´ë³¸ë‹¤.
* ê¸°ì¡´ KDì™€ ë‹¤ë¥´ê²Œ Patientë¥¼ ì ìš©í—€ë‹¤.
  * teacherì˜ last layerë§Œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë‹¤ë¥¸ ì „ ë ˆì´ì–´ë„ í•™ìŠµí•˜ë„ë¡ í–ˆë‹¤.

## 2. Related Work

### Language Model Pre-training

* íŒ¨ìŠ¤

### Model Compression & Knowledge Distillation

* Model Compressionì—ë§Œ ì§‘ì¤‘
  * compactí•˜ê²Œ ë§Œë“¤ê¸° [Han et al., 2016](https://arxiv.org/abs/1510.00149), [Cheng et al., 2015](https://arxiv.org/abs/1502.03436)
  * ì¶”ë¡  ê°€ì†í™” [Vetrov et al., 2017](https://arxiv.org/abs/1612.02297)
  * ëª¨ë¸ training ì‹œê°„ ì¤„ì´ê¸° [Huang et al., 2016](https://arxiv.org/abs/1603.09382)

## 3. Patient Knowledge Distillation

### 3.1. Distillation Objective

* Teacher: BERT, Student: BERT_kë¡œ í‘œê¸°, Bert baseëŠ” BERT_12, Bert largeëŠ” BERT_24
* Teacher í•™ìŠµ

  $$\hat \theta ^t = \text{arg}\min_\theta \sum_{i \in [N]} L^t_{CE} (x_i, y_i; [\theta_{BERT_{12}}, W])$$

  * t : teacher
  * \[N\] : Set {1,2,3, ... N}

* Output Probability from teacher

  $$\hat {y_i} = P^t (y_i\vert x_i) = softmax(\frac {W \cdot BERT_{12} (x_i; \hat \theta ^t)} T)$$

  * T: Temperature used in KD
* Student, Teacherì˜ output probabilityì˜ distance ê³„ì‚°

  $$L_{DS} = - \sum_{i \in [N]} \sum_{c \in C} [P^t (y_i = c \vert x_i;\hat \theta ^t) \cdot \log P^s (y_i = c |x_i ; \theta ^s)]$$

* ë¬¼ë¡  Teacherì˜ Soft Targetì„ í•™ìŠµí•˜ëŠ” ê²ƒë„ ì¢‹ì§€ë§Œ, hard targetë„ ë§ì¶°ì•¼ í•˜ë‹ˆ Task Specificì— ëŒ€í•´ì„œë„ Cross Entropyë¥¼ ì¶”ê°€í•œë‹¤.
* ë”°ë¼ì„œ ìµœì¢… objectiveëŠ” $$L_{KD} = (1 - \alpha) L^s_{CE} + \alpha L_{DS}$$ê°€ ëœë‹¤.

### 3.2. Patient Teacher for Model Compression

{% include image.html class="noshadow" url="/images/2020-04-16-pkd-bert/fig1.png" %}

* KD ì¤‘ studentê°€ overfittingí•  ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— patient mechanismì„ ë„£ì–´ì¤€ë‹¤.
* Teacher Modelì˜ ì¤‘ê°„ ë ˆì´ì–´ë“¤ì˜ `[CLS]` í† í° representationë“¤ì„ ê°€ì ¸ì˜¨ë‹¤.

  $$h_i = [h_{i_1}, h_{i_2}, ..., h_{i_k}] = BERT_k(x_i) \in \mathbb R^{k \times d}$$

* ê·¸ë ‡ê²Œ í•´ì„œ Stduentì˜ ê²°ê³¼ì™€ Mean-square lossë¥¼ ê³„ì‚°í•œë‹¤. $$L_{PT}$$
* final loss: $$L_{PKD} = (1 - \alpha) L^s_{CE} + \alpha L_{DS} + \beta L_{PT}$$

## 4. Experiments

### 4.1. Datasets

* GLUE

### 4.2. Baselines and Training Details

* Teacher ëª¨ë¸ë¡œ BERT_12ë¥¼ ê°ê° íƒœìŠ¤í¬ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ë‹¤ fine-tuningì‹œí‚´
* Student ëª¨ë¸ë¡œ BERT_3, BERT_6ì„ ì¤€ë¹„.
  * ì²« k ë ˆì´ì–´ë¡œ initialize
* Temperature ëŠ” {5, 10, 20}ì‚¬ì´ì—ì„œ ê³ ë¦„, $$\alpha$$ë„ {0.2, 0.5, 0.7} ì‚¬ì´ì—ì„œ ê³ ë¦„, $$\beta$$ëŠ” {10, 100, 500, 1000} ì‚¬ì´ì—ì„œ ê³ ë¦„

### 4.3. Experimental Results

{% include image.html class="noshadow" url="/images/2020-04-16-pkd-bert/fig2.png" %}

* ê·¸ëƒ¥ BERT_smallì„ Fine tuningí•œ ê²ƒë³´ë‹¤ í›¨ì”¬ ì˜ë‚˜ì˜¤ëŠ” ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, ê·¸ë˜ë„ MRPC ê°™ì€ íƒœìŠ¤í¬ë“¤ì€ ì¢€ ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤.
* ê·¸ë˜ë„ ì¢‹ì€ ì ì€ QQP, MNLI-m, MNLI-mm, QNLI ê°™ì€ íƒœìŠ¤í¬ëŠ” ì–¼ë§ˆ ë–¨ì–´ì§€ì§€ ì•Šì•˜ëŠ”ë°, ì´ íƒœìŠ¤í¬ë“¤ì˜ ê³µí†µì ì´ 60k ì´ìƒì˜ sampleì´ ìˆë‹¤ëŠ” ê²ƒë“¤ì´ê³ , training dataê°€ ë§ì„ ë•Œ ì˜ëœë‹¤ëŠ” ê²ƒì„ ì–´ëŠì •ë„ ì¦ëª…í•œ ì…ˆì´ë‹¤.

{% include image.html class="noshadow" url="/images/2020-04-16-pkd-bert/fig3.png" %}

* PKD-Lastì™€ PKD-Skipì„ ë¹„êµí•´ë³´ì•˜ì„ ë•ŒëŠ” PKD-Skipì´ ì¢€ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë³´ì¸ë‹¤.
  * ì•„ë§ˆ PKD-Skipì´ ì¡°ê¸ˆ ë” low-level ~ high-levelê¹Œì§€ diverseí•œ representationê³¼ richer semanticsì„ ì¡ì•„ë‚¼ ìˆ˜ ìˆì–´ì„œ ê·¸ëŸ° ê²ƒ ê°™ë‹¤ê³  í•œë‹¤.

### 4.4. Analysis of Model Efficiency

* Titan RTX GPUì—ì„œ batch 128, seq length 128, FP16ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í–ˆì„ ë•Œ ì•„ë˜ì™€ ê°™ì€ ê²°ê³¼ë¥¼ ë³´ì˜€ë‹¤ê³  ã…ë‚˜ë‹¤.

{% include image.html class="noshadow" url="/images/2020-04-16-pkd-bert/fig4.png" %}

### 4.5. Does a Better Teacher Help?

{% include image.html class="noshadow" url="/images/2020-04-16-pkd-bert/fig5.png" %}

* ê¼­ ê·¸ë ‡ì§€ë§Œì€ ì•Šë‹¤.
* BERT12ê°€ ë” ì˜ ê°€ë¥´ì¹  ë•Œë„ ë§ë‹¤.
* ì˜¤íˆë ¤ #3ì€ #1ë³´ë‹¤ ë‚˜ìœ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.
* í•˜ì§€ë§Œ, #3, #4ë¥¼ ë¹„êµí•´ë³¼ ë•Œ PKDëŠ” Teacher Modelì— ìƒê´€ì—†ì´ ì˜ë˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.

## 5. Conclusion

* Future works
  * Designing more sophisticated distance metrics for loss function
  * investigate Patient-KD in more complex settings such as multi-task learning and meta learning.
