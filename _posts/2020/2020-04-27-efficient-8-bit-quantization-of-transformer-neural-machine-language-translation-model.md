---
layout: post
title: "ğŸ“ƒ Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model ë¦¬ë·°"
tags:
  - paper
  - nlp
---

TensorFlow ìƒì—ì„œ FP32ë¥¼ INT8ë¡œ quantizationì„ í•´ë³´ëŠ” ë…¼ë¬¸ì´ë‹¤. 1.5ë°°ì˜ ì„±ëŠ¥ í–¥ìƒì„ ì–»ìœ¼ë©´ì„œ 0.5 BLEU score accuracyë§Œ ë–¨ì–´ì¡Œë‹¤ê³  í•œë‹¤. ë˜í•œ intel cpuì— ìµœì í™”ë¥¼ ì§„í–‰í–ˆë‹¤. arxiv ë§í¬ëŠ” [https://arxiv.org/abs/1906.00532](https://arxiv.org/abs/1906.00532)ì´ê³ , intelì—ì„œ ë‚˜ì˜¨ ë…¼ë¬¸ì´ë‹¤.

## 1. Introduction

* Contributions
  * Quantized a trained FP32 Transformer model to INT8 to achieve < 0.5 drop in state-of-the-art (SOTA) BLEU score.
  * Improve inference performance by:
    1. Optimizing quantized MatMuls for tensor shapes and sizes in the Transformer model
    1. Reducingoverheadduetoquantizationoperations in the Transformer model compute graph
    1. Optimizing input pipeline by ordering sentences by token length
    1. Implementing parallel execution of batches with increased inference throughput

## 2. Related work

íŒ¨ìŠ¤

## 3. Model Description

* TransformerëŠ” scaled dot product attention ì‚¬ìš©
* ì—¬ê¸°ì„œ softmax ì—°ì‚°ì´ ë¼ì—¬ìˆëŠ”ë° í•´ë‹¹ ì—°ì‚°ì„ quantizationí•˜ë©´ acc lossê°€ ë†’ì„ ê²ƒì´ ëª…í™•.
* layer normë„ ìˆëŠ”ë° ì´ ì—°ì‚°ì´ mean, varianceë¥¼ ì—°ì‚°í•˜ê¸° ë•Œë¬¸ì— ì´ê²ƒë„ í˜ë“¤ì§€ ì•Šì„ê¹Œ?

## 4. Quantization with accuracy

* $$scale = \frac {target} {max - min}$$, $$A_{quantized} = round((A_{float} - zero_{offset}) \cdot scale)$$
* ìœ„ì˜ ì‹ì„ ë”°ë¼ quantizationì„ ì§„í–‰í•˜ëŠ”ë° 8bitë¼ì„œ min ~ maxëŠ” ë‹¹ì—°íˆ 256ì˜ scaleì„ ê°€ì§€ê²Œ ëœë‹¤.

### 4.1. Na ÌˆÄ±ve Quantization

{%include image.html url="/images/2020-04-27-8-bit-transformer/fig1.png" class='noshadow' width=70 %}

* ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì§„í–‰í•  ë•Œ dequantizationí•˜ëŠ” ë°©ë²•: $$A_{dequantized} = (Max - Min) \cdot (A_{quantized} - zero_{offset})$$
* NMT íƒœìŠ¤í¬ì˜€ëŠ”ë°, Stop token ë‚´ë±‰ëŠ”ë° ì‹¤íŒ¨í•´ì„œ accê°€ ë§ì´ ë–¨ì–´ì ¸ë²„ë¦¼

### 4.2. KL-Divergence for optimal saturation thresholds

* ì´ê²Œ quantizationì´ ì–´ì°Œë˜ì—ˆë“  ì˜ ë§¤í•‘í•˜ëŠ” ê²ƒì´ ë¬¸ì œì´ë‹¤ë³´ë‹ˆê¹Œ representationì˜ ë²”ìœ„ë¥¼ ì ë‹¹íˆ ì˜ ì¤„ì´ê³  ëŠ˜ë¦¬ëŠ” ê²ƒì´ ì¤‘ìš”í•¨
* > This relies on the assumption that maintaining small differences between tensor values that are close together is more important than representing the absolute extreme values or the outliers. Ideally, the numerical distribution of values in the mapped INT8 tensor representations should be as close as possible to the distribution of values for FP32 tensors.
* ê·¸ë˜ì„œ KL Divergence ì‚¬ìš©í•¨
* ì•„ì´ë””ì–´ëŠ” ì—¬ê¸°ì„œ ì°¸ê³ í–ˆë‹¤ê³  í•¨ [8-bit Inference with TensorRT](http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)
* calibration dataë¡œ 600 random lengthì™€ 3003ê°œì˜ ë¬¸ì¥ì„ ì„ ì •í•¨
* min, max thresholdë¥¼ ì •í•˜ëŠ” ë°©ë²•ì„ ì„¸ê°€ì§€ í…ŒìŠ¤íŠ¸í•¨
  1. symmetricí•˜ê²Œ. "threshold_min = - threshold_max"
  2. ë…ë¦½ì ìœ¼ë¡œ ê°ê° ê³„ì‚°í•¨
  3. conjugateë¡œ ê³„ì‚°í•¨ ($$Threshold_{Max} = max(\vert Max \vert, \vert Min \vert)$$) ê·¸ë¦¬ê³  symmetricí•˜ê²Œ
* ê·¼ë° ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ì œì¼ ì¢‹ìŒ

{%include image.html url="/images/2020-04-27-8-bit-transformer/fig2.png" class='noshadow' width=50 %}

* ê²°êµ­ ì•„ë˜ì²˜ëŸ¼ quantization ì§„í–‰í•¨

{%include image.html url="/images/2020-04-27-8-bit-transformer/fig3.png" class='noshadow' width=50 %}

## 5. Improving Performance

**ì—¬ê¸°ì„œë¶€í„°ê°€ ì´ ë…¼ë¬¸ì—ì„œ ì œì¼ ì¬ë°Œë‹¤ê³  ìƒê°í•œ ë¶€ë¶„ì¸ë°, "ì‚¬ì‹¤ìƒ ì„±ëŠ¥ì„ ì´ê±¸ë¡œ ì˜¬ë¦°ê±° ì•„ëƒ??"ë¼ê³  ìƒê°ë“¤ ì •ë„ì´ë‹¤.**

* INT8ë¡œ ë³€í™˜í•˜ë ¤ëŠ” ì´ìœ :
* > INT8 MatMuls using VNNI provides a speed-up of 3.7X over FP32 MatMuls using AVX512.

{%include image.html url="/images/2020-04-27-8-bit-transformer/fig4.png" class='noshadow' %}

* MKLë¡œ TensorFlow Operationì§ì ‘ ì‘ì„±í•¨. (ì•„ë§ˆ Custom Opsì¸ë“¯?)
* TensorFlow 1.12ëŠ” GEMMLOWPë¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— INT8/VNNIë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤.
* ê·¸ë¦¬ê³  ë°ì´í„° ë³€í™˜ ê³¼ì •ë„ í•„ìš”í•´ì„œ íš¨ìœ¨ì ì´ì§€ ì•Šë‹¤.
* ê·¸ë˜ì„œ ì§ì ‘ ì‘ì„±í•´ë„ ì•ˆë¹¨ë¼ì„œ í™•ì¸í•´ë³´ë‹ˆê¹Œ MatMulì—ì„œ ìµœì í™” ì•ˆëœ ë¶€ë¶„ì´ ìˆì—ˆê³  ê·¸ ë¶€ë¶„ì„ ìµœì í™”í•¨

* ê·¸ ì™¸ì—ë„ ì•„ë˜ì²˜ëŸ¼ ìµœì í™”í•¨
  * GatherNDë¥¼ ìµœì í™”í–ˆëŠ”ë° ê·¸ ì´ìœ ëŠ” ì„±ëŠ¥ í–¥ìƒì´ ì•„ë‹ˆë¼ ë°ì´í„° í†µì‹ ì„ ë¹ ë¥´ê²Œí•˜ê¸° ìœ„í•´ì„œ ì§„í–‰í•¨. 32bitë³´ë‹¤ 8bitë‚˜ë¥´ëŠ”ê²Œ ì•½ 3.8xë°°ê°€ ë¹¨ëê¸° ë•Œë¬¸
  * input sentence sortingí•´ì„œ ì—°ì‚° ì§„í–‰í•¨
  * Quantization ì¤‘ì—ì„œ ë¶ˆí•„ìš”í•œ reshape ë“±ì˜ ì—°ì‚°ì„ ì œê±°í•¨
  * batchingì„ parallelë¡œ ì‘ì„±í•¨

## 6. Throughput Performance Results

* í™˜ê²½ ì…‹ì—…ì€ íŒ¨ìŠ¤
* ì„¤ì •ë§Œ ì˜í•´ë‘ë©´ ë³‘ë ¬ì—°ì‚°ì´ ì˜ ë˜ì–´ì„œ 4.5xê¹Œì§€ throughput í–¥ìƒë¨
* ê·¼ë° input pipeline ìµœì í™”í•œê²Œ fp32ë„ ìµœì í™”í•´ë²„ë ¤ì„œ ê²°êµ­ fp32ë³´ë‹¤ 1.5xì •ë„ ë¹ ë¥¸ ì—°ì‚°ì´ ë˜ì—ˆë‹¤

## 7. Conclusion

> We optimized the compute graph by reducing number of operations, improved kernels of key operations such as MatMuls and GatherNd, optimized order of sentences in the input pipeline and finally used parallel batching to achieve the highest throughput gains of 1.5X.

---

ê·¸ëƒ¥ "8-bitë¡œ ì—°ì‚°í•´ë„ ì˜ ëœë‹¤"ì™€ "ë” ìµœì í™” ê°€ëŠ¥í•œ ë¶€ë¶„ì´ ë§ë‹¤" ì •ë„ì˜ ë…¼ë¬¸ì¸ ê²ƒ ê°™ë‹¤. MKLë¡œ ìµœì í™”í•œ ë¶€ë¶„ì´ TF2ì— ì ìš©ê°€ëŠ¥í•œì§€ëŠ” ëª¨ë¥´ê² ë‹¤.
