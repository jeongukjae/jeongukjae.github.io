---
layout: post
title: "ğŸ“ƒ Layer Normalization ë¦¬ë·°"
tags:
  - paper
  - machine learning
---

Layer Normalizationì€ BERTì— ì“°ì´ëŠ” ê²ƒ ë•Œë¬¸ì— ì°¾ì•„ë³´ê²Œ ëœ ë…¼ë¬¸ì´ë‹¤. arxiv ë§í¬ëŠ” [https://arxiv.org/abs/1607.06450](https://arxiv.org/abs/1607.06450)ì´ë‹¤. trainingì‹œê°„ì„ ì¤„ì´ëŠ” ê²ƒì´ í° ê¸°ì—¬ì¸ë°, ì´ë¦„ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ neuronì˜ activityë¥¼ normalizeí•˜ëŠ” ê²ƒì´ë‹¤. Batch Normalizationë„ ë¹„ìŠ·í•œ ì—­í• ì„ í•  ìˆ˜ ìˆì§€ë§Œ Batch Normalizationì€ min-batchì— dependentí•œ ë¶€ë¶„ì´ ì¡´ì¬í•˜ê³  recurrent networkì—ëŠ” ì ìš©í•˜ê¸° ì–´ë µë‹¤.

AdamWë¥¼ ì°¾ì•„ë³´ë©´, weight decayì‹ì„ ë³¼ ìˆ˜ ìˆëŠ”ë° ê·¸ ì—­í• ê³¼ ë¹„ìŠ·í•˜ë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ë‹¤. Weight Decayë¥¼ í•˜ëŠ” ì´ìœ ë„ weightë¥¼ normalizeí•´ì£¼ê¸° ìœ„í•¨ì¸ë°, ì´ ë…¼ë¬¸ì€ weight ìì²´ë¥¼ normalizeí•˜ì§„ ì•Šì§€ë§Œ, neuronì˜ outputë“¤ì˜ Mean, Varianceë¥¼ ë§ì¶”ì–´ ì£¼ë©´ì„œ normalizeë¥¼ í•˜ê²Œ ëœë‹¤.

## 1, 2

* ê±´ë„ˆëœ€

## 3 Layer Normalization

{% include image.html url="/images/2020-05-01-layer-norm/fig1.png" class='noshadow' %}

* layer ë³„ë¡œ mean, vairanceë¥¼ êµ¬í•œ ë’¤ beta, gammaë¼ëŠ” learnable variableê³¼ í•¨ê»˜ recentering, rescalingí•´ì¤€ë‹¤.

## 5 Analysis

* ì´ë ‡ê²Œ í•˜ë©´ íŠ¹ì • ë„¤íŠ¸ì›Œí¬ì˜ $$W$$ì™€ $$W^\prime$$ì´ scaleë§Œ ë‹¤ë¥´ë‹¤ê³  í•´ë„ ì™„ì „íˆ ê°™ì€ outputì„ ë‚¼ ìˆ˜ ìˆë‹¤.
* weight normì´ ì»¤ì§€ë©´ learning rateê°€ ì‘ì•„ì§€ëŠ” íš¨ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.
  * ê·¸ë˜ì„œ implicití•˜ê²Œ early stoppingê³¼ ë¹„ìŠ·í•œ íš¨ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆê³ , convergenceê°€ ë” ì•ˆì •ì ìœ¼ë¡œ ëœë‹¤.

---

ì´ ë’¤ë¡œëŠ” ì „ë¶€ ì‹¤í—˜ ë‚´ìš©. ì „í›„ë¡œ riemannian metric, fisher information matrixì™€ ê´€ë ¨ëœ ì„¤ëª…ì´ ë„ˆë¬´ ì–´ë ¤ì›Œì„œ ë‹¤ìŒì— ë‹¤ì‹œ ë´ì•¼ê² ë‹¤.
