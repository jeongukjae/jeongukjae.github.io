---
layout: post
title: "ğŸ“ƒ Distilling the Knowledge in a Neural Network ë¦¬ë·°"
tags:
  - paper
---

êµ¬ê¸€ì—ì„œ Geoffrey Hinton, Oriol Vinyals, Jeff Deanì´ ì‘ì„±í•œ Distillation ê°œë…ì„ ì œì•ˆí•œ ë…¼ë¬¸ì´ë‹¤. arvix ë§í¬ëŠ” [https://arxiv.org/abs/1503.02531](https://arxiv.org/abs/1503.02531)ì´ê³ , NIPS 2014 ì›Œí¬ìƒµì— ë‚˜ì˜¨ ë…¼ë¬¸ì´ë‹¤.

## Abstract

* ëª¨ë¸ì„ ensembleí•˜ëŠ” ê²ƒì´ ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ëŠ” ê°„ë‹¨í•œ ë°©ë²• ì¤‘ í•˜ë‚˜ì§€ë§Œ ë„ˆë¬´ ì—°ì‚°ì´ ë¹„ì‹¸ê³  ë°°í¬í•˜ê¸° í˜ë“¤ë‹¤.
* ê·¸ë˜ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì••ì¶•í•˜ì—¬ ê°„ë‹¨í•œ ë‰´ëŸ´ ë„·ì— ì˜®ê²¨ì£¼ëŠ” ê²ƒì´ íš¨ê³¼ì ì¸ ë°©ë²•ì´ ë  ìˆ˜ ìˆë‹¤.

## 1 Introduction

* í° ëª¨ë¸ (ë…¼ë¬¸ì—ì„œëŠ” cumbersome modelì´ë¼ ë§í•œë‹¤)ì˜ knowledgeë¥¼ íš¨ê³¼ì ìœ¼ë¡œ transferí•˜ëŠ” ë°©ë²•ì€ í° ëª¨ë¸ì—ì„œ ë‚˜ì˜¨ class probabilitiesë¥¼ ë°”ë¡œ small modelì˜ target (soft target) ì´ìš©í•˜ëŠ” ê²ƒì´ë‹¤.
* ì´ê²ƒì´ ì™œ íš¨ê³¼ì ì¸ì§€ëŠ” ì•„ë˜ ì„¤ëª…ì„ ë³´ì

  MNISTìš©ìœ¼ë¡œ í•™ìŠµëœ í° ëª¨ë¸ì€ êµ‰ì¥íˆ ë†’ì€ ì •í™•ë„ë¡œ ìˆ«ìë“¤ì„ ë§ì¶œí…Œì§€ë§Œ, ì–´ëŠì •ë„ ë‹¤ë¥¸ í´ë˜ìŠ¤ì—ë„ probì„ ì¤€ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 2ë¥¼ ë§ì¶œ ë•Œ ë‹®ì€ ìˆ«ìì¸ 3ê³¼ 7ë„ ë‚®ì€ í™•ë¥ ì´ì§€ë§Œ ê°’ì„ ë¶€ì—¬í•  ê²ƒì´ë‹¤. ì´ ì •ë³´ë“¤ì€ êµ‰ì¥íˆ ì¤‘ìš”í•œ ì •ë³´ì¸ë°, dataì˜ structureì— ëŒ€í•œ ì •ë³´ê°€ ë“¤ì–´ìˆëŠ” ê°’ì´ê¸° ë•Œë¬¸ì´ë‹¤.

  * í•˜ì§€ë§Œ ê·¸ ê°’ë„ êµ‰ì¥íˆ ë‚®ì€ ê°’ì´ë¼, temperature ê°œë…ì„ ë„ì…í–ˆë‹¤. (ë‹¤ë¥¸ ê°’ì´ 0ì— ê°€ê¹Œìš°ë©´ hard targetì„ í•™ìŠµí•˜ëŠ” ê²ƒê³¼ ë‹¤ë¥¼ ê²ƒì´ ì—†ë‹¤.)

## 2 Distillation

* ë³´í†µì˜ softmax ì‹ê³¼ëŠ” ë‹¤ë¥´ê²Œ temperature ê°œë…ì„ ë„ì…í•œë‹¤. logit $$z_i$$ì— ëŒ€í•´ prob $$q_i$$ëŠ” ì•„ë˜ ì‹ì´ ëœë‹¤.

  $$q_i = \frac {\exp (z_i / T)} {\sum_j \exp (z_j / T)}$$

  * TëŠ” Temperatureì´ê³ , T=1ì´ë¼ë©´ ë³´í†µì˜ softmax ì‹ì´ë‹¤. Tê°€ ì»¤ì§€ë©´ í›¨ì”¬ softí•œ probability distributionì´ ë‚˜ì˜¨ë‹¤.
* lossëŠ” ë‘ê°€ì§€ë¥¼ ì£¼ê²Œ ë˜ëŠ”ë°,
  * ë†’ì€ Tì— ëŒ€í•´ì„œ distilled modelê³¼ cumbersome modelì˜ output ì‚¬ì´ì˜ cross entropy lossì™€
  * T=1ë¡œ ë‘ê³  hard labelê³¼ distilled modelì˜ output ì‚¬ì´ì˜ cross entropy lossë¥¼ ê³„ì‚°í•œë‹¤.
* í•˜ì§€ë§Œ ì²«ë²ˆì§¸ lossê°€ gradient ê³„ì‚° ì‹œ $$\frac 1 {T^2}$$ìœ¼ë¡œ scalingë˜ë¯€ë¡œ, í•´ë‹¹ lossì— weightë¥¼ ì£¼ëŠ” ê²ƒì´ ì¢‹ë‹¤. ê°ê°ì— $$T^2$$ë¥¼ ê³±í•´ì„œ ì ìš©í•´ì£¼ì. (ê²°êµ­ hard targetì€ ì•ˆê³±í•œë‹¤ëŠ” ë§ ì•„ë‹Œê°€..?)
  * softmax - cross entropy ì‹ ë¯¸ë¶„í•´ë³´ë‹ˆê¹Œ  $$\frac 1 {T^2}$$ìœ¼ë¡œ scalingëœë‹¤.
  * ì´ê²Œ hyper parameterë¥¼ ë³€ê²½í•˜ë”ë¼ë„ ê²°êµ­ sfot target, hard targetì˜ relative contributionì´ ì•ˆë°”ë€Œë„ë¡ í•´ì¤€ë‹¤.

### 2.1 Matching logits is a special case of distillation

* ë¨¼ì € Softmax - Cross Entropy ì‹ì˜ ë¯¸ë¶„ì€ [https://ratsgo.github.io/deep%20learning/2017/10/02/softmax/](https://ratsgo.github.io/deep%20learning/2017/10/02/softmax/)ë¥¼ ì°¸ê³ í•˜ì.

* Cross Entropy ê³„ì‚° (v_iëŠ” cumbersome modelì˜ ê²°ê³¼ logit)

  $$\frac {\partial C} {\partial z_i} = \frac 1 T (q_i - p_i) $$

  ì—¬ê¸°ì„œ temperatrueê°€ ì¶©ë¶„íˆ ë†’ë‹¤ë©´

  softmax ì‹ì˜ $$exp(z_i / T)$$ê°€ 0ì— ê°€ê¹Œì›Œì ¸ ê¸°ìš¸ê¸°ê°€ 1ì´ë¯€ë¡œ $$1 + z_i / T$$ë¡œ ê·¼ì‚¬ê°€ ê°€ëŠ¥í•˜ë‹¤.

  $$\frac {\partial C} {\partial z_i} \approx \frac 1 T (\frac {1 + z_i / T} {N + \sum_j z_j / T} - \frac {1 + v_i / T} {N + \sum_j v_j / T} )$$

  ì—¬ê¸°ì„œ logitì´ zero-meanì´ë¼ë©´ ì•„ë˜ì²˜ëŸ¼ ì „ê°œê°€ ëœë‹¤.

  $$\frac {\partial C} {\partial z_i} \approx \frac 1 {NT^2} (z_i - v_i)$$

* ê·¸ë˜ì„œ ë†’ì€ temperatureì—ì„œëŠ” distaillationì´ $$1/2(z_i - v_i)^2$$ì„ minimizeí•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.
  * ì–´ì°¨í”¼ gradient ê³„ì‚°í•  ë•Œ $$T^2$$ìœ¼ë¡œ scalingì„ í•´ì£¼ë‹ˆ $$T^2$$í•­ì´ ì‚¬ë¼ì§€ëŠ”ë°,
  * $$\frac {\partial C} {\partial z_i} $$ì„ ì ë¶„í•œ ê²ƒì´ lossì™€ ê°™ì•„ì•¼ í•˜ë‹ˆ $$(z_i - v_i)^2$$í•­ì„ minimizeí•´ì•¼ í›ˆë ¨ì´ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.
  * ì—¬ê¸°ì„œ ì•Œ ìˆ˜ ìˆëŠ” ê²ƒì€ ì ˆëŒ“ê°’ì´ í¬ê³  ìŒìˆ˜ì¸ logitsì€ ìœ ìš©í•œ ì •ë³´ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.
* ë‚®ì€ temperatureì—ì„œëŠ” negativeì— ì‹ ê²½ì„ ë§ì´ ì“°ì§€ ì•Šë„ë¡ í›ˆë ¨ì´ ëœë‹¤.
  * ë‚®ì€ temperatureì˜ ê²½ìš°ì—ëŠ” softmax ê°’ ìì²´ë¥¼ ë§ì¶”ë ¤í•˜ê¸° ë•Œë¬¸ì¸ê°€???
  * ê·¼ë° ì´ê²Œ logit ê°’ ìì²´ê°€ ì—„ì²­ noisyí•˜ê¸° ë•Œë¬¸ì— ì¢‹ì€ ì ì´ ë  ìˆ˜ ìˆëŠ” ìˆë‹¤.
* distilled modelì´ parent modelì˜ ì •ë³´ë¥¼ ë‹¤ ë‹´ê¸°ì— ë„ˆë¬´ ì‘ë‹¤ë©´ temperatureë¥¼ ì‘ê²Œ í•´ë³´ì. (large negative logitì„ ë¬´ì‹œí•  ìˆ˜ ìˆë„ë¡)

## ---

* 3 Preliminary experiments on MNIST
* 4 Experiments on speech recognition
* 5 Training ensembles of specialists on very big datasets

ìœ„ ì¥ë“¤ì€ ì½ì–´ë§Œ ë³´ì

## 6 Soft Targets as Regularizers

* soft targetì´ overfittingì„ ë°©ì§€í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ì“°ì¼ ìˆ˜ ìˆë‹¤.

## ------

ê·¸ ë’¤ë„ ì½ì–´ë§Œ ë³´ì
