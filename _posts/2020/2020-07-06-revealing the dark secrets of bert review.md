---
layout: post
title: "ğŸ“ƒ Revealing the Dark Secrets of BERT ë¦¬ë·°"
tags:
  - paper
---

GLUE íƒœìŠ¤í¬ì™€ ê·¸ subsetì„ ì´ìš©í•˜ì—¬ ì •ëŸ‰ì , ì •ì„±ì ìœ¼ë¡œ BERT heads ë¶„ì„í•œ ë…¼ë¬¸ì´ë‹¤. EMNLP 2019ì— Acceptëœ ë…¼ë¬¸.

Main Contribution:

* analysis of BERT's capacity to capture different kinds of linguistic information by encoding it in its self-attention weights.
* present evidence of BERTâ€™s over- parametrization & suggest simple way of im- proving its performance.

## Methodology

ì•„ë˜ ì„¸ê°œì˜ Research Questionsì— ëŒ€í•´ ì§‘ì¤‘í•¨

* What are the common attention patterns, how do they change during fine-tuning, and how does that impact the performance on a given
task?
* What linguistic knowledge is encoded in self-attention weights of the fine-tuned models and what portion of it comes from the pre- trained BERT?
* How different are the self-attention patterns of different heads, and how important are they for a given task?

ì‹¤í—˜ í™˜ê²½ì€ ì•„ë˜ì™€ ê°™ìŒ

* huggingface/pytorch-pretrained-bert ì‚¬ìš©í•˜ê³ , BERT base uncased ì‚¬ìš©.
* ì‚¬ìš©í•œ GLUE tasks: MRPC, STS-B, SST-2, QQP, RTE, QNLI, MNLI
* WinogradëŠ” ë°ì´í„°ì…‹ ì‚¬ì´ì¦ˆ ë•Œë¬¸ì— ì œì™¸í–ˆê³ , CoLAëŠ” GLUE ì‹ ë²„ì „ì—ì„œ ì œì™¸ë˜ê¸° ë•Œë¬¸ì— ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤.
* fine-tuning hyperparamì€ BERT ì› ë…¼ë¬¸ì„ ë”°ë¼ê°

## Experiments

### Bert's self-attention patterns

{% include image.html url="/images/2020/07-06-secrets/fig1.png" class='noshadow' %}

{% include image.html url="/images/2020/07-06-secrets/fig2.png" class='noshadow' %}

* BERTì˜ Self attention patternì„ ë½‘ìœ¼ë©´ ìœ„ì™€ ê°™ì€ íŒ¨í„´ë“¤ì´ ìˆìŒ
    * Vertical: `[CLS]`, `[SEP]`ê°™ì€ í† í°ì— Attentionì´ ê±¸ë¦¬ëŠ” ê²ƒ.
    * Diagonal: previous, following tokensì— Attention
    * Vertical + Diagonal
    * Block: Intra-sentence attention
    * Heterogeneous
* HeterogeneousëŠ” 32% ~ 61%ê¹Œì§€ ë‹¤ì–‘í•˜ì§€ë§Œ ì „ì²´ì ìœ¼ë¡œ ë§ì•˜ë‹¤.
* ê·¸ë˜ì„œ Heterogeneous attentionì´ ì ì¬ì ìœ¼ë¡œ êµ¬ì¡°ì ì¸ ì •ë³´ë¥¼ ì¡ì•„ë‚¼ ìˆ˜ ìˆë‹¤ê³  íŒë‹¨.

### Relation specific heads in BERT

Baker et al., 1998 ì˜ ë‚´ìš©ì„ ì¡ì•„ë‚¼ ìˆ˜ ìˆëŠ”ì§€ í…ŒìŠ¤íŠ¸. ì¡°ê±´ì„ ì¢€ ë§ì´ ê²€.

{% include image.html url="/images/2020/07-06-secrets/fig3.png" class='noshadow' %}

* ìœ„ì™€ ê°™ì€ ì˜ˆì‹œë¥¼ ë§ì´ ë³¼ ìˆ˜ ìˆì—ˆê³ , ì´ê²Œ ì–´ëŠì •ë„ì˜ ì¦ê±°ë¥¼ ì œì‹œí•´ì¤€ë‹¤ê³  í•´ì„í•¨
* ì¡°ê¸ˆ ë” ì¼ë°˜ì ì¸ ìƒí™©ì— ëŒ€í•œ ì¦ëª…ì€ future works.

### Change in self-attention patterns after fine-tuning

fine tuning ì „ í›„ì˜ headë³„ Attention weightë¥¼ ë½‘ì•„ì„œ cosine similarityë¥¼ êµ¬í•´ë´„.

{% include image.html url="/images/2020/07-06-secrets/fig4.png" class='noshadow' %}

QQPë¥¼ ì œì™¸í•˜ê³ ëŠ” ë§ˆì§€ë§‰ 2ë ˆì´ì–´ê°€ ë§ì´ ë°”ë€ŒëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

### Attention to linguistic features

{% include image.html url="/images/2020/07-06-secrets/fig5.png" class='noshadow' %}

* CLSëŠ” ì•ìª½ ë ˆì´ì–´ë§Œ Attentionì´ ë§ì´ ë“¤ì–´ê°€ë”ë¼.
* ê·¸ ë’¤ë¶€í„°ëŠ” SEPì— Attention ê±¸ë¦¬ëŠ” ê²ƒì´ ì§€ë°°ì ì´ë‹¤.
* SST-2ëŠ” SEP í† í°ì´ í•˜ë‚˜ë“¤ì–´ê°€ë‹ˆê¹Œ (ì…ë ¥ ë¬¸ì¥ì´ í•˜ë‚˜ë‹ˆê¹Œ) ìœ ë‚œíˆ í° ê°’ì„ ê°€ì§€ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.
* ì´ëŸ° ê²½í–¥ì„ ë³´ë‹ˆê¹Œ task specificí•˜ê²Œ linguistic reasoningì„ ë°°ìš°ëŠ” ê²ƒë³´ë‹¤ pretrained BERTë¡œë¶€í„° ì˜¤ëŠ” ê²ƒ ê°™ë‹¤.

### Token to token attention

íŒ¨ìŠ¤

### Disabling self attention heads

ì´ë¯¸ related works ì„¹ì…˜ì—ì„œ self attention heads ë§ˆìŠ¤í‚¹í•˜ëŠ” ë…¼ë¬¸ì„ ë ˆí¼ëŸ°ìŠ¤ë¡œ ê±¸ì–´ë†“ìŒ

* ì—­ì‹œ ì˜ ë˜ê³  ì˜¤ë¥´ê¸°ë„ í•¨
* ë ˆì´ì–´ ìì²´ë¥¼ ë“œëí•´ë„ ì˜ ë¨

{% include image.html url="/images/2020/07-06-secrets/fig6.png" class='noshadow' %}

## Discussion

baseì—¬ë„ over parameterizeê°€ ì˜ ë¨

---

BERTì˜ over parameterizationì„ ë‹¤ê°ë„ë¡œ ë³´ì—¬ì¤€ ë…¼ë¬¸ì¸ ë“¯ í•˜ë‹¤. ëª‡ëª‡ ë¶„ì„ì€ `"...?"`í•œ ê²ƒë„ ìˆì§€ë§Œ, "ì´ë ‡ê²Œ í•´ë¼!"ë¼ëŠ” ë…¼ë¬¸ë³´ë‹¤ëŠ” "ì´ë ‡ë”ë¼"ë¼ëŠ” ë…¼ë¬¸ì´ë¼ ì¬ë°Œê²Œ ì½ì—ˆë‹¤. ê²½ëŸ‰í™”ì‹œì— ì°¸ê³ í•  ë§Œí•œ ë…¼ë¬¸.
