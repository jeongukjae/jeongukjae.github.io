---
layout: post
title: "ğŸ“ƒ Q8BERT: Quantized 8Bit BERT ë¦¬ë·°"
tags:
  - paper
---

intelì—ì„œ ë‚˜ì˜¨ NeurIPS 2019ì— ë°œí‘œëœ Q8BERT ë…¼ë¬¸ì´ë‹¤. arxiv ë§í¬ëŠ” [https://arxiv.org/pdf/1910.06188.pdf](https://arxiv.org/pdf/1910.06188.pdf)ì´ë‹¤. BERTë¥¼ fine tuning phaseë•Œ quantization aware trainingì„ ì ìš©í•˜ì—¬ 4ë°° ì••ì¶•í•˜ê³ , intel CPUì˜ 8bit ì—°ì‚°ì„ ì‚¬ìš©í•´ ì—°ì‚°ì„ ê°€ì†í–ˆë‹¤.

## 1. Introduction

* quantization-aware trainingì„ fine-tuning processì— ì ìš©
* GEMM ì—°ì‚°ê³¼ FC, Embedding Layer ì—°ì‚°ì„ quantizeí•¨
* 8bit quantized inference í•´ë„ 99% accuracyë¥¼ ìœ ì§€í•¨

## 2. Method

* linear quantization ì‚¬ìš©
* Intel Xeon Cascade Lakeì˜ VNNIë¥¼ ì‚¬ìš©í•  ê²½ìš° FP32 ì—°ì‚°ì— ë¹„í•´ 3.7ë°° ë¹¨ë¼ì§ [(Bhandare et al., 2019)](https://arxiv.org/abs/1906.00532)

### 2.1. Quantization Scheme

* symmetric linear quantization ì‚¬ìš©í•¨

  $$\text{Quantize}(x \vert S^x, M) = \text{Clamp}(x \times S^x, -M, M)$$

  $$\text{Clamp}(x, a, b) = \min(\max(x, a), b)$$

* $$S^x$$ëŠ” $$x$$ì— ëŒ€í•œ quantization scaling factorì´ê³ , $$M$$ì€ highest quantized valueì´ë‹¤.
* 8bitì´ë¯€ë¡œ $$M$$ì€ 127
* scaling factor ê³„ì‚°ë²•
  * weights: $$S^W = \frac M {\max(\vert W \vert)}$$
  * activations: $$S^x = \frac M {\text{EMA}(\max(\vert x \vert))}$$
    * EMA: Exponential Moving Average

### 2.2. Quantized-Aware Training

* Quantization-aware trainingì€ inference stageì—ì„œ quantizeí•  ìˆ˜ ìˆê²Œ trainingí•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜
* post trainingì— ë°˜ëŒ€ë˜ëŠ” ë°©ì‹
* fake quantizationì„ ë„ì…
  * quantization errorë¥¼ ë³´ì—¬ì£¼ê³  quantization error gapì„ ì¤„ì´ê¸° ìœ„í•´
* fake quantizationì€ FP32ê°’ë“¤ì„ roundingí•˜ëŠ” íš¨ê³¼ê°€ ìˆë‹¤. [(Jacob et al.)](https://arxiv.org/abs/1712.05877)
  * outlierë“¤ì„ ì¤„ì—¬ì„œ quantizationì´ ì˜ ë˜ë„ë¡ í•¨
* roundingí•˜ëŠ” operationì´ ë¯¸ë¶„ê°€ëŠ¥í•˜ì§€ ì•Šìœ¼ë‹ˆ Straight-Through Estimator (STE)ë¥¼ ì‚¬ìš©í•¨ [(Bengio et al., 2013)](https://arxiv.org/abs/1308.3432)

## 3. Implementation

* training phase
  * FCëŠ” fake quantized inputê³¼ fake quantized weight ì‚¬ì´ì— GEMM ì—°ì‚°ì„ í•˜ê³  quantizeì•ˆí•œ biasë¥¼ ë”í•œë‹¤.
* inference phase
  * embedding layerëŠ” int8 embedding vectorë¥¼ ë°˜í™˜í•˜ê³ , quantized FCëŠ” int8 input, weightë¥¼ ì—°ì‚°í•˜ê³  int32 biasë¥¼ ë”í•œë‹¤. ê·¸ í›„ activationì„ ì—°ì‚°í•œë‹¤.
* pytorch transformers ì´ìš©í–ˆë‹¤ê³  í•œë‹¤. (í˜„ì¬ huggingface/transformers)
* Higher precisionì´ ì¤‘ìš”í•œ Softmax, Layer Normalization, GELUëŠ” FP32ë¡œ ì‚¬ìš©í•¨

## 4. Evaluation

{% include image.html class="noshadow" url="/images/2020/04-14-q8bert/fig1.png" %}

Dynamic Quantizationê³¼ ë¹„êµí–ˆì„ ë•Œ Quantization Aware Trainingì´ í›¨ì”¬ Accuracy Reductionì´ ì ì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

## 5. Related Work

ì½ì–´ë³´ì¥

## 6. Conclusions and Future Work

* BERT ì••ì¶•ì„ ìœ„í•´ ë‹¤ë¥¸ model compression methods ì ìš©
* quantized BERT ëª¨ë¸ì— ë‹¤ë¥¸ compresssion ì ìš©í•´ë³´ê¸°

## ____

1. ì–¼ë§ˆë‚˜ ë¹ ë¥¸ì§€ëŠ” ì í˜€ìˆì§€ ì•Šë‹¤.
2. Softmax, LayerNorm, GELUë„ int8, int32ë¡œ ì§„í–‰í•œ ê²°ê³¼ ìˆì—ˆìœ¼ë©´ ì¢‹ì•˜ì„í…ë°
3. distillationê³¼ ê°™ì´ ì ìš©ê°€ëŠ¥í• ê¹Œ?
4. quantizationì´ ì˜ë˜ëŠ” ì´ìœ ê°€ fp32ê°€ redundantí•´ì„œ ê·¸ëŸ°ê±¸ê¹Œ ì•„ë‹ˆë©´ BERTê°€ reduandantí•´ì„œ ê·¸ëŸ°ê±¸ê¹Œ
