---
layout: post
title: "ğŸ“ƒ DynaBERT: Dynamic BERT with Adaptive Width and Depth ë¦¬ë·°"
tags:
  - paper
  - nlp
---

ì´ ë…¼ë¬¸ì—ì„œëŠ” BERT, RoBERTaê°€ ë§¤ìš° ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, memory, computing powerê°€ ë„ˆë¬´ ë§ì´ í•„ìš”í•˜ë¯€ë¡œ ê·¸ë¥¼ ì••ì¶•í•´ë³´ëŠ” ë°©ë²•ì„ ì œì•ˆí•œë‹¤. ì•„ì§ WIPì¸ ë…¼ë¬¸ì´ê³ , [https://arxiv.org/abs/2004.04037](https://arxiv.org/abs/2004.04037)ê°€ ë§í¬ì´ë‹¤. í™”ì›¨ì´ì—ì„œ ë‚˜ì˜¨ ë…¼ë¬¸ì´ë‹¤.

## Abstract

* dynamic BERT Model ì œì•ˆ, width, depth ë°©í–¥ìœ¼ë¡œ dynamicí•¨
* Knowledge Distillation ë°©ì‹ìœ¼ë¡œ full BERT ëª¨ë¸ì„ width adaptive BERTë¡œ í•™ìŠµí•œ ë’¤, width, depth ëª¨ë‘ adaptiveí•˜ê²Œ í•™ìŠµí•¨

## 1. Introduction

* ê¸°ì¡´ì˜ Transformer-based modelì„ ì••ì¶•í•˜ê±°ë‚˜, ì¶”ë¡  ê°€ì†í™”ë¥¼ ì‹œë„í•œ ë°©ë²•ë¡ ë“¤:
  * low-rank approximation
    * [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
    * [A Tensorized Transformer for Language Modeling](https://arxiv.org/abs/1906.09777)
  * weight sharing
    * [Universal transformers](https://arxiv.org/abs/1807.03819)
    * [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
  * knowledge distillation
    * [Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)
    * [Patient knowledge distillation for bert model compression](https://arxiv.org/abs/1908.09355)
    * [Tinybert: Distilling bert for natural language understanding](https://arxiv.org/abs/1909.10351)
  * quantization
    * [Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model](https://arxiv.org/abs/1906.00532)
    * [Q8bert: Quantized 8bit bert](https://arxiv.org/abs/1910.06188)
    * [Q-bert: Hessian based ultra low precision quantization of bert](https://arxiv.org/abs/1909.05840)
  * pruning
    * [Pruning Convolutional Neural Networks for Resource Efficient Inference](https://arxiv.org/abs/1611.06440)
    * [Pruning a BERT-based Question Answering Model](https://deepai.org/publication/pruning-a-bert-based-question-answering-model)
    * [Are sixteen heads really better than one?](https://arxiv.org/abs/1905.10650)
    * [Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned](https://arxiv.org/abs/1905.09418)
    * [Fine-tune BERT with sparse self-attention mechanism](https://www.semanticscholar.org/paper/Fine-tune-BERT-with-Sparse-Self-Attention-Mechanism-Cui-Li/a3ef6ee560e93e6f58be2b28f27aed0eb86dc463)
* ëª‡ëª‡ ë¦¬ì„œì¹˜ì—ì„œ depth adaptive modelsë„ ì¶©ë¶„í•œ ì˜ë¯¸ê°€ ìˆìŒì„ ì¦ëª…
* ìµœê·¼ì˜ ë¦¬ì„œì¹˜ëŠ” width directionë„ ì¶©ë¶„íˆ redundantí•¨ì„ ë§í•˜ê³  ìˆìŒ
  * ex> Attention Headë¥¼ pruningí•´ë„ ì¶©ë¶„íˆ ì„±ëŠ¥ì´ ì¢‹ìŒ
* CNNì—ì„œ width, depth - adaptiveí•˜ê²Œ ëª¨ë¸ì„ ë§Œë“¤ì–´ë‚¸ ì‹œë„ê°€ ìˆì—ˆì§€ë§Œ, BERTì— ì ìš©í•˜ê¸´ í˜ë“¤ë‹¤.
  * Transformer ë ˆì´ì–´ ì•ˆì˜ Multi Head Attentionê³¼ Position wise Feed Forward Network ë•Œë¬¸
* Training ë°©ë²•
  * width adaptive BERT í•™ìŠµ : attention headsë‘ neuron ì¤‘ ì¤‘ìš”í•œ ê²ƒë“¤ë§Œ rewireí•œ ë’¤ distillation ì§„í–‰
  * adaptive BERT í•™ìŠµ : width adaptive BERTì—ì„œ initializeí•œ ë’¤ì— width, depth ë‘˜ ë‹¤ distillation

## 2. Related Work

### 2.1. Transformer Layer

ì´ê±°ëŠ” ê·¸ëƒ¥ Transformer ì„¤ëª…ì„

### 2.2. Compression for Transformer/BERT

* Low Rank Approximation
  * weight matrixë¥¼ ë‘ lower rank matrixì˜ ê³±ìœ¼ë¡œ ê·¼ì‚¬í•œë‹¤.
  * ALBERTëŠ” embedding layerë¥¼ ê·¼ì‚¬
  * Tensorized TransformerëŠ” MHA ê²°ê³¼ê°€ orthonormal base vectorsë¡œ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤ê³  í•¨ + multi-linear attention ì‚¬ìš©
* weight sharing
  * Universal TransformerëŠ” layerê°„ weight sharing
  * Deep Equilibrium Modelì€ íŠ¹ì • ë ˆì´ì–´ì˜ input, outputì´ ê°™ì•„ì§€ê²Œ í•¨ -> ??? ëª¨ë¥´ê² ë‹¤ ì°¾ì•„ë³´ì
  * ALBERTëŠ” ë ˆì´ì–´ê°„ parameter sharingì´ network parameterë¥¼ ì•ˆì •ì ìœ¼ë¡œ ë§Œë“¤ê²Œ í•´ì£¼ê³  ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ëŠ”ë‹¤ê³  í•¨
  * ê·¼ë° model sizeëŠ” ì¤„ì–´ë„ inferenceëŠ” ì•ˆë¹ ë¦„
* Distillation
  * DistilBertëŠ” soft logitì´ë‘ hidden states distillation ì‹œí‚´
  * BERT PKDëŠ” intermediate layerì— ë¡œìŠ¤ ì¤Œ
  * Tiny BERTëŠ” general distillation, task-specific distillationìœ¼ë¡œ ë‚˜ëˆ ì„œ ì§„í–‰í•¨
* Quantizaiton
  * QBERTëŠ” second order informationì„ í™œìš©í•´ ê° ë ˆì´ì–´ë³„ë¡œ ëª‡ ë¹„íŠ¸ë¥¼ í• ë‹¹í•  ì§€ ì •í•¨
    * steeper curvatureì—ëŠ” ë” ë§ì€ bit í• ë‹¹
  * Fully Quantized TransformerëŠ” uniform min max quantizationì„ ì”€
  * Q8BERTëŠ” quantization aware training + symmetric 8 bit linear quantizatio í™œìš©í•¨
* Pruing
  * "Fine-tune BERT with sparse self-attention mechanism"ì´ë€ ë…¼ë¬¸ì—ì„œ sparse self attentionì„ ì‚¬ìš©
  * "Compressing bert: Studying the effects of weight pruning on transfer learning"ì´ë€ ë…¼ë¬¸ì—ì„œ magnitude-based pruning ì‚¬ìš©
  * LayerDropì—ì„œëŠ” transformer layerë“¤ì˜ ì¶”ë¡ ì„ ìœ„í•´ structed dropoutì„ ì ìš©í•¨
* ê·¼ë° ì´ ë°©ë²•ë“¤ ëŒ€ë¶€ë¶„ì´ ì••ì¶•ê³¼ ê´€ë ¨ëœ ê±°ê³  Universal Transformerë‚˜ LayerDrop, Depth-adaptive transformerë„ ì••ì¶•ì´ë‘ ê°€ì†ì— ì‹ ê²½ì“°ê¸°ëŠ” í•˜ë‚˜ depth directionë¿ì´ë‹¤.

## 3. Method

### 3.1. Training DynaBERT_w with Adaptive Width

* CNNê³¼ ë¹„êµí•´ BERTëŠ” Transformers Layerê°€ ìŒ“ì—¬ìˆëŠ” í˜•íƒœë¼ ë” ë³µì¡
* MHAì—ëŠ” linear transformationê³¼  key, query, valueì˜ ê³±ì´ ì¡´ì¬í•¨.

#### 3.1.1. Using Attention heads and Intermediate Neurons in FFN to Adapt the Width

* MHAë¥¼ ê° Attention ì—°ì‚°ìœ¼ë¡œ ë¶„ë¦¬í•œ ë‹¤ìŒ ì¤‘ìš”í•œ attention headsë§Œì„ ì·¨í•œë‹¤.
* ê°€ì¥ ì¤‘ìš”í•œ ìˆœìœ¼ë¡œ Headì™€ Neuronì„ ì™¼ìª½ìœ¼ë¡œ ëª°ì•„ë„£ëŠ”ë‹¤.

#### 3.1.2. Network Rewiring

* "Pruning convolutional neural networks for resource efficient inference"ì™€ "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned" ì— ë”°ë¼ì„œ importance scoreë¥¼ êµ¬í•¨.
* ê·¸ëŸ° ë‹¤ìŒ ì•„ë˜ì²˜ëŸ¼ ì¬êµ¬ì„±í•¨

{% include image.html class="noshadow" url="/images/2020-04-13-dynabert/fig1.png" %}
{% include image.html class="noshadow" url="/images/2020-04-13-dynabert/fig2.png" %}

#### 3.1.3. Training with Adaptive Width

* distillationì€ logits, embedding, hidden statesë¥¼ ë¹„êµí•œë‹¤. ê°ê°ì€ ì•„ë˜ì²˜ëŸ¼ ë¡œìŠ¤ë¥¼ ê³„ì‚°
  * $$l_{pred} (y^\prime, y) = - \text{softmax} (y) \text{log_softmax} (y ^\prime) $$
  * $$l_{emb}(E^\prime, E) = MSE(E^\prime, E)$$
  * $$l_{hidn}(H^\prime, H) = \sum^L_{l=1} MSE(H^\prime_l, H_l)$$
* ê·¸ë ‡ê²Œ êµ¬í•œë‹¤ìŒ ê°ê°ì— weightë¥¼ ì£¼ì–´ì„œ ë”í•œë‹¤.
* data augmentationì´ ì ìš©ë˜ì–´ ìˆë‹¤.

{% include image.html class="noshadow" url="/images/2020-04-13-dynabert/fig4.png" %}

### 3.2. Training DynaBERT with Adaptive Width and Depth

* ê·¸ ë‹¤ìŒ depth ë°©í–¥ìœ¼ë¡œ distillationí•œë‹¤.
* width directionìœ¼ë¡œ í•™ìŠµí•œ ê²ƒì˜ catastrophic forgettingì„ ë§‰ê¸° ìœ„í•´ ì•„ë˜ì²˜ëŸ¼ í•™ìŠµí•œë‹¤.

{% include image.html class="noshadow" url="/images/2020-04-13-dynabert/fig5.png" %}

* depth layerì—ì„œëŠ” $$mod(d + 1, \frac 1 {m_d}) = 0$$ì¸ ë ˆì´ì–´ë¥¼ drop í–ˆë‹¤.
  * ex> 0.75ë¥¼ depth multiplierë¡œ ê°€ì ¸ê°€ë©´ Bert base ê¸°ì¤€ìœ¼ë¡œ 3,7, 11ì„ dropí•œë‹¤.
  * ë§ˆì§€ë§‰ ë ˆì´ì–´ë¥¼ ì·¨í•˜ê¸° ìœ„í•¨ì´ë¼ê³  í•œë‹¤.
* lossëŠ” wide adpativeì™€ ê°™ì´ ê°€ì ¸ê°€ëŠ”ë° ê° lossë³„ weightê°€ ë‹¤ë¥´ë‹¤.

{% include image.html class="noshadow" url="/images/2020-04-13-dynabert/fig6.png" %}

* ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ì— fine-tuningë„ í•  ìˆ˜ ìˆëŠ”ë°, ì´ê±´ predicted labelì´ë‘ ground-truth labelì„ cross entropyë¡œ fine tuningí•œë‹¤.

## 4. Experiment

* BERT base, RoBERTa baseì™€ ë¹„êµ
* ëª‡ëª‡ í•˜ì´í¼ íŒŒë¼ë¯¸í„°
  * Layer = 12
  * hidden size = 768
  * intermediate size = 3072
  * width multipliers = 1, 0.75, 0.5, 0.25
  * depth multipliers = 1, 0.75, 0.5
* ì¼ë‹¨ ë­ BERT, RoBERTa baseì— ë¹„í•´ ë°ì´í„°ë¥¼ ë” ë„£ê³  í•™ìŠµì„ ë” í•œ ê²ƒì´ë‹ˆ multiplierê°€ 1, 1ì¸ ê²½ìš°ì—ëŠ” ë‹¹ì—°íˆ ë” ì˜ë‚˜ì˜¨ë‹¤.
  * ê·¸ë˜ë„ ì£¼ëª©í• ë§Œí•œ ì ì€ ìƒê°ë³´ë‹¤ ê·¸ë ‡ê²Œ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì 
  * ex> CoLAì˜ ê²½ìš° BERT baseê°€ 51.5ë¥¼ ê¸°ë¡í•˜ëŠ”ë°, DynaBERTì˜ depth 0.75, width **0.25**ê°€ 51.6ì„ ê¸°ë¡í•œë‹¤. (ë‹¨ìˆœ acc ë¹„êµ)

{% include image.html class="noshadow" url="/images/2020-04-13-dynabert/fig7.png" %}

* ì•„ë˜ëŠ” FLOPsë¥¼ ë¹„êµí•œ ê·¸ë˜í”„ì´ê³ , ë” ìì„¸í•œ ì°¨ì´ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.
  * ìƒë‹¹íˆ ë§ì´ ì°¨ì´ë‚˜ëŠ” FLOPsë¡œ ì—‡ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë‹¤.

{% include image.html class="noshadow" url="/images/2020-04-13-dynabert/fig8.png" %}

### 4.3. Ablation Study

* Adaptive Width
  * ê·¸ëƒ¥ BERT, network rewiringì—†ì´ í•™ìŠµí•œ DynaBERT, network rewiringí•œ DynaBERT, Distillation + Data Augmentationê¹Œì§€í•œ DynaBERTë¥¼ ê°ê° Finetuningí•¨
  * ë‹¹ì—°íˆ rewiring + distillation + data augí•œ ê²ƒì´ ì œì¼ ì˜ ë‚˜ì˜¤ê³ , ìƒê°ë³´ë‹¤ network rewiringì´ í° ì˜í–¥ì„ ì¤€ë‹¤.
  * ì•„ì‰¬ìš´ ì ì€ data augmentationë„ ë”°ë¡œ í•´ì¤¬ìœ¼ë©´...
* adaptive width and depth
  * ê·¸ëƒ¥ Vanilla DynaBERTì™€ Distillation + Data augí•œ DynaBERT, Fine-Tuningí•œ ê²ƒì„ ë¹„êµí•¨
  * ë‹¹ì—°íˆ fine tuning + distailltion + Data augí•œ ê²ƒì´ ì˜ ë‚˜ì˜¨ë‹¤.
  * ìƒê°ë³´ë‹¤ ê·¸ í­ì´ í° ê²ƒë„ ìˆê³ , í•˜ì§€ë§Œ fine tuning ì•ˆ í•œê²ƒì´ ë” ì˜ë‚˜ì˜¤ëŠ” íƒœìŠ¤í¬ë„ ìˆë‹¤ (MRPC)

## 5. Discussion

### 5.1.Comparison of Conventional Distillation and Inplace Distillation

* í•´ë´¤ëŠ”ë°, ìŒ... ì–´.. í•œ ìˆ˜ì¤€ì´ë‹¤. ì˜ ë‚˜ì˜¤ëŠ” ê²ƒë³´ë‹¤ ìˆê³ , ì•„ë‹Œ ê²ƒë„ ìˆì§€ë§Œ ê·¸ì € ê·¸ëŸ° ìˆ˜ì¤€.

### 5.2. Different Methods to Train DynaBERT_w

* Progressive rewiring
  * ê³„ì† ì§„í–‰í•´ê°€ë©´ì„œ ë‚®ì€ width multiplierë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ì‹ì¸ë°, ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ë°©ë²•ì´ë‹¤.
  * ë³„ë¡œ ì„±ëŠ¥ ì´ë“ ì—†ë‹¤
* Universally Slimmable Training
  * ë§¤ iterationë§ˆë‹¤ ì—¬ëŸ¬ width multipliersë¥¼ ê³¨ë¼ë†“ê³  í•™ìŠµí•œë‹¤.
  * ë³„ë¡œ í° ì°¨ì´ ì—†ë‹¤.

## 6. Conclusion and Future Work

* ë‚˜ì¤‘ì—ëŠ” hidden sizeë„ ë°”ê¿€ ìˆ˜ ìˆë„ë¡ í•´ë³¼ ê²ƒ
* ì•„ë‹ˆë©´ ALBERTì²˜ëŸ¼ weight sharingí•˜ëŠ” ë°©ë²•?
* pretraining stageì—ë„?
