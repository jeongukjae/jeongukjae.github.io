---
layout: post
title: "ğŸ“ƒ Are Sixteen Heads Really Better than One? ë¦¬ë·°"
tags:
  - paper
---

Multi head attentionì´ í‘œí˜„ë ¥ì´ ì¢‹ê³  ë§ì€ ì •ë³´ë¥¼ ë‹´ì„ ìˆ˜ ìˆë‹¤ì§€ë§Œ, ëª¨ë“  headê°€ í•„ìš”í•œ ê²ƒì€ ì•„ë‹ˆë‹¤. ì´ì— ê´€í•œ ë…¼ë¬¸ì´ Are Sixteen Heads Really Better Than One? (Michel et al., 2019)ì´ê³ , arxiv ë§í¬ëŠ” [https://arxiv.org/abs/1905.10650](https://arxiv.org/abs/1905.10650)ì´ë‹¤.

## Abstract

* MultiHeadë¡œ í•™ìŠµì´ ë˜ì—ˆë”ë¼ë„ Test Timeì—ëŠ” ë§ì€ headë¥¼ ì œê±°í•´ë„ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì¡´í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•¨.
* íŠ¹íˆ ëª‡ëª‡ ë ˆì´ì–´ëŠ” single headì—¬ë„ ì„±ëŠ¥í•˜ë½ì´ ì—†ì—ˆë‹¤.

## 1. Introduction

* greedy í•˜ê³  iterativeí•œ attention head pruning ë°©ë²• ì œì‹œ
* inference timeì„ 17.5% ë†’ì˜€ë‹¤.
* MTëŠ” pruningì— íŠ¹íˆ ë¯¼ê°í–ˆëŠ”ë°, ì´ë¥¼ ìì„¸íˆ ì‚´í´ë´„

## 2. Background: Attention, Multi-headed Attention, and Masking

* ê±°ì˜ ë‹¤ íŒ¨ìŠ¤
* Multi Head Attention Maskingí•˜ëŠ” ê²ƒì€ mask variableë¡œ ê³„ì‚°í•¨
* íŠ¹ì • headì˜ ê²°ê³¼ê°’ì„ 0ìœ¼ë¡œ ì§€ì •

## 3. Are All Attention Heads Important?

* WMTì—ì„œ í…ŒìŠ¤íŠ¸

### 3.2. Ablating One Head

* í•˜ë‚˜ì˜ Headë§Œ ì œê±°í•˜ëŠ” í…ŒìŠ¤íŠ¸

{% include image.html url="/images/2020-05-18-sixteen-heads/fig1.png" class='noshadow' %}

* > **at test time, most heads are redundant given the rest of the model.**

### 3.3. Ablating All Heads but One

* ê·¸ëŸ¼ headê°€ í•˜ë‚˜ ì´ìƒ í•„ìš”í• ê¹Œ?
* ëŒ€ë¶€ë¶„ì˜ layerëŠ” 12/16 headë¡œ trianë˜ì—ˆì–´ë„ test timeì— 1 headë„ ì¶©ë¶„í•˜ë‹¤.
* ê·¼ë° NMTëŠ” ë˜ê²Œ ë¯¼ê°í•¨
  * WMTì—ì„œ enc-decì˜ last layerê°€ 1 head ì‚¬ìš©í•  ê²½ìš° 13.5 BLEU pointì´ìƒ ë–¨ì–´ì§
  * last layerê°€ decì˜ last layerì¸ê°€..?

### 3.4. Are Important Heads the Same Across Datasets?

* ì¤‘ìš”í•œ headëŠ” ë‹¤ë¥¸ íƒœìŠ¤í¬ì—ì„œë„ ì¤‘ìš”í• ê¹Œ?
* ì–´ëŠì •ë„ ì¤‘ìš”í•¨, ê·¸ëŸ° ê²½í–¥ì„ ë³´ì„

{% include image.html url="/images/2020-05-18-sixteen-heads/fig2.png" class='noshadow' %}

## 4. Iterative Pruning of Attention Heads

* iterativeí•˜ê²Œ ì ë‹¹íˆ ìë¥´ì

### 4.1. Head Importance Score for Pruning

* head maskì— ëŒ€í•œ lossë¡œ ê³„ì‚°í•œë‹¤.

{% include image.html url="/images/2020-05-18-sixteen-heads/fig3.png" class='noshadow' %}

* Molchanos et al., 2017 ë°©ë²•ì„ tayler expansioní•œ ê±°ë‘ ê°™ë‹¤
* Molchanos et al., 2017ì— ë”°ë¼ì„œ importance scoreë¥¼ l2 normìœ¼ë¡œ ì •ê·œí™”í•¨

### 4.2. Effect of Pruning on BLEU/Accuracy

* 20% ~ 40%ì •ë„ pruningì´ ê°€ëŠ¥í–ˆë‹¤.
* Appendixì— ë” ìˆìŒ

{% include image.html url="/images/2020-05-18-sixteen-heads/fig4.png" class='noshadow' %}

### 4.3. Effect of Pruning on Efficiency

{% include image.html url="/images/2020-05-18-sixteen-heads/fig5.png" class='noshadow' %}

* ì†ë„ëŠ” ì–¼ë§ˆë‚˜ ì¤„ê¹Œ?? 1080 tië¥¼ ê°€ì§„ ë¨¸ì‹  ë‘ëŒ€ì—ì„œ í…ŒìŠ¤íŠ¸í•¨
* ê°œì¸ì ìœ¼ë¡œëŠ” ì—­ì‹œ pruningì€ memory footprintë¥¼ ì¤„ì—¬ì£¼ëŠ” ê²ƒì´ í°ê°€?? ì‹¶ê¸°ë„ í•˜ë‹¤
  * ì–´ì°¨í”¼ ì—°ì‚°ì€ ì§„í–‰ì„ í•˜ê³ , ì—°ì‚°ì—ì„œ ì§„í–‰í•˜ëŠ” ë°ì´í„°ì˜ í¬ê¸°ê°€ ì£¼ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— ê·¸ë ‡ê²Œ dramaticí•œ ì„±ëŠ¥ í–¥ìƒì€ ì•„ë‹Œ ë“¯ í•¨

## 5. When Are More Heads Important? The Case of Machine Translation

* ê²°ë¡ :
  * > In other words, encoder-decoder attention is much more dependent on multi-headedness than self-attention.
* ì—­ì‹œ self-attentionì´ redundancyê°€ ë†’ì€ ê±´ê°€??

## 6. Dynamics of Head Importance during Training

* Trained Modelì—ì„œ ìˆ˜í–‰í•˜ëŠ” ê²ƒë³´ë‹¤ Training Modelì—ì„œ ìˆ˜í–‰í•˜ëŠ” ê²ƒì€ ì–´ë–¤ê°€??ì— ê´€í•œ ê²ƒ
* epoch ëë§ˆë‹¤ ê° pruning levelì— ë”°ë¼ ì„±ëŠ¥ ì¸¡ì •í•´ë´„

{% include image.html url="/images/2020-05-18-sixteen-heads/fig6.png" class='noshadow' %}

* early epoch ë•ŒëŠ” êµ‰ì¥íˆ ë¹ ë¥´ê²Œ ì„±ëŠ¥ì´ í•˜ë½í•˜ëŠ”ë°, í•™ìŠµì´ ì§„í–‰ë  ìˆ˜ë¡ ì¤‘ìš”í•œ headë§Œ ì¤‘ìš”í•´ì§€ê³  ë‚˜ë¨¸ì§€ëŠ” ì•„ë‹ˆê²Œ ë¨

## 7. Related work

* íŒ¨ìŠ¤
* ê·¼ë° ë‚˜ì¤‘ì— ë‹¤ì‹œ ë³´ì
