---
layout: post
title: "ğŸš€ Apex's FusedLayerNorm vs Torch's LayerNorm"
tags:
  - pytorch
---

[microsoft/DeepSpeedExamples](https://github.com/microsoft/DeepSpeedExamples)ì˜ BERTì—ì„œ [Apexì˜ FusedLayerNormì„ ì‚¬ìš©](https://github.com/microsoft/DeepSpeedExamples/blob/8610e5e3fcce5fb247e3b85ea2bed0f2296b5443/bing_bert/nvidia/modelingpreln.py#L308-L313)í•˜ê³  ìˆê³ , [NVIDIA/DeepLearningExamples](https://github.com/NVIDIA/DeepLearningExamples)ì—ì„œë„ [Apexì˜ FusedLayerNormì„ ì‚¬ìš©](https://github.com/NVIDIA/DeepLearningExamples/blob/6c1d562eb9079760a4deaece4806967b092b583b/PyTorch/LanguageModeling/BERT/modeling.py#L287)í•˜ê³  ìˆë‹¤. ê·¸ëŸ¼ Apexì˜ FusedLayerNormê³¼ torch.nn.LayerNormì˜ ì°¨ì´ëŠ” ë¬´ì—‡ì¼ê¹Œ?

ë‘ ëª¨ë“ˆì˜ ë¬¸ì„œë¥¼ ì‚´í´ë³´ë©´ ([apex fused_layer_norm ë¬¸ì„œ](https://nvidia.github.io/apex/layernorm.html), [torch.nn.LayerNorm ë¬¸ì„œ](https://pytorch.org/docs/master/generated/torch.nn.LayerNorm.html)) ì¸í„°í˜ì´ìŠ¤, ìˆ˜ì‹ì€ ê°™ë‹¤.

$$y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta$$

ì‹¤ì œë¡œ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ë¶€ë¶„ì€ ê°ê° [NVIDIA/apex/csrc/layer_norm_cuda_kernel.cu](https://github.com/NVIDIA/apex/blob/c3fad1ad120b23055f6630da0b029c8b626db78f/csrc/layer_norm_cuda_kernel.cu#L670), [pytorch/pytorch/aten/src/ATen/native/cuda/layer_norm_kernel.cu](https://github.com/pytorch/pytorch/blob/23739654cd6e6b55c86d74608d84d3f6c3ac8cb6/aten/src/ATen/native/cuda/layer_norm_kernel.cu#L423)ì´ë‹¤.

## ì½”ë“œì˜ ì°¨ì´

[apex ì½”ë“œ](https://github.com/NVIDIA/apex/blob/c3fad1ad120b23055f6630da0b029c8b626db78f/csrc/layer_norm_cuda_kernel.cu#L279)ì—ì„œëŠ” ì•„ë˜ì²˜ëŸ¼ LayerNormì„ ê³„ì‚°í•œë‹¤.

```c++
template<typename T, typename U> __global__
void cuApplyLayerNorm(
  T* __restrict__ output_vals,
  U* __restrict__ mean,
  U* __restrict__ invvar,
  const T* __restrict__ vals,
  const int n1,
  const int n2,
  const U epsilon,
  const T* __restrict__ gamma,
  const T* __restrict__ beta
  )
{
  // Assumptions:
  // 1) blockDim.x == warpSize
  // 2) Tensors are contiguous
  //
  for (auto i1=blockIdx.y; i1 < n1; i1 += gridDim.y) {
    SharedMemory<U> shared;
    U* buf = shared.getPointer();
    U mu,sigma2;
    cuWelfordMuSigma2(vals,n1,n2,i1,mu,sigma2,buf);
    const T* lvals = vals + i1*n2;
    T* ovals = output_vals + i1*n2;
    U c_invvar = rsqrt(sigma2 + epsilon);
    const int numx = blockDim.x * blockDim.y;
    const int thrx = threadIdx.x + threadIdx.y * blockDim.x;
    if (gamma != NULL && beta != NULL) {
      for (int i = thrx;  i < n2;  i+=numx) {
        U curr = static_cast<U>(lvals[i]);
        ovals[i] = gamma[i] * static_cast<T>(c_invvar * (curr - mu)) + beta[i];
      }
    } else {
      for (int i = thrx;  i < n2;  i+=numx) {
        U curr = static_cast<U>(lvals[i]);
        ovals[i] = static_cast<T>(c_invvar * (curr - mu));
      }
    }
    if (threadIdx.x == 0 && threadIdx.y == 0) {
      mean[i1] = mu;
      invvar[i1] = c_invvar;
    }
  }
}
```

ì‹¤ì œë¡œ mu, sigmaë¥¼ ê³„ì‚°í•˜ëŠ” `cuWelfordMuSigma2`ë¥¼ ì‚´í´ë³´ë©´ ì•„ë˜ì²˜ëŸ¼ ì½”ë“œê°€ ì‘ì„±ë˜ì–´ ìˆë‹¤.

```c++
template<typename T, typename U> __device__
void cuWelfordMuSigma2(
  const T* __restrict__ vals,
  const int n1,
  const int n2,
  const int i1,
  U& mu,
  U& sigma2,
  U* buf)
{
  // Assumptions:
  // 1) blockDim.x == warpSize
  // 2) Tensor is contiguous
  // 3) 2*blockDim.y*sizeof(U)+blockDim.y*sizeof(int) shared memory available.
  //
  // compute variance and mean over n2
  U count = U(0);
  mu= U(0);
  sigma2 = U(0);
  if (i1 < n1) {
    // one warp normalizes one n1 index,
    // synchronization is implicit
    // initialize with standard Welford algorithm
    const int numx = blockDim.x * blockDim.y;
    const int thrx = threadIdx.x + threadIdx.y * blockDim.x;
    const T* lvals = vals + i1*n2;
    int l = 4*thrx;
    for (;  l+3 < n2;  l+=4*numx) {
      for (int k = 0;  k < 4;  ++k) {
        U curr = static_cast<U>(lvals[l+k]);
        cuWelfordOnlineSum<U>(curr,mu,sigma2,count);
      }
    }
    for (;  l < n2;  ++l) {
      U curr = static_cast<U>(lvals[l]);
      cuWelfordOnlineSum<U>(curr,mu,sigma2,count);
    }
    ...
```

ê³„ì†í•´ì„œ ë£¨í”„ë¥¼ ëŒë©° OnlineSumì„ í•´ê°„ë‹¤. OnlineSumì€ Welford's Online Algorithmì„ ì‚¬ìš©í•œë‹¤.

ê·¼ë° ì´ ì•Œê³ ë¦¬ì¦˜ì€ [ì´ë ‡ê²Œ ì„¤ëª…](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm)ì´ ë˜ì–´ ìˆë‹¤.

> This algorithm is much less prone to loss of precision due to catastrophic cancellation, but might not be as efficient because of the division operation inside the loop.

---

ê·¸ì— ë¹„í•´ TorchëŠ” ë‹¤ë¥´ê²Œ ê³„ì‚°ì„ í•˜ëŠ”ë°, ìš°ì„  LayerNormKernelì´ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ [ì•„ë˜ì²˜ëŸ¼ êµ¬í˜„](https://github.com/pytorch/pytorch/blob/23739654cd6e6b55c86d74608d84d3f6c3ac8cb6/aten/src/ATen/native/cuda/layer_norm_kernel.cu#L257)í•´ë†“ì•˜ë‹¤.

```c++
template <typename T>
void LayerNormKernelImplInternal(
    const Tensor& X,
    const Tensor& gamma,
    const Tensor& beta,
    int64_t M,
    int64_t N,
    T eps,
    Tensor* Y,
    Tensor* mean,
    Tensor* rstd) {
  DCHECK_EQ(X.numel(), M * N);
  DCHECK(!gamma.defined() || gamma.numel() == N);
  DCHECK(!beta.defined() || beta.numel() == N);
  const T* X_data = X.data_ptr<T>();
  const T* gamma_data = gamma.defined() ? gamma.data_ptr<T>() : nullptr;
  const T* beta_data = beta.defined() ? beta.data_ptr<T>() : nullptr;
  T* Y_data = Y->data_ptr<T>();
  T* mean_data = mean->data_ptr<T>();
  T* rstd_data = rstd->data_ptr<T>();
  cudaStream_t cuda_stream = at::cuda::getCurrentCUDAStream();
  RowwiseMomentsCUDAKernel<T>
      <<<M, cuda_utils::kCUDABlockReduceNumThreads, 0, cuda_stream>>>(
          N, eps, X_data, mean_data, rstd_data);
  LayerNormForwardCUDAKernel<T><<<M, kCUDANumThreads, 0, cuda_stream>>>(
      N, X_data, mean_data, rstd_data, gamma_data, beta_data, Y_data);
  AT_CUDA_CHECK(cudaGetLastError());
}
```

Momentsë¥¼ ê³„ì‚°í•˜ê³ , LayerNormì„ ê³„ì‚°í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. Momentsë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì€ [`RowwiseMomentsCUDAKernel`ì— êµ¬í˜„](https://github.com/pytorch/pytorch/blob/23739654cd6e6b55c86d74608d84d3f6c3ac8cb6/aten/src/ATen/native/cuda/layer_norm_kernel.cu#L22)ë˜ì–´ ìˆë‹¤.

```c++
template <typename T>
__global__ void RowwiseMomentsCUDAKernel(
    int64_t N,
    T eps,
    const T* X,
    T* mean,
    T* rstd) {
  using T_ACC = acc_type<T, true>;
  __shared__ T_ACC m_shared[C10_WARP_SIZE];
  __shared__ T_ACC v_shared[C10_WARP_SIZE];
  const int64_t i = blockIdx.x;
  T_ACC sum1 = 0;
  T_ACC sum2 = 0;
  for (int64_t j = threadIdx.x; j < N; j += blockDim.x) {
    const int64_t index = i * N + j;
    sum1 += static_cast<T_ACC>(X[index]);
    sum2 += static_cast<T_ACC>(X[index]) * static_cast<T_ACC>(X[index]);
  }
  sum1 = cuda_utils::BlockReduceSum<T_ACC>(sum1, m_shared);
  sum2 = cuda_utils::BlockReduceSum<T_ACC>(sum2, v_shared);
  if (threadIdx.x == 0) {
    const T_ACC scale = T_ACC(1) / static_cast<T_ACC>(N);
    sum1 *= scale;
    sum2 = c10::cuda::compat::max(sum2 * scale - sum1 * sum1, T_ACC(0));
    mean[i] = sum1;
    rstd[i] = c10::cuda::compat::rsqrt(sum2 + static_cast<T_ACC>(eps));
  }
}
```

ì¼ë°˜ì ìœ¼ë¡œ í‰ê· , ë¶„ì‚°ì„ êµ¬í•˜ëŠ” ì½”ë“œì™€ ê°™ì€ ì½”ë“œì´ë‹¤.

ê·¸ ì™¸ì—ëŠ” ê±°ì˜ ê°™ì€ ì½”ë“œì´ë©°, ì´ ë¶€ë¶„ì˜ ì½”ë“œë§Œ ë‹¤ë¥¸ ê²ƒìœ¼ë¡œ ë³´ì•„ ì„±ëŠ¥ì´ ë‹¤ë¥´ë‹¤ë©´ ì´ ë¶€ë¶„ìœ¼ë¡œ ì¸í•´ ë‹¬ë¼ì§ˆ ê²ƒ ê°™ë‹¤.

## History

ê·¼ë° í˜„ì¬ êµ¬í˜„ì„ ë³´ë©´ ë©”ëª¨ë¦¬ ì ‘ê·¼ ì‹œê°„ì´ ì •ë§ ëŠë¦¬ì§€ ì•ŠëŠ” ì´ìƒ Apex FusedLayerNormì´ ë” ëŠë¦´ ê²ƒ ê°™ì€ë° ê´€ë ¨ Issue, Commit historyë¥¼ ì‚´í´ë³´ì.

* [NVIDIA/apex/issues/449](https://github.com/NVIDIA/apex/issues/449)
* [Gist - layernorm vs fused](https://gist.github.com/ptrblck/8b1c6a7efd97604a7dedbf2c3edd1019)
* [pytorch/pytorch/issues/37713](https://github.com/pytorch/pytorch/issues/37713)
* [pytorch/fairseq/issues/2012](https://github.com/pytorch/fairseq/issues/2012)
* ...

Apexê°€ ë” ë¹ ë¥´ë‹¤ëŠ” ì‚¬ëŒë„ ìˆê³ , Torchê°€ ë” ë¹ ë¥´ë‹¤ëŠ” ì‚¬ëŒë„ ìˆê³ .. í•´ì„œ Commit Historyë¥¼ ë³´ë©´ `pytorch/aten/src/ATen/native/cuda/layer_norm_kernel.cu` íŒŒì¼ì˜ ì²« ì»¤ë°‹ì´ ["Add fused layer norm impl on CUDA in PyTorch (#27634)"](https://github.com/pytorch/pytorch/commit/8b87f9a5107e8b3c4f87d5297af698bb55838d81#diff-f12c726e3e8cd2b4768f8984fef27059)ì´ë‹¤..?

íƒ€ê³  ë“¤ì–´ê°€ë³´ë©´ PyTorchì— FusedLayerNormì„ ì¶”ê°€í•˜ëŠ” PRì´ê³ , ì›ë˜ëŠ” LayerNormì´ PyTorchê°€ Apexë³´ë‹¤ ë§ì´ ëŠë ¸ë‹¤. í•˜ì§€ë§Œ [í•´ë‹¹ PRì˜ ì„¤ëª…](https://github.com/pytorch/pytorch/pull/27634)ì„ ì°¸ê³ í•˜ë©´ Apexì™€ ë¹„êµí•´ì„œ ê±°ì˜ ëª¨ë“  ì¼€ì´ìŠ¤ì—ì„œ ë¹¨ë¼ì§„ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

ì ìš©ëœ ë²„ì „ì€ 1.4.0 ì´í›„ì´ê³  ê·¸ ì „ì—ëŠ” CUDA ì»¤ë„ì´ ì—†ì—ˆë˜ ê²ƒ ê°™ì§€ë§Œ, í•´ë‹¹ ì»¤ë„ì´ ì ìš©ëœ 1.4.0ì´ìƒì„ ì“°ë©´ apex FusedLayerNormì„ ì“¸ í•„ìš”ê°€ ì—†ì–´ë³´ì¸ë‹¤.

## ëŒë ¤ë³´ì

ê·¸ë˜ì„œ ì¼ë‹¨ 1.4.0ë²„ì „ì„ ê¸°ì¤€ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ë©´ PyTorch LayerNormì´ ë” ë¹ ë¥¼ ê²ƒ ê°™ì•„ì„œ ì‹¤ì œë¡œ forward, backward í•œë²ˆì”© ëŒë ¤ë³´ì•˜ë‹¤.

apexëŠ” [NVIDIA/apex#161(comment)](https://github.com/NVIDIA/apex/issues/161#issuecomment-466611317)ì²˜ëŸ¼ ì„¤ì¹˜í–ˆê³ , Tesla V100-DGXS-32GB, torch==1.4.0ì—ì„œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í–ˆë‹¤.

```python
import torch
import apex
import time

ApexLayerNorm = apex.normalization.fused_layer_norm.FusedLayerNorm
LayerNorm = torch.nn.LayerNorm

for dim in range(128, 1025, 128):
    bsz = 2048
    apex_norm = ApexLayerNorm(dim).half().cuda()
    torch_norm = LayerNorm(dim).half().cuda()

    target = torch.rand((bsz, dim)).half().cuda()

    # warm up
    for _ in range(10):
        input_tensor = torch.rand((bsz, dim)).half().cuda().contiguous()
        output1 = apex_norm(input_tensor)
        loss = torch.nn.functional.mse_loss(output1, target)
        loss.backward()

        output2 = torch_norm(input_tensor)
        loss = torch.nn.functional.mse_loss(output2, target)
        loss.backward()

    input_tensor = torch.rand((bsz, dim)).half().cuda().contiguous()

    start = time.time()
    for _ in range(1000):
        output = torch_norm(input_tensor)
        loss = torch.nn.functional.mse_loss(output, target)
        loss.backward()
    torch_end = time.time()
    for _ in range(1000):
        output = apex_norm(input_tensor)
        loss = torch.nn.functional.mse_loss(output, target)
        loss.backward()
    apex_end = time.time()

    torch_dur = (torch_end - start) / 1000
    apex_dur = (apex_end - torch_end) / 1000
    print(f"dim {dim:4d} Batch Size {bsz:3d}, Torch: {torch_dur:.8f} Apex: {apex_dur:.8f} Imp {(torch_dur - apex_dur) / apex_dur * 100:2.2f}")
```

ì´ ì½”ë“œì˜ ê²°ê³¼ëŠ” ì•„ë˜ì²˜ëŸ¼ ë‚˜ì™”ê³ 

```shell
(env) jeongukjae@server:~$ python test.py
dim  128 Batch Size 2048, Torch: 0.00031060 Apex: 0.00035981 Imp -13.67
dim  256 Batch Size 2048, Torch: 0.00030215 Apex: 0.00035082 Imp -13.87
dim  384 Batch Size 2048, Torch: 0.00033065 Apex: 0.00037036 Imp -10.72
dim  512 Batch Size 2048, Torch: 0.00029822 Apex: 0.00035301 Imp -15.52
dim  640 Batch Size 2048, Torch: 0.00031614 Apex: 0.00036779 Imp -14.04
dim  768 Batch Size 2048, Torch: 0.00030238 Apex: 0.00036041 Imp -16.10
dim  896 Batch Size 2048, Torch: 0.00029817 Apex: 0.00036967 Imp -19.34
dim 1024 Batch Size 2048, Torch: 0.00030955 Apex: 0.00036211 Imp -14.51
```

ê²°êµ­ 1.4.0ì´ë©´ apex fusedlayernormì€ í•„ìš”ì—†ëŠ” ê²ƒ ê°™ë‹¤.

ê·¸ë˜ì„œ [ë‹¤ë¥¸ ì‚¬ëŒì´ ì˜¬ë ¤ë†“ì€ ë²¤ì¹˜ë§ˆí¬ ì½”ë“œ](https://gist.github.com/ptrblck/8b1c6a7efd97604a7dedbf2c3edd1019)ë¥¼ ê°€ì ¸ì™€ì„œ ì‹¤í–‰í•´ë³´ì•˜ë‹¤.

```shell
import torch
import torch.nn as nn

torch.backends.cudnn.benchmark = True

from apex.normalization import FusedLayerNorm

import time


# Create data
x = torch.randn(64, 16, 224, 224, device='cuda')

# upstream layernorm
norm = nn.LayerNorm(x.size()[1:]).cuda()

# cudnn warmup
for _ in range(50):
    _ = norm(x)

nb_iters = 1000
torch.cuda.synchronize()
t0 = time.time()

for _ in range(nb_iters):
    _ = norm(x)

torch.cuda.synchronize()
t1 = time.time()

print('upstream layernorm {:.3f}'.format(t1 -t0))

# apex fusedlayernorm
fused_norm = FusedLayerNorm(x.size()[1:]).cuda()

# cudnn warmup
for _ in range(50):
    _ = fused_norm(x)

nb_iters = 1000
torch.cuda.synchronize()
t0 = time.time()

for _ in range(nb_iters):
    _ = fused_norm(x)

torch.cuda.synchronize()
t1 = time.time()

print('apex layernorm {:.3f}'.format(t1 -t0))

```

```shell
(env) jeongukjae@server:~$ python test2.py
upstream layernorm 2.464
apex layernorm 3.490
```

ê·¼ë° ê·¸ëŸ¼ 1.3.1ì„ ì‚¬ìš©í•´ì„œ í…ŒìŠ¤íŠ¸í•˜ë©´ ì‹¤ì œë¡œ ëŠë¦´ê¹Œ? ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

```shell
(env) jeongukjae@server:~$ python test.py
dim  128 Batch Size 2048, Torch: 0.00047603 Apex: 0.00038654 Imp 23.15
dim  256 Batch Size 2048, Torch: 0.00045056 Apex: 0.00037928 Imp 18.79
dim  384 Batch Size 2048, Torch: 0.00052916 Apex: 0.00041193 Imp 28.46
dim  512 Batch Size 2048, Torch: 0.00048436 Apex: 0.00039859 Imp 21.52
dim  640 Batch Size 2048, Torch: 0.00047772 Apex: 0.00036401 Imp 31.24
dim  768 Batch Size 2048, Torch: 0.00048790 Apex: 0.00041975 Imp 16.24
dim  896 Batch Size 2048, Torch: 0.00048001 Apex: 0.00041013 Imp 17.04
dim 1024 Batch Size 2048, Torch: 0.00050486 Apex: 0.00045190 Imp 11.72
```

1.3.1ë²„ì „ì€ Apexê°€ ë” ë¹ ë¥´ë‹¤.

---

ì—„ì²­ ì •ë¦¬ì•ˆí•˜ê³  ê¸€ì„ ì¼ì§€ë§Œ ê°„ë‹¨í•˜ê²Œ ì •ë¦¬í•´ë³´ìë©´,

* ì›ë˜ëŠ” Apex LayerNormì´ ë” ë¹ ë¥¸ ê²ƒì´ ë§ì•˜ë‹¤.
* í•˜ì§€ë§Œ torch 1.4.0ì— ì ìš©ëœ ["Add fused layer norm impl on CUDA in PyTorch (#27634)"](https://github.com/pytorch/pytorch/commit/8b87f9a5107e8b3c4f87d5297af698bb55838d81#diff-f12c726e3e8cd2b4768f8984fef27059) ì»¤ë°‹ ì´í›„ë¡œëŠ” ì„±ëŠ¥ì´ Torchê°€ ë” ì¢‹ë‹¤.
* í•˜ì§€ë§Œ mean, varianceë¥¼ êµ¬í•˜ëŠ” ë¡œì§ì´ ë‹¤ë¥¸ë°, apex ë²„ì „ì´ ë” precisionì˜ lossê°€ ëœ í•˜ë‹¤.
