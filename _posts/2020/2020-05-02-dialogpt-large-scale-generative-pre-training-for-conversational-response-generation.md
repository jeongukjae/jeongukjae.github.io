---
layout: post
title: "ðŸ“ƒ DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation ë¦¬ë·°"
tags:
  - paper
---

GPTë¥¼ ëŒ€í™”ì²´ì— ë§žë„ë¡ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì´ë‹¤. ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ì—ì„œ ë‚˜ì˜¨ ë…¼ë¬¸ì´ê³ , arxivë§í¬ëŠ” [https://arxiv.org/abs/1911.00536](https://arxiv.org/abs/1911.00536)ì´ë‹¤. ì½”ë“œëŠ” [GitHub microsoft/DialoGPT](https://github.com/microsoft/DialoGPT)ì—ì„œ ë³¼ ìˆ˜ ìžˆë‹¤.

## 1 Introduction

* > Like GPT-2, DIALOGPT is formulated as an autoregressive (AR) language model, and uses multi-layer transformer as model architecture.
* > Unlike GPT-2, however, DIALOGPT is trained on large-scale dialogue pairs/sessions extracted from Reddit discussion chains.
* ë…¼ë¬¸ ì €ìžë“¤ì€ DialoGPTê°€ ëŒ€í™” íë¦„ì—ì„œì˜ Source, Targetì˜ joint distributionì„ í•™ìŠµí•  ê²ƒì„ ê¸°ëŒ€í–ˆë‹¤ê³  í•œë‹¤.
* DSTC-7ìœ¼ë¡œ í‰ê°€í–ˆê³ , 6,000ê°œì˜ reddit í¬ìŠ¤íŒ…ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ë½‘ì•˜ë‹¤ê³  í•œë‹¤

## 2 Dataset

* Reddit í¬ìŠ¤íŒ…ì—ì„œ root node -> leaf nodeë¡œ ê°€ëŠ” pathë¥¼ ì¶”ì¶œí•´ì„œ instanceë¡œ ì‚¬ìš©í•¨
* ë‹¤ë§Œ ì•„ëž˜ ì¡°ê±´ì€ ì œì™¸
  1. URLì´ source, targetì— ìžˆì„ ë•Œ
  2. targetì´ 3ê°œ ì´ìƒì˜ ë‹¨ì–´ ë°˜ë³µì´ ì¡´ìž¬í•  ë•Œ
  3. ìžì£¼ ë“±ìž¥í•˜ëŠ” top 50 ì˜ë‹¨ì–´(a, the, of)ê°€ í•˜ë‚˜ë„ í¬í•¨ë˜ì–´ ìžˆì§€ ì•Šì„ ë•Œ -> ì™¸êµ­ì–´ë¡œ ìƒê°í•¨
  4. special markerê°€ ì¡´ìž¬í•  ë•Œ `[`, `]`
  5. source, target sequenceê°€ 200ë‹¨ì–´ë¥¼ ë„˜ì„ ë•Œ
  6. ê³µê²©ì ì¸ ë‹¨ì–´ë¥¼ í¬í•¨í•  ë–„
  7. ë§Žì´ ë‹¨ì¡°ë¡œìš´ ë¬¸ìž¥

## 3 Method

### 3.1 Model Architecture

* GPT-2, 12~24Lë¡œ ì„¸íŒ…í•¨
* BPE ì‚¬ìš©
* SOURCE ë¬¸ìž¥ì„ ë‹¤ ì´ì–´ë¶™ì¸ë‹¤ìŒ Target ë¬¸ìž¥ì„ Generatingí•˜ë„ë¡ ìž‘ì„±í•¨

### 3.2 Mutual Information Maximization

* Open domain text generation ëª¨ë¸ì€ bland, uninformativeí•œ ìƒ˜í”Œì„ ë§Žì´ ìƒì„±í•¨
* ê·¸ëž˜ì„œ MMI scoring functionì„ ì‚¬ìš©í•¨
* top-K ìƒ˜í”Œë§ í›„ Rerank
* ê·¼ë° RL ë°©ì‹ì„ í™œìš©í•´ì„œ Policy Gradientë¥¼ ì‚¬ìš©í•´ optimizeì‹œì¼œë³´ë‹ˆ local optimaì— ë„ˆë¬´ ìž˜ ë¹ ì§„ë‹¤
  * ì•„ë§ˆ transformerì˜ representation power ë•Œë¬¸ì¸ ê²ƒìœ¼ë¡œ ì¶”ì¸¡
  * future workë¡œ ë‚¨ê²¨ë‘”ë‹¤ í•¨

## 4 Result

* 117M, 345M, 762M ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸í•¨ ì„¸ë¶€ ì‚¬í•­ì€ Radford et al. (2018)ê³¼ ê°™ìŒ
* Azure Cognitive Serviceì™€ ë¹„êµí•¨
* Beam Search ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ì´ ê½¤ ì˜¬ë¼ê°
  * ê·¼ë° grounding informationì´ ì—†ëŠ”ë° ì–´ë–»ê²Œ ìž˜ë˜ëƒ? -> ì•„ë§ˆ pretraining ë™ì•ˆ ì–»ì–´ë‚´ëŠ” informationì´ ë§Žì•„ì„œ grounding documentì—†ì–´ë„ ê´œì°®ì€ ë“¯ í•˜ë‹¤.

{%include image.html url="/images/2020-05-02-dialogpt/fig1.png" class='noshadow' %}

ê·¸ ì™¸ì—” ê°„ë‹¨í•˜ê²Œ ì½ì–´ë³´ë©´ ì¢‹ì„ ë“¯

ì´ê±° ìƒ˜í”Œì€ ë˜ê²Œ ì‹ ê¸°í•˜ë‹¤

{%include image.html url="/images/2020-05-02-dialogpt/fig2.png" class='noshadow' width=60 %}

{%include image.html url="/images/2020-05-02-dialogpt/fig3.png" class='noshadow' width=60 %}

ì•„ëž˜ ê²°ê³¼ëŠ” ì§„ì§œ ë†€ëžë‹¤. Human Responseì— ë²„ê¸ˆê°€ëŠ” í€„ë¦¬í‹°ë¥¼ ìƒì„±í•´ë‚¸ë‹¤. ë‹¤ë§Œ ì•„ì‰¬ìš´ ì ì€ 345M, 762M ë³´ì—¬ì¤„ ê±°ë©´ 117Më„ ì–´ëŠì •ë„ì¸ì§€ëŠ” ë³´ì—¬ì£¼ì—ˆìœ¼ë©´ ì–´ë• ì„ê¹Œì´ë‹¤. Table 2, 3ì—ì„œ ê·¸ë ‡ê²Œ ë‚˜ì™€ì„œ ì•„ëž˜ì²˜ëŸ¼ ë¹„êµí•œê±´ê°€??

{%include image.html url="/images/2020-05-02-dialogpt/fig4.png" class='noshadow'%}

## 6 Limitations and risks

ì–´ì©” ìˆ˜ ì—†ëŠ” ì ì´ê¸´ í•˜ì§€ë§Œ Generationì´ ì–´ëŠì •ë„ëŠ” ìœ„í—˜í•˜ê¸´ í•˜ë‹ˆê¹Œ..

> Despite our efforts to minimize the amount of overtly offensive data prior to training, DI- ALOGPT retains the potential to generate output that may trigger offense. Output may reflect gen- der and other historical biases implicit in the data. Responses generated using this model may exhibit a propensity to express agreement with proposi- tions that are unethical, biased or offensive (or the reverse, disagreeing with otherwise ethical state- ments).
