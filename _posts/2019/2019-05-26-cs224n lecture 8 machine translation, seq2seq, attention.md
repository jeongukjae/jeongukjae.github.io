---
layout: post
title: ğŸ“• CS224n Lecture 8 Machine translation, Seq2seq, Attention
tags:
  - nlp
  - cs224n
  - machine learning
---

CS224n 8ë²ˆì§¸ ê°•ì˜ë¥¼ ë“£ê³  ì •ë¦¬í•œ í¬ìŠ¤íŠ¸! machine translationì— ëŒ€í•´ ì‚´í´ë³´ê³  seq2seqì™€ attentionì„ ì‚´í´ë³¸ë‹¤.

## Machine Translation

### Pre-neural translation

ì¼ë‹¨ ê¸°ê³„ë²ˆì—­ì€ source languageì˜ ë§ë“¤ì„ target languageì˜ ë§ë¡œ ì˜®ê¸°ëŠ” íƒœìŠ¤í¬ì´ë‹¤. 1950'sê¹Œì§€ëŠ” ëŒ€ë¶€ë¶„ rule baseë¡œ êµ¬í˜„í–ˆë‹¤. (ì‚¬ì „ì„ ì´ìš©í•œ mappingì´ ë§ì•˜ë‹¤) 1990's - 2010's statistical machine translatin ë°©ì‹ì„ ì‚¬ìš©í–ˆë‹¤. dataë¡œë¶€í„° probability modelì„ ì‚¬ìš©í–ˆê³ , ì´ë¥¼ SMTë¼ê³  ì¤„ì—¬ë¶€ë¥¸ë‹¤.

$$ argmax_y P(y|x) = argmax_y P(x|y)P(y) $$

$$P(x|y)$$
ê°€ translation modelì´ê³  $$P(y)$$ê°€ LMì´ë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©´ ì •ë§ ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•˜ë‹¤..

#### alignment

SMTì—ì„œëŠ” alignmentë¥¼ í•™ìŠµí•´ì•¼í•œë‹¤.
$$P(x,a|y)$$
ë¡œ ë‚˜íƒ€ë‚´ê³ , wordë¥¼ ë§¤í•‘í•˜ê³  ë‚˜ì„œ ê°ê°ì˜ ì–¸ì–´ì— ë§ëŠ” ì–´ìˆœìœ¼ë¡œ ë°°ì—´í•˜ê¸° ìœ„í•´ alignmentë¥¼ ë”°ë¡œ í•™ìŠµí•œë‹¤.

{% include image.html url="/images/cs224n/8-1.png" description="alignment" %}

ê·¼ë° ì–´ë–¤ ë‹¨ì–´ë“¤ì€ counterpartë„ ì—†ê³ , alignì„ í•˜ëŠ” ê²ƒì´ "one to many", "many to many", "many to one" ë“±ë“± ì‹¤ì œë¡œ ë§¤í•‘ë˜ì§€ ì•ŠëŠ” ê²½ìš°ê¹Œì§€ ë„ˆë¬´ ë§ì•„ì„œ ì‰½ì§€ ì•Šë‹¤. í™•ë¥ ì ì¸ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê²ƒ ìì²´ê°€ ëª¨ë“  ë‹¨ì–´ë“¤ì„ ëŒì•„ì•¼ í•˜ëŠ” ê²ƒì¸ë°, ë„ˆë¬´ ê³„ì‚° ë¹„ìš©ì´ í¬ë‹¤.

## NMT

ì ê·¸ë˜ì„œ NMT(neural machine translation)ì„ í•œë‹¤.

{% include image.html url="/images/cs224n/8-2.png" description="NMT" %}

ì´ê±¸ seq2seqë¡œ í’€ì–´ë‚¸ë‹¤. ì ê¹ seq2seqë¡œ í‘¸ëŠ” ë¬¸ì œë¥¼ ë§í•´ë³´ìë©´, summarization, dialogue, parsing, code generation ê°™ì€ ë¬¸ì œë“¤ì´ ìˆë‹¤. (conditional LMì˜ ì¼ì¢…)

ìœ„ì²˜ëŸ¼ `<END>`ê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ ê³„ì†í•˜ëŠ”ë°, ì´ê²Œ ì•ˆë‚˜íƒ€ë‚˜ë©´..? ì´ë¼ëŠ” ìƒê°ì„ í–ˆì§€ë§Œ, ì–´ëŠì •ë„ ë¦¬ë°‹ì„ ë‘”ë‹¤ëŠ” ë§ì„ ë“¤ì—ˆë‹¤.

decodingì„ ìœ„ì²˜ëŸ¼ í•˜ëŠ” ë°©ì‹ì´ greedy decodingì¸ë°, ì´ê²Œ ë¬¸ì œì ì´ ìˆë‹¤. ì•ì˜ ê²ƒë§Œ ë³´ê³  ì˜ˆì¸¡ì„ í•˜ë‹ˆ ê·¸ë ‡ê²Œ ëœë‹¤.

ê·¸ë˜ì„œ beam search decoding ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ”ë° ì•„ë˜ì™€ ê°™ì€ ë°©ë²•ì´ë‹¤.

{% include image.html url="/images/cs224n/8-3.png" description="Beam Search Decoding" %}

### ì¥ë‹¨ì 

SMTì™€ ë¹„êµí•´ì„œ NMTëŠ” ë§ì€ ì¥ì ì„ ê°€ì§€ê³  ìˆë‹¤. ì‚¬ëŒì´ ë³´ê¸°ì— í›¨ì”¬ fluentí•œ LMì„ êµ¬í˜„í•  ìˆ˜ ìˆê³ , contextë¥¼ í™œìš©í•˜ëŠ” ëŠ¥ë ¥ ë˜í•œ ë›°ì–´ë‚˜ ë³´ì¸ë‹¤. phrase similaritiesë¥¼ ì‚¬ìš©í•˜ëŠ” ëŠ¥ë ¥ ë˜í•œ ë›°ì–´ë‚˜ë‹¤. ë˜í•œ subcomponentë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. SMTëŠ” ê·¸ íŠ¹ì„±ìƒ subcomponentë¥¼ ë§ì´ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ, NMTëŠ” single neural networkë¥¼ end-to-endë¡œ ìµœì í™” í•˜ë©´ ëœë‹¤.

í•˜ì§€ë§Œ SMTì— ë¹„í•´ì„œ ë””ë²„ê¹…ì´ í›¨ì”¬ ì–´ë µê³ , ì œì–´í•˜ê¸°ë„ ì–´ë µë‹¤. ì ë‹¹í•œ evaluation ë°©ë²• ë˜í•œ ì—†ë‹¤. ê·¸ë˜ì„œ BLEUë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ì´ ë˜í•œ ì™„ë²½í•˜ì§„ ì•Šì§€ë§Œ, ìˆ˜ì¹˜ìƒìœ¼ë¡œ ë³´ì—¬ì¤„ ë§Œí•œ ë‹¤ë¥¸ ëŒ€ì•ˆì´ ë§ˆë•…ì¹˜ ì•Šê¸° ë•Œë¬¸ì— ì‚¬ìš©í•œë‹¤ê³  í•œë‹¤.

### BLEU

Bilingual Evaluation Understudyë¡œ, ì‚¬ëŒì´ ë²ˆì—­í•œ ê²ƒê³¼ ê¸°ê³„ê°€ ë²ˆì—­í•œ ê²ƒì„ ìš°ì„  n-gram similarityë¡œ ìŠ¤ì½”ì–´ë¥¼ ë§¤ê¸´ë‹¤. ê·¸ë¦¬ê³  ë„ˆë¬´ í•¨ì¶•í•˜ì—¬ ë²ˆì—­í•œ ë¬¸ì¥ì— ëŒ€í•´ì„œëŠ” íŒ¨ë„í‹°ë¥¼ ì¤€ë‹¤. ê·¸ë ‡ê²Œ ì‚¬ëŒì´ ì§ì ‘ ë²ˆì—­í•œ ê²ƒê³¼ ê¸°ê³„ê°€ ë²ˆì—­í•œ ê²ƒì˜ ìœ ì‚¬ë„ë¥¼ íŒë‹¨í•œë‹¤.

### ì ê·¸ë˜ì„œ ì˜ í’€ë ¸ëŠ”ê°€

ìœ„ì—ì„œ ë§í•œ ê²ƒìœ¼ë¡œ ìƒë‹¹íˆ ë§ì€ ë¬¸ì œë¥¼ í•´ê²°í–ˆì„ ê²ƒ ê°™ì§€ë§Œ,

* ì‚¬ì „ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ëŠ” ë‚˜íƒ€ë‚´ê¸° í˜ë“¤ë‹¤.
* ìƒë‹¹íˆ ê¸´ ë¬¸ì¥ì— ëŒ€í•´ì„œ contextë¥¼ í™œìš©í•˜ê¸° í˜ë“¤ë‹¤.
* common senseë¥¼ í™œìš©í•˜ê¸° í˜ë“¤ë‹¤.
  * ì˜ˆë¥¼ ë“¤ì–´ì„œ paper jam(í”„ë¦°í„°ì— ì¢…ì´ê°€ ë¼ì¸ ê²ƒ)ì„ ì‹¤ì œë¡œ ì¢…ì´ë¡œ ë‹´ê·¼ ì¼ì´ë¼ê³  ë²ˆì—­í•˜ëŠ” ë“±ì˜ í˜„ìƒì´ ìˆë‹¤.
* training ë°ì´í„°ì— ë”°ë¼ biasë¥¼ ê°€ì§„ë‹¤. ê°•ì˜ì—ì„œëŠ” sex-neutralí•œ ë‹¨ì–´ì„ì—ë„ training ë°ì´í„°ì— ë”°ë¼ biasë¥¼ ê°€ì§ì„ ë³´ì—¬ì£¼ì—ˆë‹¤. (nurse -> she, programmer -> he??)
* í•´ì„ ë¶ˆê°€ëŠ¥í•œ ë¬¸ì¥ì— ëŒ€í•´ ì„ì˜ì˜ ë¬¸ì¥ì„ ë±‰ì–´ë‚¸ë‹¤.

ê·¸ë˜ì„œ ìœ„ì˜ ë¬¸ì œë“¤ì´ ìˆì–´ ë” ë‚˜ì€ NMTë¥¼ ë§Œë“¤ê³ ì ê³ ì•ˆí•´ ë‚¸ ê¸°ë²•ì´ attentionì´ë‹¤.

## Attention

seq2seqì˜ ë¬¸ì œì ì€ encoderì—ì„œ decoderë¡œ ë„˜ì–´ê°ˆ ë•Œ, í•˜ë‚˜ì˜ hidden stateë§Œì„ ê°€ì§€ê¸° ë•Œë¬¸ì— information bottleneckì´ ë  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ë‹¤. ê·¸ë˜ì„œ ì´ ì ì„ decoderì˜ ê° stepì„ encoderë¡œ ì§ì ‘ ì—°ê²°í•˜ìëŠ” ì ì´ë‹¤.[^attention]

{% include image.html url="/images/cs224n/8-4.png" description="Attention" %}

### ê·¸ë˜ì„œ ê²°ê³¼ëŠ”

ì¼ë‹¨ ê°€ì¥ ì¤‘ìš”í•œ NMT ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆë‹¤. ê·¸ë¦¬ê³  bottleneck ë¬¸ì œë„ í•´ê²°í•˜ì˜€ë‹¤. direct connectionì´ ìƒê¸°ë‹ˆ vanishing gradient problemë„ ë§ì´ í•´ê²°ë˜ì—ˆë‹¤. NMTì˜ ë‹¨ì ì´ë¡œ í‰ê°€ë°›ë˜ ë””ë²„ê·¸ í•˜ê¸° ì–´ë µë‹¤ë˜ ë¬¸ì œë„ ì–´ëŠì •ë„ í’€ë ¸ë‹¤. attentionì„ ì‹œê°í™”í•  ê²½ìš° alignmentì²˜ëŸ¼ ë‚˜ì˜¨ë‹¤.

[^attention]: [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf) Attention Is All You Needë¼ëŠ” ì œëª©ì˜ ë…¼ë¬¸ìœ¼ë¡œ Attentionì„ ì œì‹œí•œ ë…¼ë¬¸ì´ë‹¤. ê²€ìƒ‰í•´ë³´ë‹ˆ CS224nì˜ ë‚˜ì¤‘ì˜ suggested readingsì— ìˆë˜ë° ë¯¸ë¦¬ ì½ì–´ë†“ê³  ì‹¶ë‹¤.. ã… ã… 
