---
layout: post
title: ğŸ“• CS224n Lecture 11 ConvNets for NLP
tags:
  - nlp
  - cs224n
  - machine learning
---

11ê°•! ë§ˆì§€ë§‰ ê³¼ì œì¸ ê³¼ì œ 5ë„ ìŠ¬ìŠ¬ ë§ˆê°ìœ¼ë¡œ ë³´ì¸ë‹¤. ê°•ì˜ì—ì„œë„ second halfë¼ê³  í•œë‹¤! ì´ì œë¶€í„° ê±°ì˜ ì†Œê°œì— ê°€ê¹ë‹¤ê³  í•œë‹¤.

* [slide](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture11-convnets.pdf)
* [video](https://www.youtube.com/watch?v=EAJoRA0KX7I)

Suggested Readings. ë‚˜ì¤‘ì— ì½ì–´ë´ì•¼ì§€

1. [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)
2. [A Convolutional Neural Network for Modelling Sentences](https://arxiv.org/abs/1404.2188)

ì´ê±´ ì½ìœ¼ë©´ ì¢‹ë‹¤ëŠ” ì±…

[Natural language processing with PyTorch : build intelligent language applications using deep learning](https://searchworks.stanford.edu/view/13241676)

ê°•ì˜ ì´ˆë°˜ì— CNNì— ê´€í•œ ê°„ëµí•œ ì„¤ëª…ì´ ë‚˜ì™€ìˆëŠ”ë°, ì´ê²ƒì€ CS231nì—ì„œ ë” ìì„¸í•˜ê²Œ ì•Œë ¤ì¤€ë‹¤. í•˜ì§€ë§Œ í•´ë‹¹ ê°•ì˜ ì´ˆë°˜ì„ ì´ë¯¸ ë“¤ì–´ì„œ ì­‰ ë„˜ê¸°ë©´ì„œ ë“¤ì—ˆë‹¤.

## Why CNNs?

CNNì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œ RNNê³¼ êµ¬ë¶„ë˜ëŠ” CNNì˜ ì¥ì ê³¼ RNNì˜ ë‹¨ì ì„ ì•Œì•„ë³´ì.

ìš°ì„  RNNì€ phraseë¥¼ prefix context ì—†ì´ ì¡ì•„ë‚´ì§€ ëª»í•œí•˜ê³ , phraseë¥¼ ì¡ì•„ë‚¼ ë•Œ ë‹¨ì–´ë¥¼ ë„ˆë¬´ ë§ì´ ì¡ì•„ë‚¸ë‹¤.

í•˜ì§€ë§Œ CNNì€ íŠ¹ì •í•œ ê¸¸ì´ì˜ word subsequenceë¥¼ ëª¨ë‘ ë§Œë“¤ì–´ ê³„ì‚°í•˜ë¯€ë¡œ, ë¬¸ë²•ì ìœ¼ë¡œ ì˜³ì€ phraseë§Œì„ ì¡ì•„ë‚´ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤.

## Single Layer CNN for Sentence Classification

Sentence Classificationì— ê´€í•œ Yoon Kim (2014)ì˜ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ë©´ ì¢‹ë‹¤ê³  í•œë‹¤. í•´ë‹¹ ë…¼ë¬¸ì˜ ì½”ë“œëŠ” [github yoonkim/CNN_sentence](https://github.com/yoonkim/CNN_sentence)ì— ìˆë‹¤.

CNNì„ sentence classificationì— í™œìš©í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•œë‹¤. ì£¼ë¡œ sentiment ë¶„ì„ì„ ìœ„í•œ ìš©ë„ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. [A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1510.03820) ë„ ë‚˜ì¤‘ì— ì½ì–´ë³´ì.

ê°•ì˜ì—ì„œ ì¶”ê°€ë¡œ ì¢€ ë” ì‚´í´ë³¼ ìˆ˜ ìˆëŠ” ë‚´ìš©, í‚¤ì›Œë“œë¡œ ë‚˜ì˜¨ ê²ƒì€ "Multiple filterë¥¼ ì´ìš©í•˜ë©´ ì–´ë–¨ê¹Œ?", "Multiple Channelì„ ì´ìš©í•˜ë©´ ì–´ë–¨ê¹Œ?", Dropout, BatchNorm, 1x1 convolution ë“±ì´ë‹¤. ì•„ë˜ëŠ” ê·¸ ìƒì„¸í•œ ë‚´ìš© + ì¶”ê°€ ë§í¬

* [Batch Normalization](https://arxiv.org/abs/1502.03167)
* [Network in Network (1x1 convolution)](https://arxiv.org/abs/1312.4400)
* [Recurrent Continuous Translation Models](https://www.aclweb.org/anthology/D13-1176) : CNNì„ encodingì— ì‚¬ìš©í•˜ê³  RNNì„ decodingì— ì‚¬ìš©í•´ì„œ ê¸°ê³„ë²ˆì—­í•˜ëŠ” ë°©ë²•
* [Character-Aware Neural Language Models](https://arxiv.org/abs/1508.06615)
* [Learning Character-level Representations for Part-of-Speech Tagging](http://proceedings.mlr.press/v32/santos14.pdf) : word embedding
* [VDCNN](https://arxiv.org/abs/1606.01781)
* [QRNN](https://arxiv.org/abs/1611.01576)

ì—¬íŠ¼ RNNì€ ëŠë¦¬ê³ , ê·¸ë˜ì„œ ë” ë‹¤ì–‘í•œ ë°©ë²•ì„ ì°¾ëŠ”ë‹¤.
