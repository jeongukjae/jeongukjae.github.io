---
title: "ğŸ“ƒ DistilBert ë¦¬ë·°"
layout: post
tags:
  - paper
  - nlp
---

ì˜¤ëŠ˜ì€ huggingfaceê°€ [huggingface/transformers](https://github.com/huggingface/transformers) ë ˆí¬ì§€í† ë¦¬ì— ìì²´ì ìœ¼ë¡œ ê³µê°œí•œ ëª¨ë¸ì¸ DistilBertë¥¼ ì½ê³  ì •ë¦¬í•´ë³¸ë‹¤. ë‹¤ë¥¸ ì •ë¦¬ì²˜ëŸ¼ í•œë²ˆ ì½ê³  ë§ ë¶€ë¶„ì€ ë‹¤ ì œì™¸í•œë‹¤.

ì› ë…¼ë¬¸ì€ [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)ë¡œ ê°€ë©´ ì½ì„ ìˆ˜ ìˆë‹¤.

## Abstract

ìµœê·¼ ë”ìš± ì˜ í•™ìŠµì„ í•˜ê¸° ìœ„í•´ì„œ pretrain -> fine tuningìœ¼ë¡œ ê°€ëŠ” ë°©ë²•ì´ ë§ì•„ì§€ê³  í”í•´ì¡Œì§€ë§Œ, ëª¨ë¸ ìì²´ê°€ ë„ˆë¬´ í¬ê¸° ë•Œë¬¸ì— ì œí•œëœ í™˜ê²½ì—ì„œëŠ” êµ‰ì¥íˆ ì‚¬ìš©í•˜ê¸° í˜ë“¤ë‹¤. ê·¸ë˜ì„œ huggingfaceì—ì„œ DistilBertë¼ëŠ” general purpose language representation modelì„ ë§Œë“¤ì–´ë³´ì•˜ë‹¤ê³  í•œë‹¤. BERTë¥¼ 40% ì •ë„ ì¤„ì´ê³  60%ë‚˜ ë¹ ë¥´ê²Œ ì—°ì‚°í•˜ë©´ì„œ 97%ì˜ ì„±ëŠ¥ì„ ìœ ì§€í–ˆë‹¤ê³  í•œë‹¤.

## 1. Introduction

{% include image.html url="/images/2019-10-27-distilbert/fig1.png" description="ìµœê·¼ pretraining ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ìˆ˜" %}

ì´ë ‡ê²Œ í° ëª¨ë¸ë“¤ì´ ë§ì´ ë‚˜ì˜¤ê³  ìˆëŠ”ë°, Schwartz et al. [2019], Strubell et al. [2019]ì—ì„œ ì–¸ê¸‰í•œ ê²ƒì²˜ëŸ¼ í•´ë‹¹ ëª¨ë¸ì„ ì—°ì‚°í•˜ê¸° ìœ„í•œ ì»´í“¨íŒ… íŒŒì›Œê°€ ê¸‰ê²©íˆ ì¦ê°€í•˜ê³  ìˆë‹¤. ê·¸ë¦¬ê³  NLP íŠ¹ì„± ìƒ on-deviceë‚˜ real-timeì—ì„œ í™œìš©í•  ê°€ì¹˜ê°€ ë§ì„í…ë° ì´ëŸ° ì¶”ì„¸ê°€ í™œìš©í•  ìˆ˜ ìˆëŠ” ê¸¸ì„ ë§‰ëŠ” ê²ƒ ê°™ë‹¤ê³  í•œë‹¤.

## 2. Knowledge distillation

Knowledge Distillation [Bucila et al., 2006, Hinton et al., 2015]ì€ larger model(teacher model)ë¡œë¶€í„° compact model(student model)ì„ ë§Œë“¤ì–´ë‚´ëŠ” ë°©ë²•ì´ë‹¤. ì´ê²Œ ì‘ì€ ëª¨ë¸ì„ ë°”ë¡œ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒë³´ë‹¤ ì˜ë¯¸ìˆëŠ” ì´ìœ ëŠ” near-zeroì¸ í™•ë¥ ë“¤ë„ í•™ìŠµí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ê³ ì–‘ì´ ì‚¬ì§„ì„ ë¶„ë¥˜í•œë‹¤ê³  í•  ë•Œ ì°¨ì™€ í˜¸ë‘ì´ì— ëŒ€í•œ í™•ë¥ ì´ 0ì— ê°€ê¹ê² ì§€ë§Œ, í˜¸ë‘ì´ì— ëŒ€í•œ í™•ë¥ ì´ ë” í´ ê²ƒì´ê³ , ê·¸ëŸ° ì •ë³´ë„ í•™ìŠµì´ ë˜ê¸° ë•Œë¬¸ì— ì˜ë¯¸ê°€ ìˆë‹¤.

studentë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œ teacherì˜ outputì„ ê·¸ëŒ€ë¡œ ì´ìš©í•œë‹¤. teacherì˜ outputì˜ ëª¨ë¸ì˜ outputì´ê¸° ë•Œë¬¸ì— soft target probì¸ë°, ì´ probì„ ë¹„êµí•˜ëŠ” lossê°€ distillation lossì´ë‹¤. ($$L_{ce} = \sum_i t_i * \log(s_i) $$, $$t_i$$ê°€ teacher modelì˜ soft target prob) ì´ $$L_{ce}$$ì™€ í•¨ê»˜ Hinton et al. [2015]ë¥¼ ë”°ë¼ softmax-temperature $$p_i = \frac {\exp{(z_i / T)}} {\sum_j \exp{(z_j / T)}}$$ ë¥¼ ì‚¬ìš©í•œë‹¤. $$T$$ê°€ output distributionì˜ smoothnessë¥¼ ê²°ì •í•œë‹¤. training ë™ì•ˆì—ë§Œ $$T$$ë¥¼ ì¡°ì •í•˜ê³  inference ì‹œê°„ì—ëŠ” 1ë¡œ ì„¤ì •í•´ì„œ standard softmaxë¡œ ì‚¬ìš©í•œë‹¤.

final training lossëŠ” distillation loss $$L_{ce}$$ì™€ BERTì—ì„œ ì‚¬ìš©í•œ $$L_{mlm}$$ì˜ linear combinationì´ë¼ê³  í•œë‹¤.

## 3. DistilBERT: a distilled version of BERT

student layerì˜ êµ¬ì¡°ëŠ” BERTë‘ ë˜‘ê°™ì€ë° token type embeddingì´ë‘ pooler layerëŠ” ì—†ì–´ì¡Œê³ , transformer blockì„ ë‘ë°°ë¡œ ì¤„ì˜€ë‹¤. ê·¸ë¦¬ê³  initializationì€ teacherì˜ ë ˆì´ì–´ ë‘ê°œë‹¹ í•˜ë‚˜ë¥¼ ì·¨í–ˆë‹¤.

{% include image.html url="/images/2019-10-27-distilbert/fig2.png" description="DistilBertì˜ ì„±ëŠ¥ ë¹„êµí‘œ" %}

---

## ê¸°íƒ€

ablation studyì—ì„œ Masked LM lossë¥¼ ì—†ì• ëŠ” ê²ƒì€ ìƒê°ë³´ë‹¤ ì„±ëŠ¥ì— í° ì˜í–¥ì´ ì—†ì—ˆë‹¤ê³ .

ê·¸ë¦¬ê³  distillationì— ëŒ€í•œ ë‹¤ë¥¸ ì‹œë„ë“¤ë„ ì°¾ì•„ë³´ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ BERTë¥¼ LSTMìœ¼ë¡œ distillationìœ¼ë¡œ ì§„í–‰í•œ Tang et al. [2019]ì´ë‚˜, SQuADì— ì´ë¯¸ fine-tuningëœ ê²ƒì„ distillateí•œ Chatterjee [2019]ë“±ì´ ì¢‹ì„ ê²ƒ ê°™ë‹¤. ê·¸ë¦¬ê³  multitask modelì„ distillateí•œ  Yang et al. [2019]ë„ ì½ê¸°ì— ì¢‹ì„ ê²ƒ ê°™ë‹¤.

## ì½ì–´ë³´ê³  ì‹¶ì€ ê²ƒë“¤

* Knowledge Distillation [Bucila et al., 2006, Hinton et al., 2015]
* Hinton et al. [2015]
* Chatterjee [2019]
* Tang et al. [2019]
* Yang et al. [2019]
