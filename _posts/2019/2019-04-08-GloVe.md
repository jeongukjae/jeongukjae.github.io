---
layout: post
title: 📃 GloVe 논문 정리해보기
tags:
  - paper
---

이 논문은 cs224n 강의 2강에서 suggested readings로 추천된 논문이다. 스탠포드에서 작성한 논문이고, 영미권에서 베이스로 활용이 많이 된다고 해서 따로 정리를 해보기로 헀다! 사실 이번 포스트는 내가 다시 보기 위해 간단하게 정리하는 것이라 설명이 부족하다.

## Introduction

word vector를 학습하는 방법에는 두가지 방법이 있다. 하나는 global matrix factorization methods이고, 다른 하나는 local context window methods이다. 전자에는 LSA 같은 것이 해당되고, 후자에는 skipgram같은 것이 해당된다. LSA와 같은 모델들은 통계학적 정보들을 효율적으로 극대화시키는 대신 analogy task에는 형편없다. skipgram과 같은 모델들은 그에 비해 analogy에는 특화되어 있으나, 통계적인 정보들을 제대로 수집하기가 힘들다.

그래서 이 논문에서 weighted least squares model을 소개하는데 global word-word co-occurance counts를 훈련한 모델이고, 통게적인 정보도 잘 활용하게 만들었다고 한다. [^glove]

## The GloVe Model

$$X_{ij}$$가 기본 단위인 word-word co-occurance count를 $$X$$라고 한다. 그리고 $$X_i = \sum_k X_{ik}$$라 한다. context word $$i$$에서 word $$j$$가 나타날 확률은

$$P_{ij} = P(j|i) = \frac {X_{ij}} {X_i}$$

이다. 여기서 각각의 단어가 나타날 확률의 비율을 GloVe에서 활용하게 되는데 세 단어 $$i$$, $$j$$, $$k$$에 대해 아래처럼 적을 수 있다.

$$ F(w_i, w_j, \tilde {w}_k ) = \frac {P_{ik}} {P_{jk}}$$

$$\tilde{w}$$는 context word vector인데, word2vec이 $$u$$, $$v$$ 벡터를 나누어쓰는 것과 비슷하게 생각하면 될듯 싶다. 자 위의 식에서 $$F$$가 두 단어의 차이에 의존적이니 이렇게 바꾸고,

$$ F(w_i - w_j, \tilde {w}_k ) = \frac {P_{ik}} {P_{jk}}$$

또 인자는 벡터인데 반환하는 것은 스칼라값이니 이렇게 바꾸자

$$F((w_i - w_j)^\intercal \tilde {w}_k ) = \frac {P_{ik}} {P_{jk}}$$

근데 여기서 알아야 할 점이, word-word co-occurance matrix $$X$$랑, word $$w$$, context word $$\tilde{w}$$랑 구분이 모호하다. 그래서 $$X$$를 symmetric하게, $$w$$는 $$\tilde{w}$$와 바꿔쓸 수 있게 해야한다. 그러기 위해서 $$F$$가 group $$(\mathbb{R}, +)$$와 $$(\mathbb{R}, \times)$$에 대해 homomorphism함을 필요로 한다. [^homomorphism] 그러한 homomorphism을 보장받으면 이렇게 수정이 가능하다.

$$F((w_i - w_j)^\intercal \tilde {w}_k ) = F(w_i^\intercal \tilde {w}_k - w_j^\intercal \tilde {w}_k) = \frac {F(w_i^\intercal \tilde {w}_k)} {F(w_j^\intercal \tilde {w}_k)} $$

$$F$$는 마치 exp와 비슷하게 풀어지므로, 이런식으로 유도해보자. (맨 위의 식 참고)

$$w_i^\intercal \tilde {w}_k = \log P_{ik} = \log {X_{ik}} - \log {X_i}$$

근데 이게 $$\log {X_i}$$ 항이 $$k$$$에 독립적이라 이런식으로 bias로 정리가능하다.

$$w_i^\intercal \tilde {w}_k + b_i + \tilde{b}_k = \log {X_{ik}}$$

근데 여기서 또 문제점이 $$X$$가 0이 나올수 있다는 점..인데, 이걸 $$\log (X_{ik}) \rightarrow \log (1 + X_{ik})$$와 같은 형식으로 $$X$$의 sparsity를 유지하면서 divergence를 피한다고 한다. 자 여튼 여기서 cost function을 뽑아내는데, 그 식이 아래와 같다.

$$J = \sum_{i,j = 1}^V f(X_{ij}) ( w_i^\intercal \tilde{w}_j +b_i + \tilde{b}_j - \log {X_{ij}} )^2$$

이 식을 보면서 어느정도 떠오른 아이디어는 "$$\log {X_{ij}}$$가 실제 co-occurance이고, $$w_i^\intercal \tilde{w}_j +b_i + \tilde{b}_j$$는 word vector로부터 뽑아내는 예상 co-occurance니까 그 사이의 차이를 제곱한다음 각각 가중치를 주어 합하면 cost function인가?" 정도이다. 물론 혼자 생각한거라 정확한지는.. 모르겠다.

### Relationship to Other Models

$$ Q_{ij} $$ 을 word $$j$$가 context of word $$i$$에 나타날 확률이라고 할 때, 다음 식과 같아진다.

$$Q_{ij} = \frac {\exp (w_i^\intercal \tilde w _j)} {\sum_{k=1}^V \exp(w_i^\intercal \tilde w _k)}$$

softmax인데, 그를 이용한 objective function은 다음과 같다.

$$ J = - \sum_{i \in corpus \\ j \in context(i)} \log Q_{ij}$$

이를 co-occurance matrix $$X$$를 미리 계산해서 이렇게 변형하면 훨씬 빨라진다. ($$X$$의 75% ~ 90%가 0이니..)

$$ J = - \sum_{i = 1}^V \sum_{j=1}^V X_{ij} \log Q_{ij} $$

여기서 앞의 식들을 이용해 이렇게 변형이 가능하다.

$$ J = - \sum_{i = 1}^V X_i \sum_{j=1}^V P_{ij} \log Q_{ij} = \sum_{i=1}^V  X_{i} H(P_i, Q_i)$$

여기서 $$H(P_i, Q_i)$$는 Cross entropy이다. 근데, cross entropy는 distance를 측정하는 방법중 하나인데, 어떤 때에 weight를 너무 많이 준다고 한다. 그래서 이렇게 다른 거리를 쓰도록 바꿔주자.

$$ \hat J = \sum_{i, j}^V  X_{i} (\hat P_{ij} - \hat Q_{ij}) ^2$$

여기서 $$\hat P_{ij} = X_{ij}$$, $$\hat Q_{ij} = \exp(w_i^\intercal \tilde {w}_j)$$처럼 unnormalize된 분포가 된다. 근데 이것도 문제가 있다. $$X_{ij}$$는 보통 너무 큰 값을 취하게 되므로, squared error를 minimize해주자.

$$ \hat J = \sum_{i, j}^V  X_{i} (\log \hat P_{ij} - \log \hat Q_{ij}) ^2 \\
= \sum_{i, j}^V  X_{i} (w_i^\intercal \tilde {w}_j - \log X_{ij}) ^2\\
= \sum_{i, j}^V  f(X_{ij}) (w_i^\intercal \tilde {w}_j - \log X_{ij}) ^2 $$

자 근데 이건 결국 GloVe의 cost function과 같은 형태가 되었다.

---

그 뒤는 너무 어려워보여서 아직 못봤다 ㅠㅠ

[^glove]: [http://nlp.stanford.edu/projects/glove/](http://nlp.stanford.edu/projects/glove/) 여기에 소스코드가 올라가 있다.
[^homomorphism]: [https://en.wikipedia.org/wiki/Group_homomorphism](https://en.wikipedia.org/wiki/Group_homomorphism) 여기에 간략하게 잘 설명되어 있다.
