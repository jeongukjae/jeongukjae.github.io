---
layout: post
title: ğŸ“ƒ GPT ë¦¬ë·°
tags:
  - paper
---

Transformer ê´€ë ¨ì„ ì°¾ì•„ë³´ë©´ì„œ Huggingfaceì˜ transformers ë ˆí¬ì§€í† ë¦¬ì— BERT ë‹¤ìŒìœ¼ë¡œ ë‚˜ì˜¤ëŠ” [OpenAIì˜ GPT](https://openai.com/blog/language-unsupervised/)ë¥¼ ì½ì–´ë³´ê¸°ë¡œ í—€ë‹¤. ëŒ€ì‹  ì •ë¦¬ê°€ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ê²°ê³¼ ê°™ì€ ê±´ ë‹¤ ì œì™¸í•œë‹¤.

## Abstract

ëŒ€ê·œëª¨ì˜ unlabeled ë§ë­‰ì¹˜ê°€ ë§ì§€ë§Œ, íŠ¹ì • íƒœìŠ¤í¬ í•™ìŠµì„ ìœ„í•œ labeled dataëŠ” ì—„ì²­ ì ë‹¤. ê·¸ë˜ì„œ ì´ëŸ° ë°ì´í„°ë¡œ íŠ¹ì • íƒœìŠ¤í¬ë“¤ì„ ì˜ ë™ì‘í•˜ê²Œ ë§Œë“œëŠ” ê²ƒì´ ì–´ë µë‹¤. ê·¸ë˜ì„œ OpenAIì—ì„œ ë§Œë“  ê²ƒì´ Generative Pre-Training(GPT)ì´ë‹¤. pretrainì„ unlabeled ë§ë­‰ì¹˜ì—ì„œ ì§„í–‰í•˜ê³  ê·¸ ë’¤ì— íŠ¹ì • íƒœìŠ¤í¬ë“¤ë¡œ fine tuning ì‹œí‚¨ë‹¤.

## 1. Introduction

NLP íƒœìŠ¤í¬ë“¤ì˜ ì„±ëŠ¥ì„ ìœ„í•´ word embeddingì„ ì“°ëŠ” ê²ƒì²˜ëŸ¼ unsupervised í™˜ê²½ì—ì„œ ì¢‹ì€ representationì„ í•™ìŠµí•œë‹¤ë©´ êµ‰ì¥íˆ ë§ì€ íƒœìŠ¤í¬ë“¤ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, word-levelë³´ë‹¤ ë” ë†’ì€ ìˆ˜ì¤€ì˜ ì •ë³´ë¥¼ unlabeled textë¡œë¶€í„° ì–»ì–´ë‚´ëŠ” ê²ƒì€ êµ‰ì¥íˆ ì–´ë ¤ìš´ ì¼ì¸ë°, ê·¸ ì´ìœ ëŠ”, ì²«ë²ˆì§¸ë¡œ optimization objectiveê°€ ë¶ˆëª…í™•í•˜ê³  ë‘ë²ˆì§¸ë¡œ í•™ìŠµí•œ representationì„ target taskë¡œ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ë˜í•œ ë¶ˆëª…í™•í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ëŸ° ë¶ˆëª…í™•í•¨ì´ semi-supervised learningì„ ì–´ë µê²Œ ë§Œë“ ë‹¤.

ê·¸ë˜ì„œ ì´ ë…¼ë¬¸ì—ì„œëŠ” NLU íƒœìŠ¤í¬ë¥¼ ìœ„í•œ semi-supervised approachë¥¼ ì†Œê°œí•œë‹¤. ëª©í‘œëŠ” ì¡°ê¸ˆë§Œ ë°”ê¾¸ì–´ë„ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ì— ì ìš© ê°€ëŠ¥í•œ universal representationì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ì´ ë…¼ë¬¸ì—ì„œ ì†Œê°œí•˜ëŠ” ëª¨ë¸ì€ unsupervised pretrainingê³¼ supervised fine-tuning ë‘ ë‹¨ê³„ë¥¼ ê±°ì¹œë‹¤. ì²«ë²ˆì§¸ë¡œ unsupervised pre-training ë‹¨ê³„ëŠ” unlabeled dataë¡œë¶€í„° LM Objectiveë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•œë‹¤. ê·¸ ë‹¤ìŒìœ¼ë¡œ fine-tuning ë‹¨ê³„ì—ì„œ pre-training ë‹¨ê³„ì˜ weightë“¤ì„ ì ì ˆí•˜ê²Œ í•´ë‹¹ íƒœìŠ¤í¬ì— ë§ë„ë¡ ë°”ê¾¼ë‹¤.

ì—¬ê¸°ì„œëŠ” ì—­ì‹œ Transformerë¥¼ ì‚¬ìš©í–ˆê³ , ê·¸ ì´ìœ ëŠ” structureed memoryë¥¼ ì œê³µí•˜ê¸° ë•Œë¬¸ì— long-term dependenciesë¥¼ ì¡°ê¸ˆ ë” ì˜ ë‹¤ë£° ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ëª¨ë¸ì˜ ê²€ì¦ì€ ì´ëŸ° íƒœìŠ¤í¬ë“¤ì„ í†µí•´ì„œ í–ˆë‹¤ê³  í•œë‹¤ - natural language inference, question answering, semantic similarity, text classification.

## 2. Related Work

**Semi-supervised learning for NLP**: ì˜ˆì „ì—ëŠ” word-levelë§Œ semi-supervisedë¡œ ì¡ì•„ëƒˆë‹¤ë©´(word-embeddingì²˜ëŸ¼) ìš”ì¦˜ì—ëŠ” unlabeled dataë¡œ ê·¸ ì´ìƒìœ¼ë¡œ ì¡ì•„ë‚´ë ¤ê³  í•œë‹¤.

**Unsupervised pre-training**: ì—¬ëŸ¬ ì—°êµ¬ë“¤ì´ pre-trainingì´ regularization schemeì²˜ëŸ¼ ë™ì‘í•˜ì§€ë§Œ, deep neural networkì—ì„œëŠ” ë” ì¢‹ì€ generalization ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ê³  í•œë‹¤.

## 3. Framework

ì²«ë²ˆì§¸ ë‹¨ê³„ëŠ” large corpusì—ì„œ LMì„ í•™ìŠµí•˜ê³ , ê·¸ ë‹¤ìŒìœ¼ë¡œ fine tuningì„ ì§„í–‰í•œë‹¤.

### 3.1. Unsupervised pre-training

ì´ ë…¼ë¬¸ì—ì„œ ì†Œê°œí•˜ëŠ” ëª¨ë¸ì€ Language Modelingì„ ìœ„í•´ multi-layer Transformer Decoderë¥¼ ì‚¬ìš©í•œë‹¤. input context tokenë“¤ì— multi-head self-attentionì— ë„£ê³  ë‚˜ì„œ position wise feedforward layerì— ë„£ê³  target tokenì˜ distributionì„ ë‚¸ë‹¤.

$$h_0 = UW_e + W_p$$

$$h_l = \text{transformer_block} (h_{h-1}) \forall i \in [1, n]$$

$$P(u) = \text{softmax}(h_nW_e^T)$$

* $$U$$ëŠ” input context token
* $$W_e$$ëŠ” token embedding matrix
* $$W_p$$ëŠ” positional embedding
* $$n$$ëŠ” layer ê°œìˆ˜

LMì€ standard LMì„ ì‚¬ìš©í–ˆë‹¤. ì—¬ê¸°ì„œ $$\mathcal U$$ëŠ” unsupervised corpus of tokens.

$$ L_1(\mathcal U) = \sum_i \log P(u_i\rvert u_{i-k}, ..., u_{i-1}; \theta )$$

### 3.2. Supervised fine-tuning

í•™ìŠµí•œ LM ëª¨ë¸ì˜ final transformer block ê²°ê³¼ê°’ì— linear output layerë¥¼ ë¶™ì—¬ì„œ íŠ¹ì • ê°’ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤.

$$ P(y \rvert x^1, ...,, x^m) = \text{softmax}(h^m_l W_y)$$

OpenAIíŒ€ì´ ì—°êµ¬í•œ ê²ƒì— ë”°ë¥´ë©´, ì´ë ‡ê²Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ generalizationì„ ì¢‹ê²Œ í•˜ëŠ” íš¨ê³¼ì™€ convergenceë¥¼ ë¹ ë¥´ê²Œ í•˜ëŠ” íš¨ê³¼ê°€ ìˆë‹¤ê³  í•œë‹¤.

### 3.3. Task-specific input transformations

{% include image.html url="/images/2019/10-20-gpt/fig1.png" description="Transformer ì•„í‚¤í…ì³ì™€ fine-tuning ì‹œì˜ input transformation" %}

text classificationê°™ì€ íŠ¹ì • íƒœìŠ¤í¬ë“¤ì˜ ê²½ìš°ì—ëŠ” ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ë°”ë¡œ fine-tuningí•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ QAë‚˜ textual entailment ê°™ì€ ê²½ìš°ëŠ” ordered sentence pair, triplet of document, question, and answer ê°™ì€ structured inputì´ ë“¤ì–´ì˜¤ê²Œ ë˜ëŠ”ë° ì´ê²ƒì€ contiguousí•œ textì—ì„œ í•™ìŠµí•œ pretraining ëª¨ë¸ê³¼ ë¶ˆì¼ì¹˜ê°€ ìƒê¸°ê²Œ ëœë‹¤. ê·¸ë˜ì„œ ì´ëŸ´ ë•Œ pre-trained ëª¨ë¸ì„ ì˜ ì“¸ ìˆ˜ ìˆë„ë¡ ordered sequenceë¡œ ë°”ê¾¸ì–´ ì£¼ì—ˆë‹¤ê³  í•œë‹¤. ì´ëŸ° ê±¸ traversal style approachë¼ ì ì–´ë†“ì•˜ëŠ”ë°, ë­”ì§€ëŠ” ì‚´í´ë´ì•¼ê² ë‹¤.

## ìœ„ì—ì„œ ë‚˜ì˜¨ ë‚´ìš©ì—ì„œ ê°ì£¼ë¡œ ë‹¬ë ¤ìˆë˜ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸

* R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160â€“167. ACM, 2008.
* T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111â€“3119, 2013.
* J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532â€“1543, 2014.
* P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser, and N. Shazeer. Generating wikipedia by summarizing long sequences. ICLR, 2018.
* M. Rei. Semi-supervised multitask learning for sequence labeling. ACL, 2017.
* M. E. Peters, W. Ammar, C. Bhagavatula, and R. Power. Semi-supervised sequence tagging with bidirec- tional language models. ACL, 2017.
* T. RocktÃ¤schel, E. Grefenstette, K. M. Hermann, T. KocË‡isky`, and P. Blunsom. Reasoning about entailment with neural attention. arXiv preprint arXiv:1509.06664, 2015.

ì´ ì¤‘ì— ëª‡ê°œë‚˜ ë³¼ê¹Œ..?
