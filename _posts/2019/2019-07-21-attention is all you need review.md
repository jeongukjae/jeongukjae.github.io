---
title: "ğŸ“ƒ Review of \"Attention Is All You Need\""
layout: post
tags:
  - nlp
  - paper
  - machine learning
---

Transformerë¥¼ ì†Œê°œí•˜ëŠ” ë…¼ë¬¸ìœ¼ë¡œ, CS224nê°•ì˜ì˜ suggested readings ëª©ë¡ì— ìˆì–´ì„œ ì½ì–´ë³¸ ë…¼ë¬¸ì´ë‹¤. í•œêµ­ì–´ ë¦¬ë·°ë„ ì—„ì²­ ë§ì„ ì •ë„ë¡œ ìœ ëª…í•œ ë…¼ë¬¸ì´ë‹¤. í•´ë‹¹ ë…¼ë¬¸ì„ ì½ê³ , ê°„ëµí•œ ì •ë¦¬ë¥¼ í•´ë³´ì•˜ë‹¤. ë…¼ë¬¸ì€ [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)ì— ìˆë‹¤.

## Abtract

TransformerëŠ” ê¸°ì¡´ê³¼ ë‹¤ë¥´ê²Œ ì™„ì „íˆ attentionë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ êµ¬ì¡°ì´ë‹¤. 2014 WMT English-to-German translation taskì—ì„œ sotaë¥¼ ì°ì€ ëª¨ë¸ì´ë¼ê³  í•œë‹¤.

## 1. Introduction & 2. Background

Recurrent Modelì€ ìˆœì„œê°€ ì¤‘ìš”í•˜ë‹¤ëŠ” íŠ¹ì„±ìƒ ë³‘ë ¬í™”í•˜ê¸°ê°€ ì–´ë µë‹¤. í•˜ì§€ë§Œ ì´ transformerë¼ëŠ” Attentionì— ê¸°ë°˜í•œ ëª¨ë¸ì€ inputê³¼ outputì˜ global dependencyë¥¼ ë°”ë¡œ ë½‘ì•„ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë³‘ë ¬í™”í•˜ê¸° ì¢‹ë‹¤. ë”°ë¼ì„œ sotaì¸ ëª¨ë¸ì„ P100 8ëŒ€ë¡œ 12ì‹œê°„ë§Œì— ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆì—ˆë‹¤. sequence-aligned RNNì—†ì´ ì™„ì „íˆ self-attention (intra attention)ë§Œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì´ë‹¤.

## 3. Model Architecture

### 3.1 Encoder and Decoder Stack

{% include image.html url="/images/2019-07-21-transformer/1.png" description="Transformer architecture" %}

ìš°ì„  Encoder-decoder structureë¥¼ ê°€ì§€ê³  ìˆë‹¤. í•˜ì§€ë§Œ stacked self-attentionì„ ì‚¬ìš©í•˜ê³ , point-wise feed forward networkë¥¼ ì‚¬ìš©í•œë‹¤.

#### Encoder

Encoderì˜ Layer í•˜ë‚˜ëŠ” ë‘ ê°œì˜ sublayerë¡œ ë˜ì–´ ìˆìœ¼ë©°, ì²«ë²ˆì§¸ëŠ” multi-head self-attention mechanismì„ ê°€ì§€ê³  ìˆë‹¤. ë‘ë²ˆì§¸ëŠ” position-wise fully-connected feed-forward networkë¥¼ ì‚¬ìš©í•œë‹¤. residual connectionì„ ì‚¬ìš©í•œ ê²ƒì„ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë‹¤. ë…¼ë¬¸ì—ì„œ ì„¤ëª…í•˜ê¸¸ í•˜ë‚˜ì˜ sublayerë¥¼ $$\text{LayerNorm}(x + \text{SubLayer}(x))$$ë¡œ ë³´ë¼ê³  í•œë‹¤. ì´ëŸ° layer í•˜ë‚˜ë¥¼ 6ê°œë¥¼ ìŒ“ì•˜ë‹¤.

#### Decoder

Encoderì˜ ë‘ê°œì˜ sublayerì˜ ê²°ê³¼ê°’ì— multi-head attentionì„ ìˆ˜í–‰í•˜ëŠ” ë ˆì´ì–´ë¥¼ ì—°ê²°í•œë‹¤. ì—­ì‹œ ë˜ redsidual connectionì„ sub layerë§ˆë‹¤ ì—°ê²°í•´ì¤€ë‹¤. ê·¸ë¦¬ê³  ì²« ë ˆì´ì–´ì— ì¼ë°˜ì ì¸ multi-head attentionì„ ë§Œë“¤ì–´ì£¼ëŠ” ê²ƒì´ ì•„ë‹Œ masked multi-head attentionì„ ë§Œë“¤ì–´ì¤€ë‹¤. ì´ masked layerë¡œ ì¸í•´ position $$i$$ì— ëŒ€í•´ì„œ predictionì„ ìˆ˜í–‰í•  ë•Œ ì˜¤ë¡œì§€ $$i$$ë³´ë‹¤ ì‘ì€ ìœ„ì¹˜ì˜ ê²°ê³¼ì— ì˜ì¡´í•œë‹¤.

### 3.2 Attention

attention functionì€ queryì™€ set of key value pairsë¥¼ outputìœ¼ë¡œ mappingí•˜ëŠ” functionë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤. ë¬¼ë¡  ì—¬ê¸°ì„œ key, value, query, outputì€ ì „ë¶€ vectorì´ë‹¤.

{% include image.html url="/images/2019-07-21-transformer/2.png" description="Scaled Dot Product Attention && Multi Head Attention" %}

#### 3.2.1 Scaled Dot-Product Attention

ì´ ë…¼ë¬¸ì—ì„œ ì“°ëŠ” Attention êµ¬ì¡° ì¤‘ í•˜ë‚˜ê°€ Scale Dot-Product Attentionì´ë‹¤. inputì€ $$d_k$$ì°¨ì›ì˜ query, keyì´ê³ , $$d_v$$ ì°¨ì›ì˜ valueì´ë‹¤. ìœ„ ê·¸ë¦¼ì„ ì‹ìœ¼ë¡œ ë°”ê¿”ì£¼ë©´ ì•„ë˜ì²˜ëŸ¼ ëœë‹¤.

$$ \text{Attention} (Q, K, V) = \text{softmax}(\frac{QK^T} {\sqrt {d_k}})V$$

ì¼ë°˜ì ìœ¼ë¡œ ë§ì´ ì“°ì´ëŠ” attentioní•¨ìˆ˜ë“¤ì€ additive attention[^aa]ê³¼ dot-product(multiplicative) attentionì¸ë°, dot-product attentionì´ ìœ„ì˜ ì‹ê³¼ $$\sqrt{d_k}$$ë¡œ scalingí•˜ëŠ” ê²ƒë§Œ ë¹¼ë©´ ë˜‘ê°™ë‹¤ê³  í•œë‹¤. Additive Attentionì€ compatibility functionì„ 1ê°œì˜ hidden networkë¥¼ ê°€ì§„ feed-forward networkë¥¼ ê³„ì‚°í•˜ëŠ”ë°, ì´ë¡ ì ìœ¼ë¡œ dot-product attentionê³¼ ë³µì¡ë„ëŠ” ë¹„ìŠ·í•˜ì§€ë§Œ, dot-product attentionì´ ë¹ ë¥´ê³  space-efficientí•˜ë‹¤ê³  í•œë‹¤. ê·¸ ì´ìœ ëŠ” highly optimized matrix multiplication codeë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ.. ê·¸ëƒ¥ ìµœì í™”í•˜ê¸° ìš©ì´í•˜ë‹¨ë‹¤.

[^aa]: [Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.](https://arxiv.org/pdf/1409.0473.pdf) ë¥¼ ì°¸ê³ í•˜ë¼ê³  í•˜ëŠ”ë°, ë‚˜ì¤‘ì— ì‹œê°„ë˜ë©´..

$$d_k$$ê°€ ê°’ì´ ì‘ë‹¤ë©´ additiveì™€ multiplicativeëŠ” ë¹„ìŠ·í•˜ê²Œ ë™ì‘í•˜ê³ , ì˜¤íˆë ¤ additiveê°€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤. í•˜ì§€ë§Œ $$d_k$$ê°€ ì»¤ë„ softmax functionì´ small gradientsë¥¼ ê°€ì§„ ë¶€ë¶„ìœ¼ë¡œ ìˆ˜ë ´ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, $$\frac 1 {\sqrt {d_k}}$$ë¡œ scaleí–ˆë‹¤ê³  í•œë‹¤.

#### 3.2.2 Multi-Head Attention

ê°ê° key, value, queryë¥¼ single attentionì— ë„£ëŠ” ê²ƒë³´ë‹¤ key, value, queryë¥¼ ì „ë¶€ projectí•´ì„œ parallelí•˜ê²Œ attention functionì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì¢‹ì€ ê²ƒì„ ë°œê²¬í–ˆë‹¤ê³  í•œë‹¤.

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O \\
  \text{where } \text{ head}_i = \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$

êµ¬í˜„í•  ë•ŒëŠ” $$h=8$$ë¡œ ì‚¬ìš©í–ˆê³ , $$d_k = d_v = d_{model} / h = 64$$ë¥¼ ì‚¬ìš©í–ˆë‹¤ê³  í•œë‹¤. ê°ê°ì˜ headê°€ dimensionì´ ì¤„ì—ˆìœ¼ë¯€ë¡œ, total computational cost ë˜í•œ single head attentionì„ full dimensionalityí•˜ê²Œ ê³„ì‚°í•œ ê²ƒê³¼ ë¹„ìŠ·í•˜ë‹¤ê³ .

#### 3.2.3 Applications of Attention in our Model

TransformerëŠ” multi-head attentionì„ ì„¸ê°€ì§€ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.

* encoder-decoder attentionì—ì„œ queryëŠ” ì´ì „ decoder layerì—ì„œ, memory key, valueëŠ” encoderì˜ outputì—ì„œ ì˜¨ë‹¤ê³  í•œë‹¤. ì´ ë°©ì‹ì€ ëª¨ë“  positionì—ì„œ decoderê°€ input sequenceì˜ ëª¨ë“  positionì„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•œë‹¤. ì´ê±´ seq2seq ëª¨ë¸ì˜ encoder-decoder attention mechanismê³¼ ë‹®ì•˜ë‹¤ê³ .
* encoderëŠ” self-attention layerë¥¼ í¬í•¨í•œë‹¤. encoderì˜ inputì¸ key, value, queryëŠ” ì „ë¶€ ì´ì „ previous encoder layerì˜ outputì—ì„œë¶€í„° ì˜¨ë‹¤.
* ë§ˆì°¬ê°€ì§€ë¡œ decoderì˜ self attention layerëŠ” ê°ê°ì˜ decoderì—ì„œ í•´ë‹¹ decoderì˜ positionê¹Œì§€ì˜ ì •ë³´ë¥¼ ì „ë¶€ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. í•˜ì§€ë§Œ auto-regressiveí•œ íŠ¹ì„±ì„ ìœ„í•´ leftward informationì„ ì˜ ë‹¤ë£° í•„ìš”ì„±ì´ ìˆì—ˆê³ , ê·¸ë˜ì„œ illegal connectionì˜ ê°’ë“¤ì„ ì „ë¶€ $$-\infty$$ë¡œ maskingí–ˆë‹¤. -> (ì´ ë¶€ë¶„ì€ ì˜ ì´í•´ê°€ ê°€ì§€ ì•ŠëŠ”ë° ë‚˜ì¤‘ì— ë‹¤ì‹œ ìƒê°í•´ë³´ì)

### 3.3 Position-wise Feed-forward networks

ê·¸ëƒ¥ ë‘ê°œì˜ linear transformationì— ReLUë§Œ ì˜ ìš”ë ‡ê²Œ

$$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$$

### 3.4 Embeddings and Softmax

linear transformationê³¼ softmaxë¥¼ í™œìš©í•´ì„œ decoder outputì„ next token probabilityë¥¼ ê²Œì‚°í•œë‹¤. ë‘ê°œì˜ embedding layerì™€ pre-softmax linear transformationì— ë˜‘ê°™ì€ weight matrixë¥¼ ì‚¬ìš©í–ˆë‹¤ê³  í•œë‹¤.[^output]

### 3.5 Positional Encoding

sequenceì˜ ìˆœì„œë¥¼ í™œìš©í•˜ê²Œ í•˜ê¸° ìœ„í•´ PEë¥¼ ì‚¬ìš©í–ˆë‹¤ê³  í•œë‹¤. positional encodingì„ encoderì™€ decoder ì§ì „ì— ì‚¬ìš©í–ˆëŠ”ë°, ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.

$$ PE(pos, 2i) = \sin(pos / 10000^{2i / d_{model}}) \\
PE(pos, 2i + 1) = \cos(pos / 10000^{2i / d_{model}})$$

$$pos$$ëŠ” positionì´ê³ , $$i$$ëŠ” dimensionì´ë‹¤. sin, così„ í†µí•´ relative position ì •ë³´ë¥¼ í•™ìŠµí•˜ê¸¸ ê¸°ëŒ€í•œ ê²ƒì€ $$k$$ë¼ëŠ” fixed offsetì´ ìˆë‹¤ê³  í•  ë•Œ $$PE_{pos + k}$$ëŠ” $$PE_{pos}$$ì˜ linear functionìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.

[^output]: [Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.](https://arxiv.org/pdf/1608.05859.pdf)ê³¼ ë¹„ìŠ·í•œ ë°©ì‹

## 4. Why Self-Attention

{% include image.html url="/images/2019-07-21-transformer/3.png" description="Self Attention, Recurrent, Convolutional, Self-Attention(restricted)ë¥¼ ë¹„êµ" %}

ì—¬ê¸°ì„œëŠ” self-attention layerì™€ recurrent, convolution layerì™€ ë¹„êµë¥¼ í•œë‹¤.

ì´ ì„¸ê°€ì§€ ê¸°ì¤€ì´ ìˆëŠ”ë°, í•˜ë‚˜ëŠ” total computational complexity per layerì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” parallelizedë  ìˆ˜ ìˆëŠ” computationì˜ ì–‘ì´ë‹¤. ì„¸ë²ˆì§¸ëŠ” network ìƒì—ì„œ long range dependencyì˜ path lengthì´ë‹¤. path lengthê°€ ì§§ì•„ì§ˆìˆ˜ë¡ long range dependencyë¥¼ í•™ìŠµí•˜ê¸° í›¨ì”¬ ì‰¬ì›Œì§„ë‹¤. (ë°”ë¡œ ë¹„êµí•  ìˆ˜ ìˆìœ¼ë‹ˆê¹Œ)

ë³´í†µì˜ ìƒí™©ì¸(sota ëª¨ë¸, byte-pair representationì´ë‚˜ word pieceê°™ì€) sequence length $$n$$ì´ representation dimensionality $$d$$ë³´ë‹¤ ì‘ì„ ë•Œ, computational complexityë¥¼ ë¹„êµí•  ë•Œ self attentionì€ recurrent layerë³´ë‹¤ ë¹ ë¥´ë‹¤. í•˜ì§€ë§Œ ì´ê²ƒë³´ë‹¤ ë” computational complexityë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ì„œ neighborhoodë¥¼ size $$r$$ë§Œí¼ë§Œ ê³ ë ¤í•  ìˆ˜ ìˆë„ë¡ ì œí•œí•  ìˆ˜ ìˆë‹¤. ê·¸ë ‡ê²Œ ëœë‹¤ë©´ max path lengthê°€ $$O(n/r)$$ë¡œ ëŠ˜ì–´ë‚˜ê²Œ ë˜ì§€ë§Œ, ë‚˜ì¤‘ì— í•´ë³¸ë‹¤ê³  í•œë‹¤. (ë‚˜ë§ê³  ë…¼ë¬¸ì—ì„œ)

Convolution layerëŠ” ë³´í†µ recurrent layerë³´ë‹¤ computational complexityê°€ ë†’ë‹¤ê³  í•œë‹¤. í•˜ì§€ë§Œ, separable convolution[^sepcon]ì„ ì‚¬ìš©í•˜ë©´ í›¨ì”¬ ê´œì°®ë‹¤ê³ . complexityê°€ $$O(knd+ nd^2)$$ìœ¼ë¡œ ì¤€ë‹¤ê³  í•œë‹¤. ì—¬ê¸°ì„œ $$k$$ëŠ” separable convolutionì˜ factor. $$k = n$$ì¸ ìƒí™©ì—ì„œë„ separable convolutionì˜ complexityê°€ self attention + pointwise feedforward layerì˜ complexityì™€ ê°™ë‹¤ê³ .

[^sepcon]: [Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.](https://arxiv.org/abs/1610.02357)ì€ ë­”ì§€ ëª¨ë¥´ê² ìœ¼ë‹ˆê¹Œ ë‹¤ìŒì— ê°„ë‹¨í•˜ê²Œ ë³´ì

í•˜ì§€ë§Œ, self-attentionì„ ì‚¬ìš©í•˜ë©´ ì¡°ê¸ˆ ë” interpretableí•œ modelì„ ì–»ì„ ìˆ˜ ìˆë‹¤. (attention distributionì„ ë½‘ì•„ë‚´ë©´ ì–´ë””ì— ì¡°ê¸ˆ ë” attendí•œì§€?ë¥¼ ì•Œ ìˆ˜ ìˆëŠ” ê²ƒì´ ê·¸ ì´ìœ ì¸ ë“¯ ì‹¶ë‹¤)

## 5. Training

### 5.1 Training data and batching

WMT 2014 English-German Datasetì„ í†µí•´ í•™ìŠµí–ˆê³ , sentenceëŠ” byte-pairë¥¼ í†µí•´ encodingë˜ì—ˆë‹¤.

### 5.2 Hardware and schedule

8ê°œì˜ P100ì„ ì‚¬ìš©í–ˆê³ , stepí•˜ë‚˜ë§ˆë‹¤ 0.4ì´ˆ ì •ë„ ê±¸ë ¸ë‹¤. 100,000 stepì„ ê³„ì‚°í–ˆë‹¤ê³  í•˜ë‹ˆ, 12ì‹œê°„ì´ ê±¸ë ¸ë‹¤. ë”°ë¡œ í° ëª¨ë¸ì„ ì‘ì„±í•´ë³´ì•˜ì„ ë•ŒëŠ” stepì´ 1.0ì´ˆ ì •ë„ ê±¸ë ¸ê³ , 300,000 stepë™ì•ˆ trainingì„ í–ˆì„ ë•Œ 3.5ì¼ì´ ê±¸ë ¸ë‹¤.

### 5.3. Optimizer

$$\beta_1 = 0.9, \beta_2 = 0.98, \epsilon = 10^{-9}$$ë¡œ Adamì„ ì‚¬ìš©í–ˆë‹¤. learning rateëŠ” ì•„ë˜ì²˜ëŸ¼ ê²Œì‚°í–ˆë‹¤.

$$ lrate = d^{-0.5}_{model} min(step_num^{-0.5}, step_num * warmup_steps^{-1.5})$$

$$warmup_steps = 4000$$ìœ¼ë¡œ ì‚¬ìš©í–ˆë‹¤. $$warmup_steps$$ë™ì•ˆ learning rateê°€ ì¦ê°€í•˜ë‹¤ê°€ step numberì˜ inverse square rootë¡œ ê³„ì† ê°ì†Œí•œë‹¤.

### 5.4 Regularization

* **Residual Dropout**: encoder, decoder stackì—ì„œ dropoutì„ embeddingì˜ í•©ê³¼ positional encodingì— ì ìš©í–ˆë‹¤ê³  í•œë‹¤. $$P_{drop} = 0.1$$ì„ ì‚¬ìš©í–ˆë‹¤.
* **Label Smoothing**: label smoothingì„ $$\epsilon_{ls} = 0.1$$ì„ ì‚¬ìš©í–ˆë‹¤. perplexityê°€ ì•ˆì¢‹ì•„ì§€ê³  unsureí•œ ê²ƒë“¤ì„ í•™ìŠµí•˜ì§€ë§Œ, BLEU scoreëŠ” ì¢‹ì•„ì§„ë‹¤.

## 6. Results

{% include image.html url="/images/2019-07-21-transformer/4.png" %}

ìœ„ì—ì„œ ë³´ì´ë‹¤ì‹¶ì´ MTì—ì„œë„ SOTA ì°ìœ¼ë©´ì„œ ì˜í–ˆê³ ,

{% include image.html url="/images/2019-07-21-transformer/5.png" %}

English Constituency Parsingì—ì„œë„ ì˜í–ˆë‹¤. (WSJ = Wall Street Journal) ê·¸ë˜ì„œ Model Variationì„ ë³´ë©´,

{% include image.html url="/images/2019-07-21-transformer/6.png" %}

(A)ì—ì„œëŠ” attention headsì™€ attention key, value dimensionì„ ë‹¤ë¥´ê²Œ í•˜ë©´ì„œ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³´ì•˜ê³ , (B)ì—ì„œëŠ” $$d_k$$ë§Œ ì¤„ì˜€ëŠ”ë° ì´ê²Œ model qualityë¥¼ ì•ˆì¢‹ê²Œë§Œ í–ˆë‹¤ê³  í•œë‹¤. (C)ë‘ (D)ì—ì„œëŠ” dropoutì€ overfittingë°©ì§€ì— ë§¤ìš° ì¢‹ê³ , í° ëª¨ë¸ì´ ê·¸ëƒ¥ ì˜í•˜ë”ë¼ëŠ”... ê²°ê³¼ì´ë‹¤. (E)ì—ì„œëŠ” siní•¨ìˆ˜ ëŒ€ì‹  learned positional embeddingì„ ì‚¬ìš©í–ˆëŠ”ë°, ê·¸ëƒ¥ ê±°ì˜ ë¹„ìŠ·í•˜ë‹¤ê³  í•œë‹¤.

## 7. Conclusion

ì´ ë…¼ë¬¸ì—ì„œ Transformerë¥¼ ë°œí‘œí–ˆê³ , fully attention baseì¸, recurrent layerì—†ëŠ” ê²ƒì„ ë§Œë“¤ì—ˆë‹¤. ê·¸ë˜ì„œ recurrent, convolution layerë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ê²Œ í•™ìŠµì´ ê°€ëŠ¥í•˜ë©´ì„œë„ WMT 2014 English-to-German, WMT 2014 English-to-French translation taskê°™ì€ ê³³ì—ì„œ sotaê¹Œì§€ ë‹¬ì„±í–ˆë‹¤.

ë‹¤ë¥¸ ê²ƒì— attention-based modelì„ ì ìš©í•´ë³´ë ¤ í•˜ëŠ”ë°, ì¼ë‹¨ restricted attentionì„ image, audio, videoê°™ì€ large input, outputì„ ë‹¤ë£¨ê²Œ í•´ë³´ë ¤ê³  í•œë‹¤ê³  í•œë‹¤.

transformerì½”ë“œëŠ” [tensor2tensor](https://github.com/tensorflow/tensor2tensor)ë¥¼ ì°¸ê³ í•˜ì.
