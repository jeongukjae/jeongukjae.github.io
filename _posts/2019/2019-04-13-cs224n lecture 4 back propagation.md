---
layout: post
title: ğŸ“• CS224n Lecture 4 Backpropagation
tags:
  - cs224n
---

CS224n ë„¤ë²ˆì§¸ ê°•ì˜ë¥¼ ë“£ê³  ì •ë¦¬í•œ í¬ìŠ¤íŠ¸!! ì´ë²ˆ ê°•ì˜ëŠ” ë‹¤ë¥¸ ê°•ì˜ë¥¼ ë“¤ìœ¼ë©´ì„œ ë§ì´ ë³´ì•˜ë˜ ë‚´ìš©ì´ê³  ë§ì´ ë‹¤ë¥¼ ê²ƒì´ ì—†ë‹¤ ìƒê°í•˜ê³  ë³„ ê¸°ëŒ€ì—†ì´ ë“¤ì—ˆë‹¤.

## Matrix gradients for our simple neural net and some tips

í¸ë¯¸ë¶„ í•˜ëŠ” ì‹ì€ ê±´ë„ˆë›´ë‹¤! ë„ˆë¬´ ì—¬ê¸°ì €ê¸° ë§ì´ ë‚˜ì˜¤ê¸°ë„ í–ˆê³  ê°œì¸ì ìœ¼ë¡œë„ ì •ë¦¬í•  í•„ìš”ì„±ì„ ëª» ëŠë‚€ë‹¤.

ë‹¤ë§Œ, ì´ëŸ°ì €ëŸ° íŒì´ ë‚˜ì™”ëŠ”ë° ì•„ë˜ì™€ ê°™ë‹¤.

* Tip 1: Carefully define your variables and keep track of their dimensionality!
* Tip 2: Chain rule!
* Tip 3: For the top softmax part of a model: First consider the derivative wrt $$f_c$$ when $$c = y$$ (the correct class), then consider derivative wrt $$f_c$$ when $$c \neq y$$ (all the incorrect classes)
* Tip 4: Work out element-wise partial derivatives if youâ€™re getting confused by matrix calculus!
* Tip 5: Use Shape Convention. Note: The error message $$\delta$$ that arrives at a hidden layer has the same dimensionality as that hidden layer

ì—¬íŠ¼ ì­‰ ê±´ë„ˆë›°ì–´ì„œ word gradientsë¥¼ window modelì—ì„œ ê³„ì‚°í•˜ëŠ” ë¶€ë¶„ê¹Œì§€ ì™”ë‹¤. windowë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì˜ ê²½ìš° $$x$$ì˜ gradientë¥¼ ê³„ì‚°í•œ ê²°ê³¼ê°€ window ì „ì²´ì¸ë°, ì´ëŠ” word vectorë“¤ì„ ë‹¨ìˆœíˆ ì—°ê²°í•œ ê²ƒì´ë¯€ë¡œ ë‹¤ì‹œ ë‚˜ëˆ ì„œ ìƒê°í•´ì¤€ë‹¤.

$$ x_{window} = \pmatrix { x_{museums} && x_{in} && x_{Paris} && x_{is} && x_{amazing} }$$

### Updating word gradients in window model

gradientë¥¼ ê°€ì ¸ì™¸ì„œ word vectorë¥¼ ì—…ë°ì´íŠ¸í•  ë•Œ ì£¼ì˜í•´ì•¼í•˜ëŠ” ì ì´ ìˆë‹¤. ì˜ ìƒê°í•´ë³´ë©´ ì›ë˜ì˜ ML ì ‘ê·¼ë²•ì€ nì°¨ì›ì— ë°ì´í„°ë“¤ì´ ê³µê°„ì— ì¡´ì¬í•  ë•Œ decision boundaryë¥¼ ì •í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, word vectorë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì€ word vector ìì²´ê°€ ì›€ì§ì¸ë‹¤. íŠ¹ì • batchì— ëŒ€í•´ í•™ìŠµí•œë‹¤ê³  í•  ë•Œ, batchì— ì¡´ì¬í•˜ì§€ ì•Šì€ ë‹¨ì–´ë“¤ì€ ì›€ì§ì´ì§€ ì•Šì§€ë§Œ, batchì— ë“¤ì–´ìˆëŠ” ë‹¨ì–´ë“¤ì€ ì›€ì§ì´ê²Œ ëœë‹¤.

ê·¸ì— ëŒ€í•œ ë¹„êµì  ì¢‹ì€ í•´ê²°ì±…ì€ pre-trained word vectorë“¤ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ëŒ€ë¶€ë¶„, ê±°ì˜ ëª¨ë“  ê²½ìš°ì— ì¢‹ì€ ë‹µì´ ë  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ë§Œì•½ ì¢‹ì€ ë°©ëŒ€í•œ ë°ì´í„°ì…‹ì„ ê°€ì§€ê³  ìˆëŠ” ê²½ìš° pre trained ëª¨ë¸ì— ëŒ€í•´ì„œ fine tuningì„ í•´ì¤˜ë„ ì¢‹ë‹¤ê³  í•œë‹¤. (ë‹¤ë§Œ, ì‘ì€ ë°ì´í„°ì…‹ì¸ ê²½ìš° í•™ìŠµí•˜ëŠ” ê²ƒì´ ì˜¤íˆë ¤ í•´ê°€ ë ìˆ˜ë„ ìˆë‹¤ê³ )

## Computation graphs and backpropagation

ì´ì œ graphë¡œ ì„¤ëª…í•˜ëŠ” backprop ë¶€ë¶„ì¸ë°, ê±´ë„ˆë›´ë‹¤.

## Stuff you should know

ë‹¤ì–‘í•œ, ì¢€ ì•Œì•„ë‘ë©´ ì¢‹ì„ ê²ƒë“¤ì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ëŠ”ë° ì•„ë˜ì™€ ê°™ì€ ë¦¬ìŠ¤íŠ¸ë¥¼ ì•Œë ¤ì¤€ë‹¤.

* Regularization: overfittingì„ ë°©ì§€í•˜ëŠ” ê¸°ë²•
* Vecotrization: pythonicí•œ ë°©ë²•ì€ MLì—ì„œëŠ” ì¢€ ë§ì´.. ëŠë¦´ ìˆ˜ ìˆë‹¤.
* non-linearity: activation functionì— ëŒ€í•´ ì„¤ëª…ì„ í–ˆëŠ”ë°, sigmoid, tanhëŠ” ì´ì œ íŠ¹ë³„í•œ ìƒí™©ì—ì„œë§Œ ì‚¬ìš©í•œë‹¤ê³  í•œë‹¤. ReLUë¥¼ ê·¸ëƒ¥ ì²˜ìŒ ì‹œë„í•´ë³´ëŠ” ê²ƒì´ ì¢‹ì„ ê±°ë¼ê³ ..
* parameter initialization: weightë¥¼ ì²˜ìŒ ì–´ë–»ê²Œ ì´ˆê¸°í™”í• ì§€ê°€ ë¬¸ì œì¸ë°, 0ì€ ì“°ì§€ë§ê³ (backprop í•´ì•¼í•˜ë‹ˆê¹Œ) Xavierê°™ì€ ê²ƒì„ ì¨ì£¼ë©´ ì˜ ëœë‹¤ê³  í•œë‹¤.
* optimization: SGD, adargrad, RMSProp, Adam, SparseAdamê°™ì€ ê²ƒë“¤ì´ ë§ì´ ë‚˜ì™”ëŠ”ë°, SGDê°€ ë³´í†µì˜ ìƒí™©ì— ì˜ ë™ì‘í•œëŒ€ìš”.
* Learning Rate: ì ì ˆí•œ lrë¥¼ ì •í•´ì£¼ëŠ” ê²ƒì´ ì¢‹ì€ë°, cyclic learning ratesê°™ì€ ì‹ ê¸°í•œ ë°©ë²•ë„ ìˆìœ¼ë‹ˆ ì˜ ì •í•©ì‹œë‹¤.
