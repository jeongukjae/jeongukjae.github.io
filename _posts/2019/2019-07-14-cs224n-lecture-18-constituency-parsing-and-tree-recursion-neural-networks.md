---
layout: post
title: ğŸ“• CS224n Lecture 18 Constituency Parsing and Tree Recursive Neural Networks
tags:
  - cs224n
---

18ê°•ì´ê³  ê°•ì˜ ì „ì²´ê°€ ë‹¤ ëë‚˜ê¸°ê¹Œì§€ ì´ ê°•ì˜ë¥¼ ì œì™¸í•˜ê³  2ê°•ì •ë„ë§Œ ë‚¨ì•˜ë‹¤.

#### Last Minute Project Tips

ì´ê±´ íŒŒì´ë„ í”„ë¡œì íŠ¸ íŒì¸ë°, ëŠë¦¬ê³  ì•„ë¬´ê²ƒë„ ë™ì‘ì•ˆí•˜ë©´ ì²˜ìŒìœ¼ë¡œ ê·¸ëƒ¥ ë§˜í¸íˆ ëŒì•„ê°€ì„œ ë‹¤ì‹œ í•´ë³´ëŠ” ê²ƒì´ ì¢‹ë‹¤ê³ . íŒ¨ë‹‰ì¸ ìƒí™©ì¼í…Œë‹ˆê¹Œ. ë””ë²„ê¹…ì„ ìœ„í•´ ë§¤ìš° ì‘ì€ ë„¤íŠ¸ì›Œí¬ë‘ ë°ì´í„°ë¥¼ ë„£ì–´ë³´ê¸°ë„ í•˜ê³ , ì˜ ë™ì‘í•˜ë©´ ëª¨ë¸ ì‚¬ì´ì¦ˆë¥¼ í‚¤ì›Œë³´ê¸°ë„ í•˜ë¼ê³  í•œë‹¤.

## Motivation: Compositionality and Recursion

* semantic ì •ë³´ë¥¼ ìš°ë¦¬ê°€ ì˜ ê°€ì ¸ê°ˆ ìˆ˜ ìˆì„ê¹Œ?
* word embeddingë§Œìœ¼ë¡œ ì¶©ë¶„í• ê¹Œ?
* ë§Œì•½ ì˜ ê°€ì ¸ê°ˆ ìˆ˜ ìˆë‹¤ê³  í•´ë„, ì •ë§ í° phraseì— ëŒ€í•´ì„œëŠ”?

ìœ„ì˜ ë¬¸ì œ ë•Œë¬¸ì— compositionalityë¥¼ ìƒê°í•˜ê²Œ ë˜ì—ˆê³ , ë‹¨ì–´, êµ¬ë“¤ì˜ semantic compositionì„ êµ¬ì„±í•˜ìëŠ” ì•„ì´ë””ì–´ê°€ ë‚˜ì™”ë‹¤.

{% include image.html url="/images/cs224n/18-1.png" description="ì•½ê°„ ìš”ëŸ° composition??" %}

ê·¸ëŸ¼ tree í˜•íƒœë¡œ êµ¬ì„±í•˜ê¸° ìœ„í•´ì„œ recursiveí•œì§€ ë¬¼ì–´ë³¸ë‹¤ë©´ ë…¼ìŸì´ ìˆì„ ìˆ˜ ìˆê² ì§€ë§Œ, language ìì²´ë¥¼ í‘œí˜„í•˜ëŠ”ë° ì—„ì²­ ìì—°ìŠ¤ëŸ½ë‹¤ê³ .

{% include image.html url="/images/cs224n/18-2.png" description="ì•”íŠ¼ ìì—°ìŠ¤ëŸ¬ì›€" %}

## Structure prediction with simple Tree RNN: Parsing

vector spaceì— word vectorë¥¼ ë¿Œë ¤ë†“ëŠ”ë°, ê·¸ëŸ¼ êµ¬ë“¤ì€ ì–´ë–»ê²Œ í•´ì•¼í•˜ë‚˜?? -> ê·¸ëƒ¥ ë°”ë¡œ ê°™ì€ vector spaceì— ë„£ì–´ë²„ë¦¬ì. ê·¸ëŸ¼ ì–´ë–»ê²Œ phraseì˜ embeddingì„ ê²°ì •í• ê¹Œ??

1. ë‹¨ì–´ì˜ ì˜ë¯¸ë“¤ê³¼
2. ê·¸ë“¤ì„ combineí•˜ëŠ” ê·œì¹™ì„ ë§Œë“¤ì–´ì„œ!

{% include image.html url="/images/cs224n/18-3.png" description="ì´ë ‡ê²Œ ì´ë ‡ê²Œ ì˜ ë½€ê¹Œë½€ê¹Œ" %}

ìœ„ì˜ ê²½ìš°ëŠ” recursiveí•œ ê²½ìš°ì´ê³ , ì¼ë°˜ì ì¸ rnnì„ í†µí•´ì„œë„ ê²°ê³¼ëŠ” ë‹¤ë¥´ê² ì§€ë§Œ êµ¬í•´ë³¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ recursiveí•œ êµ¬ì¡°ëŠ” tree structureë¥¼ êµ¬ì„±í•´ì£¼ì–´ì•¼ í•œë‹¤ëŠ” ì–´ë ¤ì›€ì´ ë”°ë¥´ê³ , rnnì„ í†µí•œ êµ¬ì¡°ëŠ” phraseë¥¼ prefix contextì—†ì´ ì¡ì•„ë‚´ì§€ë„ ëª»í•˜ê³ , ë§ˆì§€ë§‰ ê²°ê³¼ vectorê°€ ë§ˆì§€ë§‰ ë‹¨ì–´ì— ì¢Œìš°ëœë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.

ê·¸ëŸ¼ NNì— ì–´ë–»ê²Œ structure predictionì„ í•  ìˆ˜ ìˆì„ê¹Œ? ì´ë ‡ê²Œ childrenì„ ë„£ê³  semantic representationì„ ë½‘ì•„ë‚´ê³ , ì–¼ë§ˆë‚˜ plausibleí•œì§€(ì–¼ë§ˆë‚˜ Networkê°€ ì´ ë…¸ë“œë¥¼ í™•ì‹ í•˜ëŠ”ì§€ ì •ë„..?)ë„ ë½‘ì•„ë‚¸ë‹¤. ê·¸ëŸ° NNì„ ëª¨ë“  children ëŒ€ìƒìœ¼ë¡œ ì­‰ ëŒë¦°ë‹¤ìŒì— ê·¸ ìœ„ì˜ ì¸µì—ì„œë„ ë˜ ëŒë¦¬ê³ , ë˜ ëŒë¦¬ê³  í•œë‹¤ê³  í•˜ëŠ”ë°, ë‚´ê°€ ë‹¤ì‹œ ì´ ê¸€ì„ ë´ë„ ì´í•´ ëª»í•  ë“¯ í•˜ë‹ˆ ì•„ë˜ ì‚¬ì§„ì„ ë³´ì.

{% include image.html url="/images/cs224n/18-4.png" description="ì´ë ‡ê²Œ ì´ë ‡ê²Œ ì˜" %}

{% include image.html url="/images/cs224n/18-5.png" description="ì´ê±¸ ì˜ childrenì„ ë„£ì–´ì„œ" %}

{% include image.html url="/images/cs224n/18-6.png" description="ì˜ ìš”ë ‡ê²Œ ìš”ë ‡ê²Œ" %}

{% include image.html url="/images/cs224n/18-7.png" description="ë¿…!" %}

## Backpropagation through Structure

general backpropê³¼ í¬ê²Œ ë‹¤ë¥¼ ê²ƒì€ ì—†ë‹¤. ì›ì¹™ì ìœ¼ë¡œëŠ” ê°™ë‹¤. [Goller & KuÌˆchler (1996)](https://www.semanticscholar.org/paper/Learning-task-dependent-distributed-representations-Goller-Kuchler/794e6ed81d21f1bf32a0fd3be05c44c1fa362688)ì— ì˜í•´ ì†Œê°œë˜ì—ˆë‹¤.

forward propì¼ ë•Œ, children $$c_1$$, $$c_2$$ê°€ ìˆë‹¤ê³  í•˜ì. ê±°ê¸°ì„œ parent $$p$$ë¥¼ ë½‘ì•„ë‚´ê¸° ìœ„í•´ $$p = \tanh (W \pmatrix {c_1 \\ c_2} + b)$$ì™€ ê°™ì€ ì—°ì‚°ì„ í•œë‹¤. ê·¸ëŸ¼ backward propì€ ë°˜ëŒ€ë¡œ ê°ê°ì˜ ë…¸ë“œë³„ë¡œ ì—°ì‚°ì„ í•´ì¤€ë‹¤. ì´ëŸ° Simple TreeRNNì— ëŒ€í•´ì„œ ì•„ë˜ì™€ ê°™ì€ ì ì„ ìƒê°í•´ë³¼ ìˆ˜ ìˆë‹¤.

* ìƒê°ë³´ë‹¤ ê²°ê³¼ëŠ” ë‚˜ì˜ì§€ ì•Šê³ 
* ì¢€ phraseë¥¼ ì¡ì•„ë‚¼ ìˆ˜ëŠ” ìˆëŠ”ë°, more higher order composition, more complexí•œ long sentenceë¥¼ íŒŒì‹±í•˜ëŠ”ë° ì–´ë ¤ì›€ì´ ìˆë‹¤.
* input wordë“¤ ì‚¬ì´ì˜ interactionì€ í•˜ë‚˜ë„ ì—†ë‹¤.
* composition functionì´ ë‹¤ ë˜‘ê°™ë‹¤

## More complex TreeRNN units

ë‘ë²ˆì§¸ë¡œ Syntactically-United RNNì´ ìˆëŠ”ë°, [Socher, Bauer, Manning, Ng 2013](https://www.aclweb.org/anthology/P13-1045)ì„ ì‚´í´ë³´ì.

ê¸°ë³¸ì ì¸ syntactic structureë¥¼ ìœ„í•´ì„œëŠ” symbolic Context-Free Grammar (CFG) backboneì´ ì¢‹ì•˜ë‹¤ê³  í•œë‹¤. ê·¸ë¦¬ê³  discrete syntactic categoryë¥¼ ì‚¬ìš©í–ˆë‹¤ê³  í•˜ê³ , ë‹¤ë¥¸ syntactic environmentì—ì„œëŠ” ë‹¤ë¥¸ composition matrixë¥¼ ì‚¬ìš©í•˜ê²Œ í•´ì£¼ëŠ” ê²ƒì´ í›¨ì”¬ ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚´ì—ˆë‹¤ê³  í•œë‹¤. ê·¸ë¦¬ê³  ë” ì¢‹ì€ semanticì˜ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆì—ˆë‹¤ê³ .

ì•„ì§ ìœ„ì˜ ë…¼ë¬¸ì„ ì½ì–´ë³´ì§€ ì•Šì•„ì„œ ìì„¸í•œ ë‚´ìš©ì„ ëª¨ë¥´ì§€ë§Œ, ê°•ì˜ì˜ ë‚´ìš©ëŒ€ë¡œ ì¨ë³´ìë©´, Compositional Vector Grammarë¥¼ ì´ìš©í•˜ëŠ” ê²ƒì˜ ë‹¨ì ì´ ì¼ë‹¨ ë¬¸ì œì ì´ ì†ë„ê°€ ëŠë¦¬ë‹¤ê³  í•œë‹¤. Beam Search ê³¼ì •ì—ì„œ matrix-vector productë¥¼ í•˜ë‹ˆ ì—°ì‚°ëŸ‰ì´ ë§ì•„ì§ˆ ìˆ˜ ë°–ì— ì—†ë‹¤ê³ . ê·¸ë˜ì„œ subset of treeë¥¼ ë¨¼ì € ê³„ì‚°í•´ë³´ê³  very unlikely candidatesë¥¼ ë¯¸ë¦¬ ì—†ì• ëŠ” ë°©ë²•(PCFG)ì„ ì‚¬ìš©í–ˆë‹¤ê³  í•œë‹¤. ê·¸ë˜ì„œ CVG = PCFG + TreeRNN

{% include image.html url="/images/cs224n/18-8.png" description="ë‚˜ì¤‘ì„ ìœ„í•´ ì €ì¥! (CVG = Compositional Vector Grammerì¸ë“¯..?)" %}

---

ì„¸ë²ˆì§¸ë¡œ ["Semantic Compositionality through Recursive Matrix-Vector Spaces"](https://ai.stanford.edu/~ang/papers/emnlp12-SemanticCompositionalityRecursiveMatrixVectorSpaces.pdf)ë¼ëŠ” ì œëª©ì˜ ë…¼ë¬¸ì„ í†µí•´ ì†Œê°œëœ ëª¨ë¸ë„ ì†Œê°œí•œë‹¤. ì›ë˜ëŠ” weight matrix $$W$$ë¥¼ êµ‰ì¥íˆ ì ê·¹ì ìœ¼ë¡œ ì´ìš©í–ˆëŠ”ë°, wordëŠ” ëŒ€ë¶€ë¶„ operatorì²˜ëŸ¼ í–‰ë™í•˜ë¯€ë¡œ, (`very good`ì˜ `very` ì²˜ëŸ¼) composition functionìœ¼ë¡œ ì²˜ë¦¬í•˜ìëŠ” ê²ƒì´ë‹¤.

{% include image.html url="/images/cs224n/18-10.png" description="Compositionality Through Recursive Matrix-Vector Recursive Neural Networks (matrix meaning)" %}

matrix meaningê³¼ vector meaningìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì´í•´í•˜ìëŠ” ê²ƒì¸ë°, $$A$$ê°€ matrix meaning, $$a$$ê°€ vector meaningì´ë‹¤. ì´ê±¸ ê²°í•©ë  ë‹¨ì–´ë‘ ì—°ì‚°ì„ í•œë²ˆ í•œ ë‹¤ìŒì— composition functionìœ¼ë¡œ ë„˜ê¸´ë‹¤. ì™¼ìª½ì€ vector meaning of phraseë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì´ê³  ì˜¤ë¥¸ìª½ì€ matrix meaning of phraseë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì´ë‹¤.

{% include image.html url="/images/cs224n/18-11.png" description="MV RNNì˜ semantic relationship ë¶„ë¥˜ ê²°ê³¼" %}

ì „í†µì ì¸ ë°©ë²•ì— ë¹„í•´ ì—„ì²­ë‚œ í–¥ìƒì„ ì´ë£¨ì—ˆê³ , ì¼ë°˜ì ì¸ RNNì— ë¹„í•´ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. featureë¥¼ ë” ë„£ê²Œ ëœë‹¤ë©´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆì„ ê²ƒì´ë¼ê³  ë§í–ˆë‹¤.

í•˜ì§€ë§Œ ì—¬ê¸°ì„œ ë©ˆì¶”ì§€ ì•Šê³  ë¬¸ì œì ì„ ì§šì–´ë³´ìë©´,

1. matrixì™€ vectorë¥¼ ê°™ì´ ì´ìš©í•˜ê²Œ ë˜ë©´ì„œ ì—„ì²­ë‚œ ìˆ˜ì˜ íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í–ˆê³ 
2. ê½¤ë‚˜ í° phraseì— ëŒ€í•´ì„œ matrix meainingì„ ì¡ì•„ë‚¼ ì¢‹ì€ ë°©ë²•ì´ ì—†ì—ˆë‹¤.

---

ê·¸ë˜ì„œ ë„¤ë²ˆì§¸ë¡œ ê·¸ ë‹¤ìŒí•´ì— ["Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)ë¼ëŠ” ë…¼ë¬¸ìœ¼ë¡œ MV RNNë³´ë‹¤ parameterìˆ˜ë¥¼ ì¤„ì¸ ëª¨ë¸ì„ ê³µê°œí–ˆë‹¤ê³  í•œë‹¤. Recursion Neural Tensor Networkë¼ RNTNì´ë¼ê³  ë¶€ë¥´ëŠ” ê²ƒ ê°™ë‹¤.

ê·¸ ì „ì— sentiment detectionì— ëŒ€í•´ í•œë²ˆ ì§šê³  ë„˜ì–´ê°€ìë©´, ì¼ë°˜ì ìœ¼ë¡œ sentiment analysisëŠ” ë‹¨ì–´ë“¤ì„ ì˜ ë½‘ì•„ë‚´ëŠ” ê²ƒìœ¼ë¡œ ì˜ ë™ì‘í•œë‹¤ê³  í•œë‹¤. ê¸´ documentì— ëŒ€í•´ì„œë„ detection accuracyëŠ” 90% ìˆ˜ì¤€ìœ¼ë¡œ ë†’ë‹¤. í•˜ì§€ë§Œ, `the movie should have been funnier and more entertaining`ê°™ì€ ë¬¸ì¥ì€ negative meaningì´ì§€ë§Œ, `funnier`, `entertaining`ê³¼ ê°™ì€ ë‹¨ì–´ë“¤ë§Œ ë³¸ë‹¤ë©´ positiveí•˜ê²Œ ë¶„ë¥˜í•  ê°€ëŠ¥ì„±ì´ ìˆë‹¤.

ìœ„ì™€ ê°™ì€ ë¬¸ì œëŠ” MV-RNNì—ì„œë„ ì¶©ë¶„íˆ ì˜ ë™ì‘í–ˆê³ , treebankì²˜ëŸ¼ ì¢‹ì€ ë°ì´í„°ì…‹ì„ ë„£ì–´ì¤„ ê²½ìš° ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ì§€ë§Œ, hard negationê³¼ ê°™ì€ ë¬¸ì¥ë“¤ì€ ì—¬ì „íˆ ë¶€ì •í™•í–ˆê³ , ë” ì¢‹ì€ ëª¨ë¸ì´ í•„ìš”í–ˆë‹¤.

attentionì„ ë½‘ì„ ë•Œ 1 x n, n x n, n x 1ì˜ vector ë‘ê°œ (ì£¼ë¡œ ë‹¨ì–´ë“¤ì˜ vector)ì™€ matrix í•˜ë‚˜ë¥¼ ì¤‘ê°„ì— ë¼ì›Œì„œ ì“°ëŠ”ë°, ì´ê±¸ ë‹¤ë¥´ê²Œ ìƒê°í•´ì„œ matrixë§ê³  3-dim tensorë¥¼ ë¼ì›Œì„œ ì“°ìëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ë§ì€ ì •ë³´ë¥¼ ì˜ interactí•´ì„œ ì¡ì•„ë‚´ë„ë¡ í•˜ìëŠ” ê²ƒì´ë‹¤.

{% include image.html url="/images/cs224n/18-12.png" description="ì´ë ‡ê²Œ?? ê·¼ë° ì œëŒ€ë¡œ ì´í•´ ëª» í–ˆë‹¤." %}

sentence labels ë°ì´í„°ì…‹ì—ì„œëŠ” ì˜ ì„±ëŠ¥ì´ ì•ˆë‚˜ì™”ì§€ë§Œ, treebankì—ì„œ ë‚˜ë¨¸ì§€ë“¤ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ìœ¼ë¡œ ì˜ ë‚˜ì™”ê³ , negationì´ íŠ¹íˆ ì˜ ë˜ì—ˆë‹¤.

## Other uses of tree-recursive neural nets

ë‹¤ë¥¸ ê²ƒì— ëŒ€í•œ ì†Œê°œë¡œ [QCD-Aware Recursive Neural Networks for Jet Physics](https://arxiv.org/abs/1702.00748)ì— ëŒ€í•œ ì†Œê°œë„ ìˆëŠ”ë°, ì´ê±´ ë¬¼ë¦¬ìª½ ë¶„ì•¼ë¡œ ì ìš©í•œ ê²ƒì´ë¼ ê°„ë‹¨í•˜ê²Œ ìŠ¤í‚µ

ê·¸ ë‹¤ìŒ êµ‰ì¥íˆ ì¬ë°Œì–´ë³´ì´ëŠ” ê²ƒì€ [Tree-to-tree Neural Networks for Program Translation](https://arxiv.org/abs/1802.03691)ìœ¼ë¡œ CoffeeScriptë¥¼ JavaScriptë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë¸ì„ ì‘ì„±í•œ ê²ƒì´ë‹¤.
