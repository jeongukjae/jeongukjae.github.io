---
layout: post
title: "ğŸ‘¨â€ğŸ’» CS224n assignments 1 & 2"
tags:
  - nlp
  - cs224n
  - machine learning
  - python
---

cs224n ìŠ¤í„°ë””ë¥¼ í•˜ë©´ì„œ ë‚˜ì˜¤ëŠ” ê³¼ì œë“¤ë„ ê°™ì´ í•˜ê¸°ë¡œ í—€ë‹¤. ê·¸ë˜ì„œ 1, 2ì£¼ì°¨ ê³¼ì œë¥¼ ëª°ì•„ì„œ í•´ë´¤ë‹¤. ê³¼ì œë¥¼ í•˜ë©´ì„œ ë‚´ê°€ ë‹¤ì‹œ ë´ì•¼í•  ë‚´ìš©ê°™ì€ ê²ƒì„ ì ì–´ë†“ì•˜ë‹¤.

## [1ì£¼ì°¨ ê³¼ì œ](https://github.com/jeongukjae/cs224n-assignments/blob/master/assignment%201/exploring_word_vectors.ipynb)

Question 1ì—ì„œ Word Vectorë¥¼ ê°„ë‹¨í•˜ê²Œ ì¨ë³´ê³ , Question 2ì—ì„œ gensimìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ analogyë“±ì„ í•´ë³´ëŠ” ê³¼ì œì˜€ë‹¤. ì „ì²´ì˜ ì†ŒìŠ¤ì½”ë“œëŠ” jupyter notebookìœ¼ë¡œ ì œê³µë˜ì—ˆê³ , ë¹„ì–´ìˆëŠ” ì¼ë¶€ ì†ŒìŠ¤ì½”ë“œë¥¼ ì±„ìš°ê±°ë‚˜, ê²°ê³¼ë¥¼ ë³´ê³  ì„¤ëª…ì„ ì¨ë‚´ëŠ” ê³¼ì œì˜€ë‹¤.

### Question 1

ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ìë©´, co occuranceë¥¼ ì° ë’¤ truncated svd(PCAë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤)ì™€ matplotlibì„ ì´ìš©í•˜ì—¬ ëª‡ëª‡ ë‹¨ì–´ë“¤ì„ í•˜ëŠ” ê²ƒì´ë‹¤.

{% include image.html url="/images/cs224n/a1-1.png" description="cooccurance matrix" %}
{% include image.html url="/images/cs224n/a1-2.png" description="Truncated SVD (Singular Value Decomposition)" %}

ìœ„ì˜ ë‚´ìš©ê³¼ ì•„ë˜ì˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ë©´ ì¢‹ë‹¤ê³  í•œë‹¤.

* [Computation Broadcasting](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html)

ê²°êµ­ ê·¸ ì •ë³´ë¥¼ ì´ìš©í•´ì„œ plotting í•˜ë©´ ì•„ë˜ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤. ë‚˜ë¼ ì´ë¦„ë“¤ì€ ìƒë‹¹íˆ ë§ì´ ëª¨ì—¬ìˆëŠ” ëª¨ìŠµì´ë‹¤.

{% include image.html url="/images/cs224n/a1-3.png" description="Result of question1 in a1" %}

### Question 2

prediction-based word vectorsì— ê´€í•œ ë‚´ìš©ì´ë‹¤. ì§ì ‘ êµ¬í˜„í•´ë³´ëŠ” ë‚´ìš©ì€ ì•„ë‹ˆê³  ì‚¬ìš©í•´ë³´ëŠ” ë‚´ìš©ì´ê¸° ë•Œë¬¸ì— í¬ê²Œ ë³¼ ë‚´ìš©ì€ ì—†ê³ , ì•„ë˜ ë…¼ë¬¸ë§Œ ì‚´í´ë³´ë©´ ë  ê²ƒ ê°™ë‹¤.

* [origin paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) (ë¬´ì—‡ì¸ê°€ í–ˆëŠ”ë° negative sampling ë…¼ë¬¸)

## [2ì£¼ì°¨ ê³¼ì œ](https://github.com/jeongukjae/cs224n-assignments/tree/master/assignment%202/a2)

ì´ê²ƒë„ Questionì´ ë‘ê°€ì§€ê°€ ìˆëŠ”ë°, ì²«ë²ˆì§¸ëŠ” word2vecì— í•„ìš”í•œ ìˆ˜ì‹ì„ êµ¬í•´ë³´ëŠ” ë‹¨ê³„ì´ê³ , ë‘ë²ˆì§¸ëŠ” ê·¸ ìˆ˜ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ” ë‹¨ê³„ì´ë‹¤.

### a2 Question 1

Question 1ì€ ì´ [pdf íŒŒì¼](https://github.com/jeongukjae/cs224n-assignments/blob/master/assignment%202/a2.pdf)ì„ ë³´ì. ì•Œì•„ë‘ì–´ì•¼ í•  ì‹ì€ ì•„ë˜ ì •ë„ì´ë‹¤. ì´ê±´ written ê³¼ì œë¼ ì—¬ê¸°ì— ì •ë¦¬í•´ë†“ëŠ”ë‹¤.

$$P(O=o|C=c)= \frac {\exp (u_o^\intercal v_c)} {\sum_{w\in Vocab} \exp (u_w^\intercal v_c)}$$

$$J_{naive-softmax}(v_c, o, U) = - \log P(O=o|C=c)$$

#### a

aëŠ” naive-softmax lossê°€ cross entropy lossì™€ ê°™ì•„ì§€ëŠ” ì´ìœ ë¥¼ ì ì–´ë¼ê³  í•œë‹¤. ì¦‰, ì•„ë˜ ì‹ì´ ì°¸ì¸ ì´ìœ ë¥¼ ë§í•˜ë¼ê³  í•œë‹¤.

$$ - \sum_{w \in Vocab} y_w \log \hat y_w = - \log \hat y_o $$

$$y$$ëŠ” ì‹¤ì œ í™•ë¥  ë¶„í¬ì´ê³ , $$\hat y$$ëŠ” ëª¨ë¸ì—ì„œ êµ¬í•œ í™•ë¥  ë¶„í¬ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, $$y$$ëŠ” context word $$o$$ì— í•´ë‹¹í•˜ëŠ” elementë§Œ 1ì¸ one-hot vectorì´ê³ , ìœ„ì˜ ì‹ì´ ì°¸ì´ ëœë‹¤.

#### b, c

bëŠ” naive softmax lossì‹ì„ $$v_c$$ì— ëŒ€í•´ í¸ë¯¸ë¶„ í•  ë•Œ!

$$\frac {\partial J} {\partial v_c} = - u_o + \sum_{x \in Vocab} P(x|c) u_x$$

ê·¼ë° ìœ„ì˜ ì‹ì´ ì‹¤ì œ ë¶„í¬ì™€ ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ê°’ì„ ê³„ì‚°í•˜ëŠ” ê²ƒì¸ë°, $$y$$ê°€ outside word $$o$$ì— ëŒ€í•´ì„œë§Œ 1ì´ë‹ˆ ê²°êµ­ ê·¸ëƒ¥ ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì‹¤ì œ ë¶„í¬ì™€ ê³„ì‚°í•œ í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ì™€ ê°™ë‹¤.

$$ \frac {\partial J} {\partial v_c} = U (\hat y - y) $$

cëŠ” naive softmax lossì‹ì„ $$u$$ì— ëŒ€í•´ í¸ë¯¸ë¶„ í•  ë•Œ! ê³„ì‚°í•˜ë©´ $$ w = o $$ì¸ ê²½ìš°ëŠ” ì•„ë˜ì²˜ëŸ¼ ë‚˜ì˜¨ë‹¤.

$$\frac {\partial J } {\partial u_o} = (\hat y_o - y_o) v_c $$

$$(\hat y_o - y_o)$$ ëŠ” í™•ë¥  ë¶„í¬ì˜ elementë¼ë¦¬ ë”í•˜ê³  ëº€ê±°ë‹ˆê¹Œ ìŠ¤ì¹¼ë¼ê°’!

$$ w \neq o$$ì¸ê²½ìš°ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

$$ \frac {\partial J } {\partial u_w} = \hat y_w v_c$$

ê·¼ë°, ì´ê²Œ $$y$$ê°€ $$o$$ë²ˆì§¸ elementë§Œ 1ì¸ one-hot vectorì´ë‹ˆ ê²°êµ­ ì „ì²´ $$U$$ì— ëŒ€í•´ í¸ë¯¸ë¶„ í•˜ë©´ ì•„ë˜ì™€ ê°™ì•„ì§„ë‹¤.

$$ \frac {\partial J} {\partial U} = (\hat y - y) v_c^\intercal$$

#### d

sigmoid í¸ë¯¸ë¶„. ì´ê±´ ë‹¤ë¥¸ ê³³ì—ë„ ì„¤ëª…ì´ ì›Œë‚™ ë§ìœ¼ë‹ˆ...

$$\frac {\partial \sigma} {\partial x} = \sigma (1 - \sigma)$$

#### e

ì´ê±´ negative sampleì— ëŒ€í•œ lossì˜ í¸ë¯¸ë¶„ ì‹ì„ êµ¬í•˜ëŠ” ê²ƒì¸ë°, ì¼ë‹¨ neg sampleì˜ lossëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

$$ J_{neg-sample}(v_c, o, U) = - \log (\sigma (u_o^\intercal v_c)) - \sum_{k=1}^K \log (\sigma (-u_k^\intercal v_c))$$

$$K$$ê°€ negative samplesì´ê³ , $$o$$ëŠ” neg sampleì— ì•ˆë“¤ì–´ìˆë‹¤.

ì´ ë•Œ ê°ê°ì˜ ë¯¸ë¶„í•œ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

$$\frac {\partial J} {\partial v_c} = - (1 - \sigma (u_o^\intercal v_c))u_o + \sum_{k = 1}^K (1 - \sigma(-u_k^\intercal v_c))u_k$$

ì´ ê²½ìš° ì‹¤ì œ ì½”ë“œë¡œ êµ¬í˜„í•  ë•ŒëŠ” $$U$$ì—ì„œ $$o$$ë²ˆì§¸ë¥¼ ì œì™¸í•˜ê³  ì „ë¶€ -1ì„ ê³±í•´ì¤€ í›„ í•´ë‹¹ matrix ì „ì²´ì— ëŒ€í•´ sigmoidë¥¼ ì—°ì‚°í•´ì„œ ì‚¬ìš©í–ˆë‹¤. ë˜ $$o$$ë²ˆì§¸ë§Œ $$ - (1 - \sigma)$$ì´ê³  ë‚˜ë¨¸ì§€ëŠ” $$1- \sigma$$ì¸ì ë„ ë¯¸ë¦¬ ì „ì²´ì— ëŒ€í•´ ì—°ì‚°í•´ì„œ ì‚¬ìš©í–ˆë‹¤.

$$\frac {\partial J} {\partial u_o} = - (1 - \sigma (u_o^\intercal v_c))v_c$$

$$\frac {\partial J} {\partial u_k} = \sum_{x=1}^K (1 - \sigma(-u_x^\intercal v_c))\frac {\partial u_x^\intercal v_c} {\partial u_k} $$

ì´ê²Œ ìœ„ì˜ ì‹ì²˜ëŸ¼ ì‘ì„±í•œ ì´ìœ ëŠ” neg sampleì— ì—¬ëŸ¬ë²ˆ ë“¤ì–´ê°ˆ ê²½ìš° ê·¸ ìˆ˜ë§Œí¼ ë”í•´ì£¼ì–´ì•¼ í•œë‹¤.

---

ì´ê²Œ ê·¼ë° ë‹¤ ë§ëŠ”ì§€ëŠ” ëª¨ë¥´ê² ê³  ì¼ë‹¨ í’€ì–´ë³¸ê±°ë‹¤. ì•„ë˜ ì½”ë“œë¡œ êµ¬í˜„í–ˆì„ ë•Œ ì˜ ë‚˜ì™”ìœ¼ë‹ˆ ë§ëŠ” ê±°ê² ì§€..?

### a2 Question 2

êµ¬í˜„!!!ì€ ê·¸ë ‡ê²Œê¹Œì§€ ì–´ë µì§„ ì•Šê³ , ìˆ˜í•™ìˆ˜ì‹ì„ ê·¸ëŒ€ë¡œ ì˜®ê²¨ì•¼ í•˜ëŠ”ë°, ê±°ê¸°ì„œ í—·ê°ˆë ¸ë‹¤. ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ ê²°ê³¼ë¥¼ ë½‘ì•„ë‚´ê¸°ê¹Œì§€ì˜ ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦°ë‹¤. (numpyë¡œ ì‹¤ì œ í•™ìŠµì„ ì‹œì¼œë³¸ë‹¤)

ê·¸ë ‡ê²Œ ì–»ì€ ê²°ê³¼ëŠ” ì•„ë˜ì •ë„ì´ë‹¤.

{% include image.html url="https://github.com/jeongukjae/cs224n-assignments/raw/master/assignment%202/a2/word_vectors.png" description="word vectors" %}

ê·¸ë ‡ê²Œ ê²°ê³¼ê°€ ì˜ ë‚˜ì˜¨ê²ƒê°™ì§„ ì•Šë‹¤. ê·¸ëƒ¥ì €ëƒ¥ ë½‘ì•„ë³¸ ê²ƒì— ë§Œì¡±í•œë‹¤.
