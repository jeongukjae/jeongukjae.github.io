---
layout: post
title: ğŸ”ª Mecabì„ ì‚´í´ë³´ì
tags:
  - nlp
  - paper
---

[ì´ ë…¼ë¬¸(Applying Conditional Random Fields to Japanese Morphological Analysis)](https://www.aclweb.org/anthology/W04-3230.pdf)ì„ ì°¸ê³ í•´ì„œ ì ì–´ë³¸ë‹¤.

CRFsê°€ word boundary ambiguityê°€ ì¡´ì¬í•  ë•Œ ì–´ë–»ê²Œ í•´ê²°í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ë³´ì—¬ì¤€ë‹¤ê³  í•˜ë‹ˆ, MeCabì€ ì¼ë³¸ì–´ë¥¼ word boudnaryë¥¼ ì°¾ê¸° ìœ„í•´ ì‹œì‘í•œ í”„ë¡œì íŠ¸ì¸ ê²ƒ ê°™ë‹¤. ê·¸ë¦¬ê³  CRFsê°€ corpus basedë‚˜ statisticalí•œ ì¼ë³¸ì–´ morphological analysis(í˜•íƒœì†Œ ë¶„ì„ì´ë¼ê³  ë¶€ë¥´ë©´ ë˜ë ¤ë‚˜?)ì— ìˆëŠ” ë¬¸ì œì ë“¤ì„ í•´ê²°í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. hierarchical tagsetsì„ ìœ„í•œ flexible feature designì´ ê°€ëŠ¥í•´ì§€ê³ , label bias, length biasì˜ ì˜í–¥ì´ ì ì–´ì§„ë‹¤.

## 1. Introduction

ì¼ë‹¨ ì¼ë³¸ì–´ëŠ” ì¤‘êµ­ì–´ì²˜ëŸ¼ non-segmented languageì´ë‹¤. ê·¸ë˜ì„œ word boundaryë¥¼ ì°¾ëŠ” ê²ƒì´ word segmentationì„ ì°¾ì•„ë‚´ëŠ” ê±°ë‚˜ POS Taggingí•˜ëŠ” ê²ƒì— ë§¤ìš° ì¤‘ìš”í•˜ë‹¤. CRFsë¥¼ ì“°ë©´ ì´ ë¬¸ì œë“¤ì„ í’€ì–´ë‚¼ ìˆ˜ ìˆë‹¤. HMMsì€ generativeí•˜ê¸° ë•Œë¬¸ì— hierarchical tagsetsë“¤ë¡œë¶€í„° ë‚˜ì˜¨ featureë“¤ì„ ì‚¬ìš©í•˜ê¸° í˜ë“¤ë‹¤. suffix, prefix ê°™ì€ ì •ë³´ë“¤ì´ ì˜ˆì‹œì´ë‹¤. MEMMsì€ label biasë‚˜ legnth biasë¥¼ í•´ê²°í•˜ê¸° í˜ë“¤ë‹¤.

## 2. Japanese Morphological Analysis

word boundaryë¥¼ ì°¾ëŠ” ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ characterë¥¼ tokenìœ¼ë¡œ ì·¨ê¸‰í•´ì„œ Begin/Insideë¥¼ taggingí•˜ë„ë¡ ë§Œë“œëŠ” ê²ƒì´ë‹¤. (character based BI tagging) í•˜ì§€ë§Œ ì´ ê²ƒì€ word segmentationì— ëŒ€í•´ ë¯¸ë¦¬ ì •ë³´ê°€ ìˆëŠ” lexiconì„ í™œìš©í•˜ê¸° í˜ë“¤ê³ , decoding ìì²´ê°€ ì •ë§ ë§ì´ ëŠë ¤ì§„ë‹¤ëŠ” ë° ë¬¸ì œê°€ ìˆë‹¤. (BI Taggingì€ candidatesë¥¼ ë§ì´ ìƒì„±í•´ë†“ì•„ì•¼ í•œë‹¤)

ë‹¨ì–´, í’ˆì„¸ í˜ì–´ë¥¼ í¬í•¨í•˜ëŠ” lexiconì´ ì—†ëŠ” ê²ƒì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì´ë¥¼ í™œìš©í•˜ìëŠ” ê²ƒì´ í•µì‹¬ì¸ ê²ƒ ê°™ë‹¤. (MeCabì„ ì›ë˜ ë¹Œë“œí•  ë–„ ì‚¬ì „ì„ ë„£ëŠ” ì¼ì´ í¬ê¸°ë„ í•˜ê³ )

ì–´ì°Œë˜ì—ˆë“ , ì¼ë³¸ì–´ morph analysisë¥¼ ì •ë¦¬í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

* let $$x$$ be an input, unsegmented sentence.
* let $$y$$ be a path, sequence of tuples containing word $$w_i$$ and pos $$t_i$$

  $$y = [(w1, t1,), ..., (wn, tn)]$$
* let $$\mathcal Y(x)$$ be a set of candidate paths in a lattice built from the input sentence $$x$$ and a lexicon
* let $$\hat y$$ be a corrent path by input sentence $$x$$

### 2.2. Long-Standing Problems

#### 2.2.1. Hierarchical Tagset

ì¼ë³¸ì–´ morphological analzerì¸ ChaSenì´ë‘ JUMANì€ hierarchical structureë¥¼ ì‚¬ìš©í•œë‹¤. CharSenì€ IPA tagsetì„ ì‚¬ìš©í•˜ëŠ”ë° IPA tagsetì€ ì„¸ê°€ì§€ ë¶€ë¶„ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤. POS, conjugation form(cform), conjugate type(ctype). cformì´ë‘ ctypeì€ ë‹¨ì–´ë‘ conjugateì—ë§Œ ì§€ì •ë˜ê³  POSëŠ” 4ë ˆë²¨ì˜ ì„œë¸Œì¹´í…Œê³ ë¦¬ê°€ ìˆë‹¤.

ëª…ì‚¬ë¥¼ ì˜ˆë¥¼ ë“¤ì–´ì„œ ì„¤ëª…í•˜ìë©´, ëª…ì‚¬ëŠ” ì¼ë°˜ ëª…ì‚¬, ê³ ìœ  ëª…ì‚¬ì™€ ê°™ì´ ë‚˜ëˆ„ì–´ì§€ê³ , ê³ ìœ ëª…ì‚¬ê°€ ë˜ ë‹¤ì‹œ ì‚¬ëŒ, ì¡°ì§, ì¥ì†Œì™€ ê°™ì´ ë‚˜ëˆ„ì–´ì§„ë‹¤. ì´ëŸ° POS taggingì— ëŒ€í•´ í’€ì–´ë‚´ì•¼í•˜ëŠ” ë¬¸ì œëŠ” ì–´ë–»ê²Œ ì„œë¡œ ë‹¤ë¥¸ ë ˆë²¨ì˜ ì •ë³´ë“¤ì„ í™œìš©í•˜ëƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ì„œ íŠ¹ì • suffixëŠ” ì´ë¦„ ë’¤ì— ì˜¤ê¸° ë•Œë¬¸ì— Name POSë€ ì •ë³´ë¡œ ë¶„ë¥˜í•´ë‚´ê¸° í¸í•˜ë‹¤.

#### 2.2.2. Label Bias and Length BIas

next state classifierë¥¼ í™œìš©í•˜ëŠ” discriminative modelì€ ëŒ€ë¶€ë¶„ length biasë‚˜ label biasê°€ ìˆì„ ìˆ˜ ìˆë‹¤. label biasëŠ” ì•„ë˜ ê·¸ë¦¼ì—ì„œ `BOS` - `A` ê¹Œì§€ transition scoreê°€ 0.6ì´ê³ , `A`ì—ì„œëŠ” `C`, `D`ë¡œ ë‘ê°œì˜ transitionì´ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— 0.6ì˜ í™•ë¥ ì´ ë” ì¤„ì–´ë“¤ ìˆ˜ ë°–ì— ì—†ë‹¤. ê²°êµ­ $$P(A, D\rvert x) = 0.36$$ì´ ëœë‹¤. íˆì§€ë§Œ `B` - `C`ì˜ ê²½ë¡œëŠ” `B`ì—ì„œ transitionì´ í•˜ë‚˜ë°–ì— ì¡´ì¬í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì¢‹ì§€ ì•Šì€ ê²½ë¡œë¼ê³  í•˜ì—¬ë„ $$P(B, E \rvert x) = 0.4$$ë¡œ probì€ ë” ë†’ë‹¤.

{% include image.html url="/images/2019-10-09-mecab/label bias.png" description="label bias" %}

{% include image.html url="/images/2019-10-09-mecab/length bias.png" description="length bias" %}

length biasëŠ” ë§ ê·¸ëŒ€ë¡œ pathì˜ lengthì— ê´€ë ¨ëœ ë¬¸ì œì´ë‹¤. ìœ„ ê·¸ë¦¼ì—ì„œ $$P(A, D \rvert x) = 0.36$$ìœ¼ë¡œ ì¢‹ì€ ê²½ë¡œì—¬ë„ probì´ ë‚®ì€ë°, $$P(B\rvert x) = 0.4$$ë¡œ ì•ˆì¢‹ì€ ê²½ë¡œê°€ probì´ ë” ë†’ë‹¤.

ìœ„ì˜ ë‘ ì˜ˆì‹œëŠ” $$P(y\rvert x) = \prod^{n}_{i = 1}p((w_i, t_i)\rvert (w_{i-1}, t_{i-1}))$$ê³¼ ê°™ì€ ì‹ì„ ì‚¬ìš©í•˜ëŠ” maximum entropy modelì„ ì´ìš©í•  ë•Œì˜ ì˜ˆì‹œë‹¤.

## 3. Conditional Random Fields

CRFsëŠ” Section 2.2.ì— ì–¸ê¸‰ëœ ë¬¸ì œì ì„ í•´ê²°í•  ìˆ˜ ìˆê³  discriminative modelsì´ë©´ì„œ, corrleated featruesë¥¼ ì¡ì„ ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ hierarchical tagsetsìœ¼ë¡œë„ flexible feature designì„ í•  ìˆ˜ ìˆë‹¤. CRFsëŠ” joint prob($$x$$, $$y$$)ì˜ single exponential modelì´ë¯€ë¡œ, label, lenth biasì˜ ë¬¸ì œì ì„ ë§ì´ ì¤„ì¼ ìˆ˜ ìˆë‹¤. MEMMsì€ sequential combination of exponenetial modelsì´ë‹¤.

ì–´ì°Œë˜ì—ˆë“  word boudary ambiguityë¥¼ í’€ê¸° ìœ„í•´ BI taggingë§ê³  latticeë¥¼ í™œìš©í•˜ê¸°ë¡œ í–ˆë‹¤ê³  í•œë‹¤. ì¼ë³¸ì–´ morphological anlysisë¥¼ ìœ„í•œ CRFs ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. (ë³„ë¡œ ì•ˆë‹¬ë¼ì§)

$$P (y|x) = \frac 1 {Z_x} \exp(\sum_{e i = 1}^{n} \sum_k \lambda_k f_k ((w_{i-1}, t_{i-1}), (w_{i}, t_{i})))$$

$$Z_x$$ëŠ” normalization factorì´ë‹¤.

$$Z_x = \sum_{y' \in \mathcal y (x)} \exp(\sum_{e i = 1}^{n'} \sum_k \lambda_k f_k ((w'_{i-1}, t'_{i-1}), (w'_{i}, t'_{i})))$$

$$f_k ((w_{i-1}, t_{i-1}), (w_{i}, t_{i}))$$ëŠ” $$i$$, $$i - 1$$ë²ˆì§¸ì˜ í† í°ì˜ feature fuctionì´ë‹¤. $$\lambda_k$$ëŠ” learned weightì´ê³ , $$f_k$$ì™€ ì—°ê´€ì´ ìˆë‹¤ê³  í•œë‹¤.

ê·¼ë° ì´ê²Œ ì›ë˜ ë§ì´ ì“°ì´ëŠ” CRFsì‹ê³¼ëŠ” ë‹¤ë¥´ë‹¤ê³  í•˜ëŠ”ë°, ê·¸ ì´ìœ ê°€ MeCabì—ì„œ ì‚¬ìš©í•˜ëŠ” CRFsë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” word boundary ambiguityë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê²ƒì´ê³ , ë”°ë¼ì„œ output sequenceì˜ ê¸¸ì´ê°€ ê³ ì •ë˜ì–´ ìˆì§€ ì•Šë‹¤.

global feature vectorë¼ëŠ” ê²ƒì„ ì •ì˜í•˜ëŠ”ë° $$\vec F(\vec y, \vec x) = \{F_1 (\vec y, \vec x), ... ,F_K(\vec y, \vec x)\}$$ì´ê³ , $$F_K(\vec y, \vec x) = \sum^n_{i = 1} f_k ((w_{i-1}, t_{i-1}), (w_{i}, t_{i}))$$ì´ë‹¤. ê·¸ë˜ì„œ ì•„ë˜ì²˜ëŸ¼ ì“¸ ìˆ˜ ìˆë‹¤. (ê·¼ë° ë²¡í„° í‘œê¸°í•´ì•¼í•˜ëŠ” ê±° ê¹Œë¨¹ì€ ê²Œ ë§ì€ ê²ƒ ê°™ì€ë° ê·€ì°®ìœ¼ë‹ˆê¹Œ ì•ˆê³ ì¹ ë˜..)

$$P(\vec y \rvert \vec x) = \frac 1 {Z_x} exp(\vec \Lambda \vec F(\vec y, \vec x))$$

ê·¸ë˜ì„œ most probable path $$\hat y$$, ì¦‰, ì°¾ê³ ì í•˜ëŠ” ê²½ë¡œëŠ” ì•„ë˜ì™€ ê°™ì•„ì§„ë‹¤.

$$\hat y = \underset {y \in \mathcal Y (x)} {\text{argmax}} P(y\rvert x) = \underset {y \in \mathcal Y (x)} {\text{argmax}} \vec \Lambda \vec F(\vec y, \vec x)$$

Viterbi algorithmìœ¼ë¡œ ì°¾ì.

### 3.1 Parameter Estimation

CRFsëŠ” standard MLEë¡œ trainingì´ ê°€ëŠ¥í•˜ë‹¤.

$$\hat \Lambda = \underset {\Lambda \in \mathbb R^K} {\text{argmax}} \mathcal L_{\Lambda}, \text where \mathcal L_{\Lambda} = \sum_j \log P(y_j \rvert x_j) = \sum_j [\Lambda F(y_j, x_j) - log(Z_{x_j})]$$

ì´ê²Œ optimal pointì˜ first derivativeê°€ 0ì´ ëœë‹¤ê³  í•˜ëŠ”ë°, ì´ê±´ Lafferty et al., 2001ì—ì„œ ë§í•˜ëŠ” convergenceë¥¼ ë³´ì¥í•˜ëŠ” ë‚´ìš©ì„ ë§í•˜ëŠ” ê²ƒ ê°™ë‹¤. ìœ„ ì‹ì„ ë¯¸ë¶„í•˜ë©´ ì•„ë˜ì²˜ëŸ¼ ì •ë¦¬ê°€ ê°€ëŠ¥í•˜ë‹¤.

$$\frac {\partial \mathcal L_{\Lambda}} {\partial \lambda_k}  = \sum_j (F_k(y_j, x_j) - E_{P(y \rvert x_j)} F_k(y, y_j)) = O_k - E_k = 0$$

ìœ„ì˜ ì°¨ì´ê°€ 0ì´ ëœë‹¤. Expectation ì€ forward-backward algorithmì˜ variantë¡œ ì‰½ê²Œ ê³„ì‚°ë  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. (ì´ê±´ ì§„ì§œ ì ê¸° ë„ˆë¬´ ê·€ì°®ë‹¤.. ë…¼ë¬¸ ë‹¤ì‹œ ë³´ì..)

overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë‘ê°€ì§€ ë°©ì‹ì˜ regularizaitonì„ ì‚¬ìš©í•˜ëŠ”ë°, Gaussian prior (L2-norm), Laplacian prioor(L1-norm)ì„ ì‚¬ìš©í•˜ë©´ ëœë‹¤. L1-norm, L2-normì„ ì‚¬ìš©í•˜ëŠ” CRFsë¥¼ L1-CRFs, L2-CRFsë¼ê³  ë¶€ë¥¸ë‹¤.

---

ì´ê±°ë„ ì¼ë‹¨ ì—¬ê¸°ê¹Œì§€ í•´ë³´ê³  ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•´ë³´ê³  ë‹¤ì‹œ ë´ì•¼ê² ë‹¤!
