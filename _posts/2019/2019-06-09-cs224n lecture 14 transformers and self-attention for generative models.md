---
layout: post
title: ğŸ“• CS224n Lecture 14 Transformers and Self-Attention For Generative Models
tags:
  - cs224n
---

14ê°•ì€ ê°•ì—°ìë¥¼ ì´ˆëŒ€í•´ì„œ ê°•ì˜ë¥¼ ì§„í–‰í•œë‹¤. Google AIì—ì„œ ë‚˜ì˜¨ ì—°ì‚¬ì ë‘ë¶„ì´ë¼ê³  í•œë‹¤. NLP ê³µë¶€í•˜ë ¤ê³  ë“£ëŠ” ê²ƒì´ê³ , ë‹¤ë¥¸ ê²ƒë“¤ í•˜ê¸°ì—ë„ ì•½ê°„ ë²…ì°¬ë“¯ ì‹¶ì–´ì„œ NLP ë‚´ìš©ì„ ì¢€ ë²—ì–´ë‚˜ëŠ” ì´ë¯¸ì§€ ì²˜ë¦¬, ìŒì„± ì²˜ë¦¬ ê°™ì€ ë¶€ë¶„ì€ ë§ì´ ê±´ë„ˆë›°ì—ˆë‹¤ ã… ã… 

## Previous works

Variable Length Dataì˜ representationì„ í•™ìŠµí•˜ëŠ” ê²ƒì€ NLPì—ì„œ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤. ê·¸ë¥¼ ìœ„í•œ ì„ íƒì§€ë¥¼ ì—¬ëŸ¬ê°€ì§€ ê¼½ì„ ìˆ˜ ìˆëŠ”ë°, ìš°ì„  RNNì€ Variable Length Representationì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ì¢‹ì€ ì„ íƒì§€ì´ê³ , LSTM, GRUê°™ì€ ê²€ì¦ëœ ëª¨ë¸ì´ ë‚˜ì™€ìˆì§€ë§Œ, Sequential Computationì„ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ë³‘ë ¬í™”ê°€ ì–´ë µê³ , long, short range dependencyì— ëŒ€í•œ ëª¨ë¸ë§ì´ ì–´ë µë‹¤. ê·¸ë˜ì„œ ë³‘ë ¬í™”ê°€ ì‰¬ìš´ CNNì„ ì´ìš©í•˜ë©´ long dependencyë¥¼ í•™ìŠµí•˜ê¸°ê°€ ë§¤ìš° ì–´ë ¤ì›Œì§„ë‹¤. layerë¥¼ ì—„ì²­ ìŒ“ì•„ì•¼ì§€ë§Œ í•™ìŠµì´ ê°€ëŠ¥í•´ì§„ë‹¤. NMTì˜ Encoderì™€ Decoderì‚¬ì´ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ Attentionê°™ì€ ê²½ìš°ëŠ” Representationì—ë„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì–´ë–¤ì§€ì— ëŒ€í•œ ì•„ì´ë””ì–´ê°€ ë‚˜ì™”ê³  ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤ê³  í•œë‹¤. ì£¼ë¡œ ì–¸ê¸‰ë˜ëŠ” ëª¨ë¸ì€ self-attention.

{% include image.html url="/images/cs224n/14-1.png" description="Self Attention" %}

self attentionì—ì„œëŠ” short dependencyë˜, long dependencyë˜ constant path lengthë¥¼ ì¤€ë‹¤ê³  í•œë‹¤. ê·¸ë¦¬ê³  gating/multiplicative interactionì´ ê¸°ë°˜ì¸ ëª¨ë¸ì´ë‹¤. (matmul ê°™ì€) "ê·¸ëŸ¼ ì´ ëª¨ë¸ì´ sequential computationì„ ëŒ€ì²´í•  ìˆ˜ ìˆì„ê¹Œ?"ë¼ëŠ” ì§ˆë¬¸ì´ ìì—°ìŠ¤ë ˆ ë‚˜ì˜¤ê²Œ ë˜ê³ , ê·¸ì— ëŒ€í•œ ëŒ€ë‹µì´ Transformerì´ë‹¤.

ì¶”ê°€ì ìœ¼ë¡œ ì‚´í´ë³¼ ìˆ˜ ìˆëŠ” ìë£Œë“¤ì€ ì•„ë˜ì™€ ê°™ë‹¤.

* Classification & regression with self-attention: Parikh et al. (2016), Lin et al. (2016)
* Self-attention with RNNs: Long et al. (2016), Shao, Gows et al. (2017)
* Recurrent attention: Sukhbaatar et al. (2015)

## Transformer

{% include image.html url="/images/cs224n/14-2.png" description="Transformer" %}

residual connection, self-attention layer ê°™ì€ ì´ì „ì˜ ê°•ì˜ì— ì„¤ëª…ì´ ë˜ì—ˆë˜ ë¶€ë¶„ì— ëŒ€í•´ ì „ì²´ì ìœ¼ë¡œ ì„¤ëª…ì„ í•˜ë©´ì„œ ì‹œì‘í•œë‹¤. ì˜¤ë¥¸ìª½ ì•„ë˜ ë ˆì´ì–´ì˜ attention ì¼ë¶€ë¶„ì´ ë³´ì´ì§€ ì•ŠëŠ” ì´ìœ ëŠ” masked multi-head attention layerì´ê¸° ë–„ë¬¸ì´ë‹¤. (Attention is All You Need ë…¼ë¬¸ ì°¸ê³ )

### Attention is Cheap

Self Attentionì˜ computational complexityëŠ” $$O(length^2 * dim)$$ì¸ë°, RNNì˜ computation complexityëŠ” $$O(length * dim^2)$$ì´ë‹¤. ë”°ë¼ì„œ lengthê°€ dimë³´ë‹¤ ì‘ì€ ìƒí™©ì—ì„œ í›¨ì”¬ ì ì–´ì§„ë‹¤. ê°•ì˜ì—ì„œ ë‚˜ì˜¨ LSTMì˜ ìƒí™©ì€ lengthì™€ dimì´ ê°™ë”ë¼ë„ 4ë°°ë‚˜ ì ì€ complexityë¥¼ ê°€ì§„ë‹¤.

### Convolution vs Attention vs Multihead Attention

í•˜ì§€ë§Œ Attentionì€ ë¬¸ì œì ì´ ìˆëŠ”ë°, ì˜ˆë¥¼ ë“¤ì–´ `I kicked the ball`ì´ë¼ëŠ” ë¬¸ì¥ì— ëŒ€í•´ì„œ convolutionì„ ì§„í–‰í•œë‹¤ê³  í•˜ë©´, ê¸±ê¸±ì˜ ë‹¨ì–´ì— filterê°€ ë‹¤ë¥¸ê°’ì„ ì ìš©í•˜ë©´ì„œ í•„ìš”í•œ ê°’ì„ ë½‘ì•„ë‚¸ë‹¤. í•˜ì§€ë§Œ, Attentionì€ ê·¸ë¥¼ averagingí•˜ë¯€ë¡œ, í•„ìš”í•œ ì •ë³´ë¥¼ ë½‘ì•„ë‚´ê¸°ê°€ í˜ë“¤ë‹¤. ê·¸ë˜ì„œ multi-head attentionì´ ë‚˜ì™”ë‹¤. ê·¸ë˜ì„œ í•„ìš”í•œ ì •ë³´ë§Œì„ ì ë‹¹íˆ ë½‘ì•„ë‚¼ ìˆ˜ ìˆê²Œ ëœë‹¤.

{% include image.html url="/images/cs224n/14-3.png" description="Multihead Attention" %}

### Results

ë„ˆë¬´ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ê³ , SOTAë„ ë§ì´ ì°ìœ¼ë‹ˆê¹Œ ìµœê·¼ì— ë§ì€ ëª¨ë¸ë“¤ì´ transfomer ê¸°ë°˜ìœ¼ë¡œ ë‚˜ì˜¨ë‹¤. frameworkë“¤ì€ tensor2tensor[^tensor2tensor], Sockeye[^sockeye]ë¥¼ ì°¾ì•„ë³´ì.

[^tensor2tensor]: [github tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor) tensor2tensor repository
[^sockeye]: [github awslabs/sockeye](https://github.com/awslabs/sockeye) sockeye repository

### Importance of Residual Connections

Residual connectionì„ ì´ìš©í•˜ë©´ positional informationì„ higher layerë¡œ ë‹¤ë¥¸ ì •ë³´ì™€ í•¨ê»˜ ì „ë‹¬í•´ì¤„ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.

{% include image.html url="/images/cs224n/14-4.png" description="With Residuals" %}

## _

Attentionì„ ì´ìš©í•˜ëŠ” ë§Œí¼ ì´ìš©í•  ìˆ˜ ìˆëŠ” Transfer Learningê³¼ ê°™ì€ í‚¤ì›Œë“œë¥¼ ì°¾ì•„ë´ë„ ì¢‹ì„ ê²ƒ ê°™ë‹¤. (ì•„ì§ ì´í•´ ì˜ ëª»í•¨) ë­”ê°€ ì†Œê°œí•˜ëŠ” ê²ƒì„ ìœ„ì£¼ë¡œ ì­ˆìš°ìš± ì§€ë‚˜ê°”ëŠ”ë° ë„ˆë¬´ ë¹¨ë¦¬ ì­‰ ì§€ë‚˜ê°€ì„œ í¥ë¯¸ë¡œìš´ ë‚´ìš©ë„ ë§ì•˜ì§€ë§Œ, ì œëŒ€ë¡œ ìºì¹˜ë¥¼ ëª»í•œ ê²ƒ ê°™ì•„ì„œ CS224n ìŠ¤í„°ë””ê°€ ëë‚˜ê³  ë‚˜ë©´ ì´ ê°•ì˜ë§Œ ë‹¤ì‹œ ë³´ì•„ë„ ì¢‹ì„ ê²ƒ ê°™ë‹¤.
