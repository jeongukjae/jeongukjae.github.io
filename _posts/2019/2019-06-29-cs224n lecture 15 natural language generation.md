---
layout: post
title: ğŸ“• CS224n Lecture 15 Natural Language Generation
tags:
  - nlp
  - cs224n
  - machine learning
---

LNGì— ëŒ€í•œ Neural Approachì— ëŒ€í•œ ë°©ë²•ì„ 15ê°•ì—ì„œ ê°•ì˜í•œë‹¤ê³  í•œë‹¤. [ìŠ¬ë¼ì´ë“œ](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture15-nlg.pdf)ëŠ” ì—¬ê¸°ë¡œ.

* LNGì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì‚¬í•­ë“¤ì„ ì•Œë ¤ì£¼ê³ 
* decoding algorithmì— ëŒ€í•´ ë§í•˜ë©°
* NLG taskì™€ í•´ë‹¹ taskë¥¼ ìœ„í•œ neural approachë¥¼ ì•Œë ¤ì£¼ê³ 
* NLGëŠ” ì–´ë–»ê²Œ evaluationì„ ì§„í–‰í•˜ëŠ”ì§€,
* NLG researchì™€ í˜„ì¬ íŠ¸ë Œë“œ, ê·¸ë¦¬ê³  ë¯¸ë˜ëŠ” ì–´ë–¤ì§€

ì— ëŒ€í•´ì„œ ë§í•´ì¤€ëŒ€ìš”.

## Recap: LMs and decoding algorithms

### NLG

NLGëŠ” ìƒˆë¡œìš´ textë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ì‘ì—…ì´ê³ , ì•„ë˜ì™€ ê°™ì€ ê²ƒë“¤ì˜ subcomponentê°€ ë  ìˆ˜ ìˆë‹¤.

* Machine Translation
* (Abstractive) Summarization
* Dialogue (chit-chat and task-based) -> ì´ê±´ ì•„ë§ˆ ì¼ìƒëŒ€í™”ì¸ë“¯í•œë°..?
* Creative writing: storytelling, poetry-generation
* Freeform Question Answering -> ë‹µë³€ì„ ê·¸ëƒ¥ ìƒì„±
* Image captioning

### LM

LMì€ next wordë¥¼ predictingí•˜ëŠ” taskì˜€ê³ , $$ P(y_t \vert y_1,...,y_{t-1}) $$ì˜ í™•ë¥  ë¶„í¬ë¥¼ ë±‰ì–´ë‚¸ë‹¤. ê·¸ë¦¬ê³  RNNì„ ì´ìš©í•˜ë©´ RNN-LMì´ë‹¤.

conditional LMì´ë¼ëŠ” ê²ƒë„ ìˆëŠ”ë°, ì´ê±°ëŠ” `x`ë¼ëŠ” inputì´ ì˜¬ ë•Œê¹Œì§€ ê³ ë ¤í•œë‹¤. ì¦‰, $$ P(y_t \vert y_1, ..., y_{t - 1}, x) $$ê°€ ë˜ëŠ” ê²ƒì´ë‹¤. ì´ê²Œ íŠ¹ë³„í•˜ê²Œ ìƒê°í•˜ê¸° ë³´ë‹¤, ì „ì²´ í…ìŠ¤íŠ¸ `x`ì— ëŒ€í•´ `y`ë¡œ summarizeí•˜ëŠ” ê²ƒì„ ìƒê°í•˜ë©´ ë  ê²ƒ ê°™ë‹¤.

{% include image.html url="/images/cs224n/15-1.png" description="training RNN-LM" %}

### Decoding Algorithm

conditional LMì„ í•™ìŠµí•˜ê³  ë‚˜ë©´ ì´ê²ƒì„ ì–´ë–»ê²Œ NLGì— í™œìš©í•  ê²ƒì¸ê°€?? -> decoding algorithmì„ í†µí•´ í•  ìˆ˜ ìˆë‹¤. ì´ ì „ì— Greedy Decodingê³¼ Beam Search ë‚´ìš©ì„ ë°°ì› ì—ˆê³ , ê°ê°ì˜ ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ë‹¤.

{% include image.html url="/images/cs224n/15-2.png" description="Greedy Decoding" %}

{% include image.html url="/images/cs224n/15-3.png" description="Beam Search (if beam size is 2)" %}

#### Beam Search

Greedy Searchì—ì„œëŠ” ì–´ì°¨í”¼ ê·¸ ìƒí™©ì—ì„œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ê²ƒë§Œ ê³ ë¥¸ë‹¤ì§€ë§Œ, Beam SearchëŠ” beam sizeë¼ëŠ” hyper parameterê°€ í•˜ë‚˜ ë” ìˆìœ¼ë¯€ë¡œ, "beam sizeë¥¼ ì–´ë–»ê²Œ ê³ ë¥¼ ê²ƒì¸ê°€?"ë¥¼ ê³¨ë¼ì•¼ í•œë‹¤.

beam sizeê°€ ì‘ìœ¼ë©´, Greedy Searchì™€ ê±°ì˜ ê°™ì•„ì§„ë‹¤. (beam sizeê°€ 1ì¸ beam searchê°€ greedy searchì´ë‹ˆ...) beam searchê°€ í¬ë©´, computationally expensiveí•˜ë‹¤. ë˜ ë„ˆë¬´ í¬ë©´ NMTì—ì„œ BLEU scoreë¥¼ ë–¨ì–´ëœ¨ë¦´ ìˆ˜ ìˆë‹¤. (ì´ê²Œ short translationì„ ì°¾ì•„ë‚´ê¸° ë•Œë¬¸ì´ë¼ê³ ) ê·¸ë¦¬ê³  ì¼ìƒëŒ€í™” (chit chat) ë¶„ì•¼ì—ì„œëŠ” genericí•˜ê²Œë§Œ ë‹µë³€ì„ ë‚¼ ìˆ˜ë„ ìˆë‹¤.

{% include image.html url="/images/cs224n/15-4.png" description="Effect of beam size in chit chat" %}

#### Sampling based decoding

ì´ê±´ ì²˜ìŒ ë‚˜ì˜¤ëŠ”ë°, ë‹¤ìŒ ë‹¨ì–´ë¥¼ ê³¨ë¼ë‚¼ ë•Œ, step tì—ì„œ í™•ë¥  ë¶„í¬ $$P_t$$ë¡œë¶€í„° ëœë¤í•˜ê²Œ ìƒ˜í”Œë§í•œë‹¤ê³  í•œë‹¤. Greedy decodingì²˜ëŸ¼ í•˜ì§€ë§Œ, argmaxê°€ ì•„ë‹ˆë¼ samplingì„ ì‚¬ìš©í•œë‹¤ê³ .

í•˜ì§€ë§Œ ë„ˆë¬´ ëœë¤í•˜ê²Œ ìƒ˜í”Œë§í•˜ë©´ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ top-nì˜ ë‹¨ì–´ë¡œ ì œí•œí•˜ì—¬ ìƒ˜í”Œë§ í•˜ëŠ” ë°©ë²•ë„ ì‚¬ìš©í•œë‹¤. (top-n sampling) ì—¬ê¸°ì„œë„ top-nì˜ nì„ 1ë¡œ ë‘ë©´ Greedy Searchì™€ ê°™ì•„ì§„ë‹¤. Beam Searchì²˜ëŸ¼ hyper parameterë¥¼ ê³ ë¥´ëŠ” ê²ƒì€ ì¤‘ìš”í•œë°, ì—¬ê¸°ì„œëŠ” nì— í•´ë‹¹í•œë‹¤. nì´ ì»¤ì§€ë©´ diverseí•˜ê³  riskyí•œ ê²°ê³¼ë¥¼ ë‚´ëŠ”ë° ë°˜í•´ nì„ ì¤„ì´ë©´ genericí•˜ê³  safeí•œ ê²°ê³¼ë¥¼ ë‚¸ë‹¤.

#### Softmax Temperature

step tì—ì„œ, LMì´ softmax functionì„ scoreë“¤ì˜ vectorì¸ $$s \in \mathbb R^{\vert V \vert} $$ì— ì ìš©í•˜ì—¬ $$P_t$$ì˜ í™•ë¥  ë¶„í¬ë¥¼ ê³„ì‚°í•œë‹¤ê³  í•œë‹¤. ë§ë§Œ ë“¤ì–´ì„œëŠ” ì €ë„ ë­”ì§€ ëª¨ë¥´ê² ê³  ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.

$$ P_t (w) = \frac {\exp(\frac {s_w} \tau)} {\sum _{w^\prime \in V} \exp(\frac {s_{w^\prime}} {\tau})}$$

ì´ê²Œ $$\tau$$ëŠ” temperature hyperparameterì´ê³ , ì ìš©í•´ë„ ë˜ê³  ì•ˆí•´ë„ ë˜ëŠ” ê°’ì¸ê°€ ë³¸ë° ì´ê±¸ ëŠ˜ë¦¬ë©´ $$P_t$$ë¥¼ ë” uniformí•˜ê²Œ ë§Œë“¤ê³  outputì„ ë” diverseí•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ì¦‰, í™•ë¥  ë¶„í¬ê°€ ë‹¨ì–´ ì „ì²´ì— ê³ ë£¨ í¼ì§„ë‹¤. ë°˜ëŒ€ë¡œ ì¤„ì´ë©´, í™•ë¥  ë¶„í¬ê°€ ë” spikyí•˜ê²Œ ë˜ê³ , outputì€ ëª‡ëª‡ê°œì˜ ë‹¨ì–´ì—ë§Œ ì§‘ì¤‘í•˜ê²Œ ëœë‹¤ê³  í•œë‹¤.

ê·¼ë°!!! ì´ softmax tempaeratureëŠ” ìˆ˜ì‹ì„ ë³´ë©´ ì•Œê² ì§€ë§Œ, decoding algorithmì€ ì•„ë‹ˆê³ , decoding algorithmê³¼ í•¨ê»˜ ì“´ë‹¤.

## NLG Tasks and Neural Approach to them

### Summarization

summarizationì€ input text `x`ì—ëŒ€í•´ summary `y`ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” íƒœìŠ¤í¬ì´ë‹¤. Summarizationì€ single documentì— ê´€í•œ ê²ƒì¼ ìˆ˜ë„ ìˆê³ , multi documentì— ê´€í•œ ê²ƒì¼ ìˆ˜ë„ ìˆë‹¤. summarizationì— ê´€í•´ ì‹¤ì œ ì½”ë“œ/ë°ì´í„°ì…‹ì„ ì°¸ê³ í•´ë³´ê³  ì‹¶ë‹¤ë©´ [GitHub - mathsyouth/awesome-text-summarization](https://github.com/mathsyouth/awesome-text-summarization)ì„ ì°¸ê³ í•˜ì. (ì—­ì‹œ awesome-**)

summarizationì€ ë‘ê°€ì§€ ë°©ì‹ì´ ìˆëŠ”ë°, Extractive summarizationê³¼ Abstractive summarizationì´ë‹¤. ì „ìëŠ” ì¤‘ìš”í•œ ë¶€ë¶„ì„ ê³¨ë¼ë‚´ëŠ” ê²ƒì´ê³ , í›„ìëŠ” ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ì•„ì˜ˆ ë§Œë“¤ì–´ë‚¸ë‹¤.

#### Pre Neural Summarization

Pre-Neural Summarizationì— ëŒ€í•´ ë§í•´ì¤€ë‹¤. mostly extractiveí•˜ê³ , MTì²˜ëŸ¼ pipelineì´ ìˆëŠ”ë°, Content Selectionì„ í•˜ê³  Information Orderingì„ í•œ í›„ Sentence Realizationì„ í•œë‹¤. ì´ë¥¼ ìœ„í•´ì„œ ê°ê°ì˜ ë¬¸ì¥ì— ëŒ€í•´ ì ìˆ˜ë¥¼ ë§¤ê¸°ëŠ” ê³¼ì •ì´ í•„ìš”í•œë°, ì´ê²ƒì€ topicì´ í•´ë‹¹ ë¬¸ì¥ì— ì¡´ì¬í•˜ëŠ”ì§€ ë“±ì„ ë³´ê³  ê³„ì‚°í•œë‹¤ê³  í•œë‹¤. ì•„ë‹ˆë©´ ë¬¸ì¥ì˜ ìœ„ì¹˜ë¼ë˜ê°€..?

#### Evaluation: ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

{% include image.html url="/images/cs224n/15-5.png" description="Equation of ROUGE" %}

BLEUì²˜ëŸ¼ ngram baseì¸ë°, ë‹¤ë¥¸ ì ì€ ë¬¸ì¥ì´ ì§§ë‹¤ê³  íŒ¨ë„í‹°ë¥¼ ì£¼ì§„ ì•ŠëŠ”ë‹¤ëŠ” ì ì´ë‹¤. ê·¸ë¦¬ê³  ROUGEëŠ” recallì— ê¸°ë°˜í•˜ê³ , BLEUëŠ” precisionì— ê¸°ë°˜í•œë‹¤. ê°ê°ì˜ ì–´ë–¤ ë¬¸ì œë¥¼ í’€ê³ ì í•˜ëŠ”ì§€ì— ë”°ë¼ ì„ íƒí•œê±°ë¼ ë³´ë©´ ë  ê²ƒ ê°™ë‹¤. ê·¼ë°, F1 version of ROUGEë„ ìì£¼ ì“°ì¸ë‹¤.

#### Neural Summarization

ì²˜ìŒì€ single-document abstractive summarizationì„ translation taskë¡œ ë³´ê³  seq2seqë¡œ í’€ì–´ë´ë„ ì¢‹ì§€ ì•Šì„ê¹Œ? í•˜ëŠ” ìƒê°ì— 2015ë…„ì— seq2seq + attentionì„ ì ìš©í•œ ê´€ë ¨ ë…¼ë¬¸ì´ ë‚˜ì™”ë‹¤ê³  í•œë‹¤. [^rush] ìµœê·¼ì—ëŠ” Hierarchical / multi-level attentionì„ ì ìš©í•˜ê¸°ë„ í•˜ê³ , global contentsì— ëŒ€í•´ ì²˜ë¦¬í•˜ê¸°, high-level content selectionì— ëŒ€í•´ ì‹ ê²½ì“°ê¸°ë„ í•œë‹¤ê³  í•œë‹¤. ê·¸ë¦¬ê³  pre-neural summarizationì—ì„œ ë‚˜ì˜¨ ì•„ì´ë””ì–´ë¥¼ ê°™ì´ ì ìš©í•˜ê¸°ë„ í•œë‹¤ê³ . ROUGEë¼ëŠ” scoreê°€ ìˆìœ¼ë‹ˆ RLë„ ì‹œë„í•œë‹¤ê³  í•œë‹¤.

[^rush]: [https://arxiv.org/abs/1509.00685](https://arxiv.org/abs/1509.00685) ì´ ë…¼ë¬¸ì„ ì°¸ê³ í•´ë³´ì

ê·¼ë° ì´ê²Œ ë¬¸ì œì ì´, seq2seq + attentionì— ì˜ì¡´í•˜ë©´ ë§ì€ ì˜ ë‚˜ì˜¤ì§€ë§Œ, ë””í…Œì¼ì„ ì˜ ì¡ì•„ë‚´ì§€ ëª»í•œë‹¤ê³  í•œë‹¤. ê·¸ë˜ì„œ Copy Mechanismì„ seq2seqì™€ í•¨ê»˜ ì‚¬ìš©í•´ì„œ ê·¸ëŸ° ê²ƒì„ ì¡ì•„ë‚¸ë‹¤ê³  í•˜ëŠ”ë°, ê°•ì˜ì—ì„œ ì„¤ëª…í•˜ëŠ” ë°©ì‹ì€ ê°ê°ì˜ decoding ê³¼ì •ì—ì„œ $$p_{gen}$$ì„ ë¯¸ë¦¬ ê³„ì‚°í•œ í›„ Context Vectorë‘ Attention Distributionì´ë‘ ì˜ ì„ì–´ì„œ generation distributionê³¼ copying distributionì„ ê³„ì‚°í•´ ë‚¸ í›„ ê·¸ê±¸ í† ëŒ€ë¡œ $$P(w)$$ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤ê³  í•œë‹¤.

{% include image.html url="/images/cs224n/15-6.png" description="Copy Mechanism" %}

í•˜ì§€ë§Œ Copy Mechanismë„ ë¬¸ì œê°€ ìˆëŠ”ë°, ë„ˆë¬´ ë§ì´ copyí•œë‹¤ëŠ” ì ê³¼ input documentê°€ ë„ˆë¬´ ê¸¸ë‹¤ë©´ content selection ì„±ëŠ¥ì´ ë„ˆë¬´ ë–¨ì–´ì§„ë‹¤ëŠ” ì ì´ë‹¤.

#### Bottom up summarization

ê¸°ì¡´ì˜ pre-neural summarizationì€ content selectionê³¼ surface realizationìœ¼ë¡œ ë‚˜ëˆ„ì–´ì§„ë‹¤. í•˜ì§€ë§Œ, neural approachëŠ” ê·¸ëŸ° ê²ƒì—†ì´ í•˜ë‚˜ë¡œ ë¬¶ì—¬ì„œ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— global content selection strategyê°€ ë¶€ì¡±í•  ìˆ˜ ë°–ì— ì—†ë‹¤. ê·¸ë˜ì„œ bottom-up summarizationì„ í•˜ì!

neural sequence tagging modelì„ ì‚¬ìš©í•´ì„œ í¬í•¨í•  ë‹¨ì–´ì™€ í¬í•¨í•˜ì§€ ì•Šì„ ë‹¨ì–´ë¥¼ ìš°ì„  ì„ íƒí•œë‹¤. ê·¸ë¦¬ê³ ë‚˜ì„œ seq2seq + attentionìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.[^bus]

{% include image.html url="/images/cs224n/15-7.png" description="Bottom-up Summarization" %}

[^bus]: [https://arxiv.org/abs/1808.10792](https://arxiv.org/abs/1808.10792) ì´ê²ƒë„ ì°¸ê³ í•´ë³´ì

#### Neural summarization via RL

ROUGE-Lì„ optimizeí•˜ê¸° ìœ„í•´ ë°”ë¡œ RLì„ ì‚¬ìš©í•˜ì. ê·¼ë° ì´ê²Œ ì–´ì°Œë³´ë©´ ë‹¹ì—°í•  ì •ë„ë¡œ ROUGE scoreëŠ” ë†’ê²Œ ë‚˜ì™”ë‹¤. í•˜ì§€ë§Œ, human judgement scoreëŠ” ë‚®ê²Œ ë‚˜ì™”ë‹¤.

### Dialogue

ëŒ€ëµ ì•„ë˜ê°™ì€ ì¢…ë¥˜ê°€ ìˆë‹¤.

* Task oriented dialogue
  * assistive
  * co-operative
  * adversarial
* social
  * chit-chat

#### seq2seq based dialogue

dialogueë¥¼ text summarization ë¬¸ì œì—ì„œë„ í•«í–ˆë˜ seq2seqë¡œ í’€ê³ ì í–ˆëŠ”ë° ì•„ë˜ì •ë„ì˜ ë¬¸ì œê°€ ë‚˜íƒ€ë‚¬ë‹¤.

* genericness / boring responses
  * beam searchì—ì„œ rare wordsë¥¼ upweightí•œë‹¤.
* irrelevant responses
  * inputê³¼ response ì‚¬ì´ì— maximum mutual informationì„ optimizeí•˜ë„ë¡ í•´ì„œ ê´€ë ¨ìˆê²Œ ë§Œë“¤ì
* repetition
  * beam searchì—ì„œ ë°”ë¡œ n-gram ë°˜ë³µë˜ëŠ” ê²ƒì„ ë§‰ì•„ë²„ë¦°ë‹¤.
* lack of context
* lack of consistent persona

#### Storytelling

ì´ê±° ì¬ë°Œì–´ ë³´ì—¬ì„œ [ê´€ë ¨ ë¯¸ë””ì›€ ê¸€](https://medium.com/@samim/generating-stories-about-images-d163ba41e4ed) ë§í¬ ì €ì¥!

ì´ë¯¸ì§€ë¡œë¶€í„° storytellingê°™ì€ paragraphë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤.

ê·¼ë° ì´ê²Œ ë„ˆë¬´ ì–‘ì´ ë§ì€ì§€ ì—„ì²­ ê±´ë„ˆë›´ë‹¤. ìì„¸í•œê±´ ìŠ¬ë¼ì´ë“œ ì°¸ê³ 

## NLG Evaluation

word overlap based metrics ë“¤ì´ ìˆë‹¤. í•˜ì§€ë§Œ ì´ëŸ° ê²ƒë“¤ì€ machine translationê³¼ ê°™ì€ íƒœìŠ¤í¬ì— ì ì ˆí•˜ì§€ ì•Šê³ , summarizationì´ë‚˜ dialogueì—” ì˜¤íˆë ¤ ì•ˆì¢‹ì„ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.

{% include image.html url="/images/cs224n/15-8.png" description="Metric vs Human judgement" %}

ê·¸ëŸ¼ perplexityëŠ” ì–´ë–¨ê¹Œ? LMë§Œ ê´€ë ¨í•´ì„œëŠ” ê´œì°®ì„í…ë°, generationì€ ì˜í–¥ì„ ì•ˆë°›ëŠ”ë‹¤. word embedding based metricsì€ human judgementì™€ ë³„ ê´€ë ¨ì´ ì—†ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.

ê·¸ë¦¬ê³  ì „ì²´ì ìœ¼ë¡œ ì ì ˆí•œ metricsì´ ì—†ë‹¤. í•˜ì§€ë§Œ íŠ¹ì • íƒœìŠ¤í¬ì— ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” metricì€ ì¶©ë¶„íˆ ìˆê¸° ë•Œë¬¸ì— ì˜ ê³¨ë¼ì“°ëŠ” ê²Œ ì¢‹ì„ ê²ƒ ê°™ë‹¤.

ê·¼ë° ê·¸ë ‡ë‹¤ê³  human judgementëŠ” ë¬´ì¡°ê±´ ì¢‹ì€ê°€?ëŠ” ë˜ ì•„ë‹ˆë‹¤. ë¹„ì‹¸ê³  ëŠë¦¬ê³  ì¼ê´€ì ì´ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.
