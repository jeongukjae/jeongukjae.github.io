---
layout: post
title: ğŸ“ƒ Word2Vec ë…¼ë¬¸ ì •ë¦¬í•´ë³´ê¸°
tags:
  - machine learning
  - nlp
  - paper
  - cs224n
---

CS224nì„ ë“£ê¸° ì‹œì‘í•˜ê³  ë‚˜ì„œ ê°™ì´ ë‚˜ì˜¤ëŠ” suggested readingsë¥¼ ê°€ë” ì½ëŠ”ë°, word2vecì˜ ë…¼ë¬¸ìœ¼ë¡œ ìœ ëª…í•œ [Efficient Estimation of Word Representation in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)ë„ ê·¸ ì¤‘ í•˜ë‚˜ì´ë‹¤. ê³µë¶€ë¥¼ í•˜ë©´ì„œ ë‹¤ì‹œ ë³¼ë§Œí•œ ë‚´ìš©, ìƒê°ë‚¬ë˜ ë‚´ìš©ë“¤ì„ ì ì–´ë‘ê¸° ìœ„í•´ ì´ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í–ˆë‹¤.

## word2vec

word2vecëŠ” word embedding ë°©ë²• ì¤‘ì˜ í•˜ë‚˜ë¡œ, ë…¼ë¬¸ì— ì œì•ˆí•œ ëª¨ë¸ì— ëŒ€í•œ ê°„ëµ ì„¤ëª…ìœ¼ë¡œ "for computing continous vector representations fof words from very large data"ë¼ê³  ì í˜€ìˆë‹¤.

ì²˜ìŒìœ¼ë¡œ NLP ìª½ìœ¼ë¡œ ê³µë¶€ë¥¼ í•˜ëŠ” ê±°ë¼ ì–´ë µê¸°ë„ í•˜ê³  ì¬ë°Œê¸°ë„í•œ ê°œë…ì´ ë§ì´ ë‚˜ì™”ë‹¤. ê·¸ë˜ë„ CS231nì˜ ê°•ì˜ ëª‡ê°œë¥¼ ì°¾ì•„ë³´ê³  ì •ë¦¬í–ˆì—ˆëŠ”ë°, ê·¸ ì´í›„ì— ë³´ë‹ˆê¹Œ ë‚˜ë¦„ ê·¸ëŸ­ì €ëŸ­ ì´í•´í• ë§Œí•œ ë…¼ë¬¸ì´ì—ˆë˜ ê²ƒ ê°™ë‹¤. CS224n êµìˆ˜ë‹˜ì´ ì„¤ëª… ì˜ í•˜ì‹  ê²ƒë„ ìˆê² ì§€ë§Œ..?

ì—¬íŠ¼ ë‹¤ì‹œ ë³¼ë§Œí•œ ë‚´ìš©ë§Œ ì‘ì„±í•˜ê³ , conclusionê°™ì€ ê²ƒë“¤ì€ ë¹¼ê³  ì‘ì„±í•œë‹¤.

## Introduction

2013ì— ë‚˜ì˜¨ ë…¼ë¬¸ì¸ë°, ê·¸ ë•Œì˜ ë§ì€ NLP ë°©ë²•ë“¤ì€ ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ ì‘ì€ ë‹¨ìœ„ë¡œ ì·¨ê¸‰í•˜ê³  ìˆì—ˆë‹¤ê³  í•œë‹¤. ê·¼ë° ì´ëŸ° ë°©ë²•ì€ ë‹¨ì–´ ì‚¬ì´ì˜ ìœ ì‚¬ë„ë¥¼ íŒë³„í•˜ê¸°ê°€ í˜ë“¤ë‹¤. ê·¸ë¦¬ê³  ì„±ëŠ¥ì´ ë°ì´í„°ì˜ í¬ê¸°ì— í¬ê²Œ ì¢Œìš°ëœë‹¤. í•˜ì§€ë§Œ ì–´ëŠì •ë„ ì¥ì ë„ ê°€ì§€ê³  ìˆëŠ”ë°, ê°„ë‹¨í•˜ê³  robustí•˜ë©°, ë‚˜ë¦„ ì¢‹ì€ ì„±ëŠ¥ì„ ëƒˆë‹¤ê³  í•œë‹¤. Ngramì´ ê·¸ ì¤‘ í•˜ë‚˜ë¼ê³  í•œë‹¤. [^ngram] í•˜ì§€ë§Œ í†µê³„ì ì¸ ë°©ë²•ì„ ì´ìš©í•˜ì§€ ì•Šê³  ê¸°ê³„í•™ìŠµì„ ì‚¬ìš©í•œ ë°©ë²•ì„ ì´ìš©í•˜ë©´ì„œ distributed representationì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ ê½¤ ì¢‹ì€ ì ‘ê·¼ë²•ìœ¼ë¡œ ë– ì˜¬ëë‹¤ê³  í•œë‹¤. [^nnlm]

ê·¸ë˜ì„œ 50 ~ 100ì •ë„ dimensionì— word vectorë¡œ ì„ë² ë”© í•˜ëŠ” ê²ƒì„ ì†Œê°œí•œë‹¤. ê·¸ëŸ¬ë©´ì„œ word vectorì˜ ë†€ë¼ìš´ ì ì„ ì†Œê°œí•œë‹¤. word2vecì„ ì„¤ëª…í•˜ë‹¤ ë³´ë©´ ë§ì´ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë¥¼ ë”í•˜ê³  ë¹¼ëŠ” ì—°ì‚°ì„ ì•„ë˜ì²˜ëŸ¼ ì˜ˆì‹œë¥¼ ë³´ì—¬ì£¼ë©´ì„œ ì†Œê°œí•œë‹¤.

> *vector(â€Kingâ€) - vector(â€Manâ€) + vec- tor(â€Womanâ€)* results in a vector that is closest to the vector representation of the word Queen

ë‹¨ì–´ë¥¼ ì—°ì†ì ì¸ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ì´ ë…¼ë¬¸ì´ ì²˜ìŒì€ ì•„ë‹ˆë‹¤. í•˜ì§€ë§Œ í•´ë‹¹ ë°©ì‹ì„ ì œì•ˆí–ˆë˜ ë§ì€ ë…¼ë¬¸ë“¤ ì¤‘ NNLMì— ê´€í•œ ë…¼ë¬¸[^nnlm]ì—ì„œ ì œì•ˆí•œ ë°©ì‹ì´ ë§ì€ ì£¼ëª©ì„ ë°›ì•˜ë‹¤. feedforward neural network ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤ê³  í•œë‹¤.

## Model Architecture

neural networkë¥¼ ì‚¬ìš©í•´ì„œ distributed representationì„ êµ¬í˜„í•œë‹¤. í•´ë‹¹ ì•„í‚¤í…ì³ë¥¼ ë§í•˜ê¸° ì „ì— computational complexityë¥¼ ë¨¼ì € ë…¼í•œë‹¤ê³  í•œë‹¤. ëª¨ë¸ì„ ì†Œê°œí•˜ë©´ì„œ ì–´ë–»ê²Œ ì •í™•ë„ëŠ” ë†’ì´ê³  complexityëŠ” ë‚®ì¶”ì—ˆëŠ”ì§€ ì†Œê°œí•´ì¤€ë‹¤. ì¼ë‹¨ ì•ìœ¼ë¡œ ë‚˜ì˜¬ ëª¨ë¸ë“¤ì€ ì´ëŸ° training complexityë¥¼ ê°–ëŠ”ë‹¤ê³  í•œë‹¤.

$$ O = E \times T \times Q $$

$$E$$ê°€ training epoch, $$T$$ê°€ training setì— ì¡´ì¬í•˜ëŠ” ë‹¨ì–´ì˜ ìˆ˜, $$Q$$ëŠ” ì•ìœ¼ë¡œ ê° ëª¨ë¸ì—ì„œ ì •ì˜ë  ê²ƒì´ë¼ê³  í•œë‹¤. ë³´í†µ epochëŠ” 3 ~ 50ìœ¼ë¡œ ì •í•˜ê³ , $$T$$ëŠ” 1 billionê¹Œì§€ë¡œ ì •í•œë‹¤ê³  í•œë‹¤. í•™ìŠµì€ SGDì™€ backpropì„ ì´ìš©í•œë‹¤ê³ .

### NNLM

NNLMì€ input, projection, hidden, output layerë¡œ êµ¬ì„±ì´ ë˜ì–´ ìˆë‹¤. input layerì—ì„œ $$V$$ê°€ ë‹¨ì–´ì˜ ìˆ˜ë¼ê³  í•˜ë©´, 1-of-$$V$$ coding(one hot)ì„ ì‚¬ìš©í•´ì„œ ë‹¨ì–´ë¥¼ ì¸ì½”ë”©í•˜ê³ , ê³µìœ ë˜ëŠ” projectionìš© í–‰ë ¬ì„ ì‚¬ìš©í•´ì„œ $$N \times D$$ ì°¨ì›ì˜ projection layerë¡œ projectí•œë‹¤ê³  í•œë‹¤. $$N \times D$$ ì°¨ì›ì´ë‹ˆê¹Œ í•œë²ˆì— $$N$$ê°œì˜ ë‹¨ì–´ë¥¼ ì‚¬ìš©ê°€ëŠ¥í•˜ë‹¤. ë³´í†µ $$N$$ì€ 10ì •ë„ë¡œ ì“°ê³ , projection layerëŠ” 500 ~ 2000 ì°¨ì›ì¯¤ ì“´ë‹¤ê³  í•œë‹¤. hiddden layerëŠ” 500 ~ 1000ì°¨ì›ì¯¤, output layerì— ëŒ€í•´ì„œëŠ” $$V$$ë¼ê³ ë§Œ ë‚˜ì™€ìˆë‹¤. ì ì´ë ‡ê²Œ ë˜ë‹ˆê¹Œ íŒŒë¼ë¯¸í„° ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

$$ Q = N \times D + N \times D \times H + H \times V $$

$$ H \times V $$ê°€ ë§¤ìš° ë¹„ì‹¼ ì—°ì‚°ì´ë¯€ë¡œ hierarchical softmaxë¥¼ ì‚¬ìš©í•œë‹¤ê³  í•œë‹¤. (ì´ê±´ ë‹¤ìŒì— ì •ë¦¬í•´ì•¼ì§€) ë˜ëŠ” normalizeë¥¼ ì•ˆí•œë‹¤ê³ . ì–´ì©„ë“  ê·¸ëŸ° ë°©ë²•ì„ ì ìš©í•˜ë©´ $$ log_2 V$$ ì •ë„ë¡œ ì¤„ì–´ë“¤ì–´ì„œ $$ N \times D \times H $$ê°€ ì œì¼ complexityí•´ì§„ë‹¤.

word2vecì—ì„œëŠ” huffman binary treeë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ì´ëŠ” $$V$$ë¥¼ $$log_2(Unigram\_perplexity(V))$$ì •ë„ë¡œ ì¤„ì—¬ì¤€ë‹¤ê³  í•œë‹¤. ì´ë ‡ê²Œ ì ì–´ë‘ê³ .. ì‚¬ì‹¤ì€ $$V$$ëŠ” bottleneckì´ ì•„ë‹ˆë‹ˆ ê·¸ë ‡ê²Œê¹Œì§€ ì¤‘ìš”í•˜ì§„ ì•Šë‹¤ê³  í•œë‹¤.

### RNNLM

RNNì˜ í™œìš©ì´ Feedforward NNLMì˜ í•œê³„(context lengthë¥¼ ëª…ì‹œí•œë‹¤ë˜ê°€)ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì œì•ˆë˜ì—ˆë‹¤. ê·¸ ê²°ê³¼ ì´ë¡ ì ìœ¼ë¡œ ì¼ë°˜ NNë³´ë‹¤ ë³µì¡í•œ íŒ¨í„´ì— ëŒ€í•´ í›¨ì”¬ íš¨ìœ¨ì ì¸ í‘œí˜„ì´ ê°€ëŠ¥í–ˆë‹¤.

## New Log-linear Models

ì´ì „ [Model Architecture](#model-architecture)ì—ì„œ ì†Œê°œí–ˆë˜ ê²ƒë“¤ì€ neural netì´ ë§¤ë ¥ì ì„ì„ ì•Œê²Œ í•´ì£¼ì—ˆì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ ë³µì¡ì„±ì€ non-linear hiddne layerì—ì„œ ì˜¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤. ($$ N \times D \times H $$) ê·¸ë˜ì„œ ìƒˆë¡œìš´ ëª¨ë¸ì—ì„œëŠ” NNLMì„ ë‘ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ì„œ í•™ìŠµì„ í•œë‹¤ê³  í•œë‹¤. ìš°ì„  continous word vectorë¥¼ ê°„ë‹¨í•œ ëª¨ë¸ë¡œ í•™ìŠµì‹œí‚¨ ë‹¤ìŒì— N-gram NNLMì„ ê·¸ ìœ„ì—ì„œ í•™ìŠµì‹œí‚¨ë‹¤.

### Continuous Bag of words model

ê·¸ë˜ì„œ ì´ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ì•„í‚¤í…ì³ ì¤‘ í•˜ë‚˜ê°€ feedforward NNLMê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ, non-linear hidden layerë¥¼ ì—†ì• ê³ , projection matrix ë¿ë§Œ ì•„ë‹Œ layerê¹Œì§€ ëª¨ë“  ë‹¨ì–´ë“¤ì´ ê³µìœ í•˜ê²Œ í•œ ëª¨ë¸ì´ë‹¤. ê·¸ë˜ì„œ ëª¨ë“  ë‹¨ì–´ê°€ ë˜‘ê°™ì´ projectëœë‹¤. ê·¸ë¦¬ê³  ì´ë¥¼ ì„¤ëª…í•˜ë©´ì„œ ì•„ë˜ì²˜ëŸ¼ ì„¤ëª…ì„ í•˜ëŠ”ë°,

> Furthermore, we also use words from the future

ì´ëŠ” ì•„ë§ˆ Ngramê³¼ ë‹¤ë¥´ê²Œ ë‹¨ì–´ì˜ ì• ë¿ë§Œ ì•„ë‹ˆë¼ ë’¤ì— ìˆëŠ” ë‹¨ì–´ë“¤ë„ ì°¸ê³ í•œë‹¤ëŠ” ë§ì¸ ê²ƒ ê°™ë‹¤. ê·¸ë˜ì„œ complexityëŠ” ì•„ë˜ì™€ ê°™ì•„ì§„ë‹¤.

$$ Q = N \times D + D \times log_2 (V) $$

ì• ë’¤ë¡œ 4ê°œì˜ ë‹¨ì–´ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ window sizeë¥¼ ê²°ì •í–ˆì„ ë•Œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì´ì—ˆë‹¤ê³  í•œë‹¤.

### Continuous skip-gram mdoel

ë‘ë²ˆì§¸ ì•„í‚¤í…ì³ëŠ” CBOWì™€ ìœ ì‚¬í•œë°, í•œ ë‹¨ì–´ë¡œ ê°™ì€ ë¬¸ì¥ì•ˆì˜ ì£¼ìœ„ì˜ ë‹¨ì–´ë¥¼ classificationì„ maximizeí•˜ëŠ” ëª¨ë¸ì´ë‹¤. training complexityëŠ” ì•„ë˜ì™€ ê°™ë‹¤. $$ C$$ê°€ maximum distance of the wordsë¼ê³  í•˜ëŠ”ë°, window sizeì™€ ë¹„ìŠ·í•œ ì˜ë¯¸ë¡œ ë°›ì•„ë“¤ì´ë©´ ë  ê²ƒ ê°™ë‹¤.

$$ Q = C \times ( D + D \times log_2 V ) $$

ë­ ì´ë ‡ê²Œ ë§í•˜ëŠ” ê²ƒë³´ë‹¤ ì‰½ê²Œ ë§í•˜ë©´ CBOWëŠ” í˜„ì¬ ë‹¨ì–´ë¥¼ ì£¼ìœ„ ë‹¨ì–´(context)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê³ , Skip-Gramì€ í˜„ì¬ ë‹¨ì–´ë¡œ ì£¼ìœ„ ë‹¨ì–´(context)ë¥¼ ì˜ˆì¸¡í•œë‹¤.

{% include image.html url="/images/2019-04-06-word2vec/cbow-skip-gram.png" description="cbowì™€ skip-gram ëª¨ë¸ ê·¸ë¦¼" %}

---

[^ngram]: [T. Brants et al. 2007](https://www.aclweb.org/anthology/D07-1090.pdf) Ngramì˜ ë…¼ë¬¸ìœ¼ë¡œ í†µê³„ì ì¸ ë°©ë²•ë¡ ì„ ì‚¬ìš©í–ˆë‹¤ê³  í•œë‹¤.
[^nnlm]: [A Neural Probabilistic Language Model](http://www.jmlr.org/papers/v3/bengio03a.html) word2vecì˜ NNLMì— ê´€í•œ ë…¼ë¬¸ìœ¼ë¡œ, distributed representationì„ ì„¤ëª…í•˜ë©° ì°¸ê³ ë¡œ ë‹¬ë ¤ìˆëŠ” ë…¼ë¬¸ ì¤‘ í•˜ë‚˜ì´ë‹¤.
