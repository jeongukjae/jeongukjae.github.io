---
layout: post
title: ğŸ“• CS224n Lecture 9 Practical Tips for Final Projects
tags:
  - cs224n
---

ì´ë²ˆ ê°•ì˜ëŠ” ìœ íŠœë¸Œë¡œ ì—…ë¡œë“œë˜ëŠ” ê°•ì˜ëŠ” ì•„ë‹ˆê³ , ìŠ¬ë¼ì´ë“œì™€ ë…¸íŠ¸ë§Œ ì˜¬ë¼ì™”ë‹¤. í•˜ì§€ë§Œ í•œë²ˆ ì¶©ë¶„íˆ í›‘ì–´ë³¼ë§Œí•œ ê²ƒ ê°™ì•„ì„œ ì •ë¦¬ë¥¼ í•´ë³¸ë‹¤.

## final projects

ìŠ¤í„°ë”” í•˜ëŠ” ì‚¬ëŒë“¤ë¼ë¦¬ final projectë„ ê¸°ë³¸ìœ¼ë¡œ ì£¼ì–´ì§€ëŠ” í”„ë¡œì íŠ¸ë¥¼ êµ¬í˜„í•´ë³´ê¸°ë¡œ í–ˆê¸° ë•Œë¬¸ì— ì´ë²ˆ ê°•ì˜ ìŠ¬ë¼ì´ë“œë¥¼ ì‚´í´ë³´ê¸°ë¡œ í–ˆë‹¤. SQuAD question answeringì´ ê¸°ë³¸ final projectì´ë‹¤. 1 ~ 3ëª…ì˜ ì¸ì›ìœ¼ë¡œ í•œë‹¤ê³  í•œë‹¤. ì–¸ì–´ë‚˜ í”„ë ˆì„ì›Œí¬ì˜ ì œí•œì€ ì—†ë‹¤ê³ . ê·¸ë¦¬ê³  ê¸°ë³¸ì ìœ¼ë¡œ ì£¼ì–´ì§€ëŠ” starter codeê°€ pytorchë¼ê³  í•œë‹¤.

## research topic

ì¼ë‹¨ final projectì˜ ì£¼ì œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ SQuADì´ì§€ë§Œ, project type (topic)ë„ ì •í•´ì•¼ í•œë‹¤ê³  í•œë‹¤.

1. ëª¨ë¸ì˜ applicationì„ ì°¾ì•„ë³´ê³  ì–´ë–»ê²Œ íš¨ìœ¨ì ìœ¼ë¡œ ì ìš©í•  ì§€ ì°¾ê±°ë‚˜
2. complex neural architectureë¥¼ êµ¬í˜„í•´ë³´ê³  íŠ¹ì • ë°ì´í„°ì— ëŒ€í•œ performanceë¥¼ ì¸¡ì •í•´ë³´ê±°ë‚˜
3. new, variant NN modelì„ êµ¬í˜„í•´ì„œ ì‹¤í—˜ì ì¸ ë°ì´í„°ë¡œ í–¥ìƒì„ ë³´ì—¬ì£¼ê±°ë‚˜
4. ëª¨ë¸ì˜ ë™ì‘ë²•ì„ ë¶„ì„í•˜ê±°ë‚˜
5. rare theoretical project, ê·¸ëƒ¥ ê°œì©ŒëŠ” ê±¸ ê°€ì ¸ì˜¤ê±°ë‚˜..?

ëŒ€ì¶© ìœ„ì˜ ë‹¤ì„¯ê°€ì§€ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤.

## finding data

ë°ì´í„°ëŠ” ì•Œì•„ì„œ ë§Œë“¤ì–´ì„œ, íšŒì‚¬ì—ì„œ ì“°ëŠ”ê±¸ ìƒ˜í”Œë§Œ ë“¤ê³ ì™€ì„œ ì“¸ ìˆ˜ë„ ìˆì§€ë§Œ, ì´ë¯¸ ì˜ ì„ ë³„ëœ ë°ì´í„°ì…‹ì„ ì“°ëŠ” ê²ƒë„ ì¢‹ë‹¤. ì•„ë˜ì˜ ì‚¬ì´íŠ¸ë“¤ì„ ì°¸ê³ í•´ë³´ì.

* [https://catalog.ldc.upenn.edu](https://catalog.ldc.upenn.edu)
* [https://linguistics.stanford.edu/resources/resources-corpora](https://linguistics.stanford.edu/resources/resources-corpora)
* [http://statmt.org](http://statmt.org)
* [https://universaldependencies.org](https://universaldependencies.org)
* ì—­ì‹œ ê°“ê°“ ìºê¸€
* research paper ì°¸ê³ í•˜ëŠ” ê²ƒë„ ì¢‹ìŒ
* [github - niderhoff/nlp-datasets](https://github.com/niderhoff/nlp-datasets)
* [https://machinelearningmastery.com/datasets-natural-language-processing/](https://machinelearningmastery.com/datasets-natural-language-processing/)

## review of gated neural sequence models

ì´ ë¶€ë¶„ì€ ì—­ì‹œ ê°•ì˜ê°€ ì—†ìœ¼ë‹ˆ ì´í•´í•˜ê¸° í˜ë“¤ì–´ì„œ ëˆˆì— ë³´ì¸ ê²ƒë§Œ ì •ë¦¬í•˜ìë©´

* ì§ê´€ì ìœ¼ë¡œ RNNì—ì„œ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€ ì´í•´í•˜ê³  ì‚¬ìš©í•˜ì (ì´ê±° ì˜ ëª¨ë¥´ê² ë‹¤)
* vanishing gradient ë¬¸ì œë¥¼ ì¡°ì‹¬í•˜ì
* naive transition function (tanhê°™ì€)ì„ ì“°ëŠ” ê²ƒì´ ë¬¸ì œê°€ ë ê¹Œ..? -> ë‚´ ìƒê°ì—ëŠ” ë  ê²ƒ ê°™ì§€ë§Œ ì•„ì§ í™•ì‹¤í•˜ì§€ ì•Šë‹¤.
* backpropì€ ì˜ ì•ˆë ìˆ˜ë„ ìˆë‹¤.
  * shortcut connectionì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.
  * adaptive shortcut connectionë„ ë§Œë“¤ ìˆ˜ ìˆë‹¤. -> ì„ íƒì ìœ¼ë¡œ unnecessary connectionë“¤ì€ ì—†ì•¤ë‹¤.

## a couple of MT topics

ì ê·¸ë˜ì„œ ì¢€ ë¬¸ì œë¡œ ì—¬ê²¨ì§€ëŠ” topicë“¤ì€?

* softmax computationì´ ë„ˆë¬´ costê°€ í¬ë‹¤.
* word generation problem -> ì´ê±° ìŠ¬ë¼ì´ë“œ ì´í•´ë¥¼ ëª»í•˜ê³˜ë‹¤ ã… ã… ã… 

ê·¸ë˜ì„œ Hierarchical softmax ì‚¬ìš©ê°€ëŠ¥í•˜ê³  noise contrastive estimationë„ ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë‹¤. attentionë„ ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ê³  ì¼ë‹¨ í‚¤ì›Œë“œë§Œ ì ì—ˆë‹¤.

ê·¸ëŸ¼ Evaluationì€ ì–´ë–»ê²Œ í•  ìˆ˜ ìˆì„ê¹Œ?

automatic metricì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆëŠ”ë°, BLEUë‚˜, TER, METEORê°™ì€ ê²ƒë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì‹¤ì œë¡œ ì‚¬ìš©í•  ë•Œ ì—¬ê¸° ë‹¤ì‹œ ë´ì•¼ê² ë‹¤.

## doing your research

í•œë²ˆ ì—ì‹œë¥¼ ë“¤ì–´ë³´ìë©´,

1. summarizationì´ë¼ëŠ” taskë¥¼ ì •í•˜ì
2. datasetì„ ì •í•˜ì
    1. search for academic datasets
        1. newsroom summarization datasetì„ ì‚¬ìš©í•œë‹¤!
    2. define your own data (harder one)
        2. íŠ¸ìœ—, ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸, ë‰´ìŠ¤ë“¤ë„ ë°ì´í„°ì…‹ì´ ë  ìˆ˜ ìˆë‹¤.
3. dataset hygiene
    1. ì‹œì‘í•  ë•Œ ë°”ë¡œ í…ŒìŠ¤íŠ¸ì…‹ê°™ì€ ê²ƒì€ ë¶„ë¦¬í•´ë‘ì
4. metricë„ ì •í•˜ì!
    1. summarizationì€ rougeê°™ì€ ê²ƒë„ ì“¸ ìˆ˜ ìˆë‹¤.
5. baselineì„ ì •í•˜ì
    1. ë„ˆë¬´ ì˜ë‚˜ì˜¤ë©´ ë¬¸ì œê°€ ë„ˆë¬´ ì‰¬ì› ë˜ ê±°ë‹¤. ë‹¤ì‹œí•˜ì
6. Always be close to your data!
    1. visualize the dataset
    2. collect summary datset
    3. look at erros
    4. analyze how different hyperparameters affect performance
7. ë‹¤ë¥¸ ì‹œë„ë„ ë§ì´ í•´ë³´ì

ì´ëŸ° ë‚´ìš©ë„ ê°™ì´ ìˆë‹¤.

* overfitë„ ì¡°ì‹¬í•´ë¼
* validationì´ë‘ test setì„ ë”°ë¡œ ë‘ê³  ì˜ ì‚´í´ë³´ì•„ë¼
* training/tuning/dev/test setê°™ì€ ê²ƒì„ ì˜ êµ¬ë¶„í•´ë¼

RNNì„ í•™ìŠµí•  ë• ì•„ë˜ì™€ ê°™ì€ ë‚´ìš©ë„ ì‚´í´ë³´ì

1. LSTMì´ë‚˜ GRUë¥¼ ì¨ë³´ì
2. orthogonalí•˜ê²Œ recurrent matricesë¥¼ ì´ˆê¸°í™”í•˜ì
3. ë‹¤ë¥¸ matricesë“¤ì„ sensible scaleë¡œ ë§Œë“¤ì
4. forget gate biasë¥¼ 1ë¡œ ë‘ì (default to rememberingì´ë‹¤)
5. adaptive learning rateë¥¼ ì‚¬ìš©í•˜ì
6. clip the norm of the gradient. (1 ~ 5ê°€ ì ë‹¹í•œ thresholdì´ë‹¤)
7. dropoutì„ verticallyí•˜ê²Œ ì ìš©ì‹œí‚¤ê±°ë‚˜ Bayesian Dropoutì„ ì‚¬ìš©í•˜ì
8. í•™ìŠµì€ ì¢€ ê¸°ë‹¤ë¦¬ì
