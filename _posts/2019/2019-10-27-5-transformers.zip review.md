---
layout: post
title: ğŸ“ƒ transformers.zip ë¦¬ë·°
tags:
  - paper
---

[transformers.zip: Compressing Transformers with Pruning and Quantization](https://web.stanford.edu/class/cs224n/reports/custom/15763707.pdf)ì´ë¼ëŠ” ë…¼ë¬¸ì¸ë°, urlì„ ë³´ë‹ˆê¹Œ cs224n reportsì¸ ë“¯ ì‹¶ë‹¤.. [https://github.com/robeld/ERNIE](https://github.com/robeld/ERNIE)ë¡œ ê°€ë©´ ì†ŒìŠ¤ì½”ë“œë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

## Abstract

Transformer architectureì— compression tenchniqueì„ ì—¬ëŸ¬ê°€ì§€ ì ìš©í–ˆë‹¤. quantization ë°©ë²•ì„ Song Han et al., 2015ì—ì„œ ì–¸ê¸‰í•œ k-means apporachë‘ Maximilian Lam, 2018ì˜ ìˆ˜ì •ëœ ë²„ì „ì¸ binarizationì„ ì ìš©í•´ë³´ì•˜ë‹¤ê³  í•œë‹¤. ê·¸ë¦¬ê³  iterative magnitude pruningë„ í•´ë³´ì•˜ë‹¤ê³ .

## Introduction

quantizationì€ performance ì†ì‹¤ì—†ì´ ì—„ì²­ë‚˜ê²Œ ì••ì¶•í•  ìˆ˜ ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. (float32ë¥¼ float16ìœ¼ë¡œë§Œ ê³ ì³ë„ ì ˆë°˜ì´ë‹ˆê¹Œ..) Song Han et al., 2015ì—ì„œ 5% ì•ˆìª½ìœ¼ë¡œë§Œ ì†ì‹¤ì„ ë³´ë©´ì„œ computation resourceë‘ memory costsë¥¼ ì—„ì²­ ì¤„ì´ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤¬ë‹¤.

ê·¸ë˜ì„œ ê²°ë¡ ì ìœ¼ë¡œ ì´ ë…¼ë¬¸ì—ì„œëŠ” ì´ë ‡ê²Œ í•œë‹¤.

* k-means algorithmì„ êµ¬í˜„í•´ì„œ 4 bit representationìœ¼ë¡œ ë§Œë“¤ì–´ì„œ 5.85x ì••ì¶•ì„ í•˜ê³ ë„ ì›ë˜ ì„±ëŠ¥ì˜ 98.43% ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤.
* binarization algorithmì„ êµ¬í˜„í•´ì„œ ì–¼ë§ˆë‚˜ ë¹ ë¥´ê³ , ì–¼ë§ˆë‚˜ ì„±ëŠ¥ ì†ì‹¤ì´ ìˆëŠ”ì§€ ë¹„êµí•´ë³¸ë‹¤.
* iterative magnitude pruningì„ êµ¬í˜„í•´ë³¸ë‹¤.
* self-attentionì„ visualizingí•˜ëŠ” ê²ƒì— ì´ˆì ì„ ë§ì¶”ì–´ë³¸ë‹¤. íŠ¹íˆ, compressionì´ ì§„í–‰ë  ìˆ˜ë¡ modelì´ sharper representationì„ ë½‘ì•„ë‚´ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.

## Related Works

ê·¼ë° ì—¬ê¸°ì„œ ì†Œê°œí•˜ëŠ” ë°©ë²•, ì•Œê³ ë¦¬ì¦˜ ë“±ë“±ì´ ê±°ì˜ ë‹¤ ì› ë…¼ë¬¸ì„ ì¼ê±°ì•¼ ì´í•´ê°ˆ ê²ƒ ê°™ì•„ì„œ ë‚˜ì¤‘ì— ì­‰ ì½ìœ¼ë©´ì„œ ë‹¤ ì´í•´í•´ì•¼í•  ë“¯ ì‹¶ë‹¤.

## Approach / Methods

### Quantization

#### K-Means

* Song Han, Huizi Mao, and William J. Dally. â€œDeep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Codingâ€. In: CoRR abs/1510.00149 (2015). arXiv:1510.00149. URL: http://arxiv.org/abs/1510.00149.

ìœ„ ë…¼ë¬¸ì— ë‚˜ì˜¨ êµ¬í˜„ì„ ì´ìš©í–ˆëŠ”ë°, Linear initializationì„ í–ˆë‹¤ê³  í•œë‹¤. ì§€ê¸ˆ ë´ì„œ ì´í•´ ëª»í•˜ë‹ˆ ì½ê³  ë‹¤ì‹œ ì½ì.

#### Modified Binarization

* Maximilian Lam. â€œWord2Bits - Quantized Word Vectorsâ€. In: CoRR abs/1803.05651 (2018). arXiv:
1803.05651. URL: http://arxiv.org/abs/1803.05651.
* Matthieu Courbariaux and Yoshua Bengio. â€œBinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1â€. In: CoRR abs/1602.02830 (2016). arXiv: 1602.02830. URL:
http://arxiv.org/abs/1602.02830.

ì½ê³  ë‹¤ì‹œ ì½ì–´ì•¼ í•  ë“¯ ì‹¶ë‹¤.

### Pruning

* Song Han et al. â€œLearning both Weights and Connections for Efficient Neural Networksâ€. In: CoRR abs/1506.02626 (2015). arXiv: 1506.02626. URL: http://arxiv.org/abs/1506.02626.

ìœ„ ë…¼ë¬¸ êµ¬í˜„ ì´ìš©í–ˆë‹¤ê³  í•œë‹¤.

## Experiments

* WMT English - German translation taskë¡œ í–ˆë‹¤.
* ì••ì¶• ì •ë„ - ì„±ëŠ¥ì„ ë¹„êµí•˜ê¸° ìœ„í•´ì„œ BLEU scoreì™€ ì••ì¶• ë¹„ìœ¨ì„ ë¹„êµí–ˆë‹¤ê³  í•œë‹¤.

{% include image.html url="/images/2019/10-27-transformers-zip/fig1.png" description="ì„±ëŠ¥ ë¹„êµ" %}

### Quantitative Analysis

pruningì´ ìƒê°ë³´ë‹¤ ì•ˆì¢‹ì•˜ë‹¤ê³ . quantizationë³´ë‹¤ ì•ˆì¢‹ì•˜ê³ , Gale et al.ì— ìˆëŠ” ì„±ëŠ¥ì„ ì¬í˜„í•´ë‚´ê¸° í˜ë“¤ì—ˆë‹¤ê³  í•œë‹¤. 90%ë¥¼ ì—†ì• ê³ , 90%ì˜ ì„±ëŠ¥ì„ ìœ ì§€í–ˆë‹¤ê³  í•˜ëŠ”ë°, ê·¸ê²Œ í˜ë“¤ì—ˆë‹¤ê³  í•œë‹¤. ì•„ë§ˆ ì´ê²Œ ì´ ë…¼ë¬¸ì—ì„œ ë§í•˜ê¸¸ hyperparameter tuningì´ ë¶€ì¡±í•´ì„œ ê·¸ëŸ° ê²ƒ ê°™ë‹¤ê³ .

### Qualitative Analysis

{% include image.html url="/images/2019/10-27-transformers-zip/fig2.png" description="attention ë¹„êµ" %}

ìƒê°ë³´ë‹¤ 4-bitê¹Œì§€ ì••ì¶•í•œê²Œ ì—„ì²­ ì„±ëŠ¥ì´ ì¢‹ì•˜ë‹¤ê³ . ë§¨ ìœ„ì—ì„œë¶€í„° ì›ë˜ ëª¨ë¸ - 8bit model - 4bit model - binarized modelì´ë‹¤. ë¶€ë¡ì—ì„œë„ ë‹¤ë¥´ê²Œ ë¹„êµí—€ëŠ”ë°, ì›ë˜ ëª¨ë¸ê³¼ 8bit model, 4bit modelì€ ê±°ì˜ êµ¬ë¶„ì´ ë¶ˆê°€ëŠ¥í–ˆë‹¤ê³  í•œë‹¤.

## ì½ì„ ê²ƒë“¤

ì´ ë…¼ë¬¸ì—ì„œ ë­”ê°€ë¥¼ ìƒˆë¡­ê²Œ ì œì‹œí•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ê·¸ëƒ¥ ì´ê²ƒì €ê²ƒ í•´ë³´ê³  ì´ê±° ì¢‹ë”ë¼ í•˜ëŠ” ë‚´ìš©ì´ë¼ ì´í•´í•˜ë ¤ë©´ ë‹¤ë¥¸ ê±¸ ë§ì´ ì½ì–´ë³´ì•„ì•¼ í•  ë“¯ ì‹¶ë‹¤. ã… ã…  ì—¬ê¸°ì„œ ë‚˜ì¤‘ì— ì ë‹¹íˆ ë³¼ ì‹œê°„ ë˜ëŠ” ê²ƒë§Œ ë´ì•¼ì§€.

* Yann Le Cun, John S. Denker, and Sara A. Solla. â€œOptimal Brain Damageâ€. In: Advances in Neural Information Processing Systems. Morgan Kaufmann, 1990, pp. 598â€“605.
* B. Hassibi, D. G. Stork, and G. J. Wolff. â€œOptimal Brain Surgeon and general network pruningâ€. In: IEEE International Conference on Neural Networks. Mar. 1993, 293â€“299 vol.1. DOI: 10.1109/ICNN. 1993.298572.
* Yunchao Gong et al. â€œCompressing Deep Convolutional Networks using Vector Quantizationâ€. In: CoRR abs/1412.6115 (2014). arXiv: 1412.6115. URL: http://arxiv.org/abs/1412.6115.
* Song Han, Huizi Mao, and William J. Dally. â€œDeep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Codingâ€. In: CoRR abs/1510.00149 (2015). arXiv: 1510.00149. URL: http://arxiv.org/abs/1510.00149.
* Song Han et al. â€œLearning both Weights and Connections for Efficient Neural Networksâ€. In: CoRR abs/1506.02626 (2015). arXiv: 1506.02626. URL: http://arxiv.org/abs/1506.02626.
* Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. â€œDistilling the Knowledge in a Neural Networkâ€. In: arXiv e-prints, arXiv:1503.02531 (Mar. 2015), arXiv:1503.02531. arXiv: 1503.02531 [stat.ML].
* Matthieu Courbariaux and Yoshua Bengio. â€œBinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1â€. In: CoRR abs/1602.02830 (2016). arXiv: 1602.02830. URL: http://arxiv.org/abs/1602.02830.
* Fengfu Li and Bin Liu. â€œTernary Weight Networksâ€. In: CoRR abs/1605.04711 (2016). arXiv: 1605. 04711. URL: http://arxiv.org/abs/1605.04711.
* Jacob Devlin. â€œSharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPUâ€. In: CoRR abs/1705.01991 (2017). arXiv: 1705.01991. URL: http://arxiv.org/abs/ 1705.01991.
* Yew Ken Chia and Sam Witteveen. â€œTransformer to CNN: Label-scarce distillation for efficient text classificationâ€. In: 2018.
* Maximilian Lam. â€œWord2Bits - Quantized Word Vectorsâ€. In: CoRR abs/1803.05651 (2018). arXiv: 1803.05651. URL: http://arxiv.org/abs/1803.05651.
* Jerry Quinn and Miguel Ballesteros. â€œPieces of Eight: 8-bit Neural Machine Translationâ€. In: CoRR abs/1804.05038 (2018). arXiv: 1804.05038. URL: http://arxiv.org/abs/1804.05038.
* Jean Senellart et al. â€œOpenNMT System Description for WNMT 2018: 800 words/sec on a single-core CPUâ€. In: Proceedings of the 2nd Workshop on Neural Machine Translation and Generation. Melbourne, Australia: Association for Computational Linguistics, 2018, pp. 122â€“128. URL: http: //aclweb.org/anthology/W18- 2715.
