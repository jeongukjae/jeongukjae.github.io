---
layout: post
title: ğŸ“• CS330 Lecture 2 Multi-Task & Meta-Learning Basics
tags:
  - cs330
---

2ê°•ì´ê³  Multi-Task & Meta-Learning Basicsì´ë‹¤.

* ê°•ì˜ ì‚¬ì´íŠ¸ <http://cs330.stanford.edu/>
* ê°•ì˜ ë¹„ë””ì˜¤ <https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5>
* [2ê°• pdf](http://cs330.stanford.edu/slides/cs330_multitask_transfer_2020.pdf)

---

## Multitask learning

### Model

* MultiTask Learning objective: $$\min_\theta \sum^T_{i=1}\mathscr{L}_i(\theta, \mathscr{D}_i) $$ (Loss: $$\mathscr{L}$$, Dataset: $$\mathscr{D}$$)
* ê°€ì¥ ì‰½ê²Œ multitaskë¥¼ í•˜ëŠ” ë°©ë²•: ì—¬ëŸ¬ Expert Modelì„ ë§Œë“  ë‹¤ìŒ íƒœìŠ¤í¬ ì¢…ë¥˜ì— ë”°ë¼ì„œ ì‚¬ìš©í•œë‹¤. -> No Shared Parameters.
* ë˜ ë‹¤ë¥¸ ë°©ë²•: Classifierì— íƒœìŠ¤í¬ ì¸ë±ìŠ¤ë¥¼ í”¼ì³ë¡œ ë„£ì–´ì¤€ë‹¤.
  * ì˜ê²¬: one-hot vectorë¡œ ë„£ì–´ì¤€ë‹¤ëŠ” ëŠë‚Œì¸ ê²ƒ ê°™ì€ë° í•™ìŠµì´ ì˜ ë ê¹Œ? ë‹¤ë¥¸ classifierë¥¼ ì“°ëŠ” í¸ì´ ì¢‹ì•„ë³´ì´ëŠ”ë°
* ë˜ ë‹¤ë¥¸ ë°©ë²•ë“¤
  * Multi-head classification -> ì¼ë°˜ì ìœ¼ë¡œ ë‚´ê°€ ì•Œê³  ìˆëŠ” Multi-task learning. MT-DNNì„ ìƒê°í•˜ë©´ ëœë‹¤.
  * Input vectorì— íƒœìŠ¤í¬ì˜ ì„ë² ë”©ì„ ê³±í•´ì£¼ì–´ì„œ ë¶„ë¥˜í•˜ëŠ” ë°©ë²• (multiplicative gating)
    * Multiplicative conditioningì€ ë„¤íŠ¸ì›Œí¬ì™€ headë“¤ì„ ì „ë¶€ í•œêº¼ë²ˆì— generalizeí•œë‹¤.
    * ì˜ê²¬: í•˜ì§€ë§Œ ì ìš©í•  ë¶€ë¶„ì„ ì°¾ê¸°ê°€ í˜ë“¤ì–´ ë³´ì¸ë‹¤. ë¹„ìŠ·í•œ ì¢…ë¥˜ì˜ ë¶„ë¥˜ë¥¼ í•´ì•¼í•˜ê³ , ë ˆì´ë¸” ê°¯ìˆ˜ê°€ ê°™ì•„ì•¼í•˜ë©°, íƒœìŠ¤í¬ ê°„ì˜ ë ˆì´ë¸”ì´ ê°ê° ìƒê´€ê´€ê³„ê°€ ìˆì–´ì•¼ í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì¸ë‹¤.
* Conditioning ë°©ë²•ì„ ê³ ë¥´ëŠ” ë²•
  * Problem Dependent
  * Largely guided by intuition or knowledge of the problem
  * currently more of an art than a science

### objective

* Vanilla MTL Objectiveê°€ ì˜ ë™ì‘í•˜ê¸´ í•˜ëŠ”ë°, weighted sumì„ ì“¸ë•Œë„ ë§ë‹¤. $$\min_\theta \sum^T_{i=1}w_i\mathscr{L}_i(\theta, \mathscr{D}_i)$$
  * -> ë‚˜ë„ ì´ ë°©ë²•ì„ ë” ë§ì´ ì”€
* weightingí•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆëŠ”ë° ì•„ë˜ì •ë„
  * various heuristics (Chen et al. GradNorm. ICML 2018)
  * use task uncertainty (see Kendall et al. CVPR 2018) <https://arxiv.org/abs/1705.07115>
    * ê°„ë‹¨íˆ ì‚´í´ë´¤ëŠ”ë° ì´ê²Œ ì¼ë°˜ì ì¸ ê²½ìš°ì— ì¢‹ì•„ë³´ì¸ë‹¤.
  * optimize for the worst-case task loss for fairness and robustness

### optimization

* ë³„ë‹¤ë¥¸ ë‚´ìš©ì€ ì—†ê³ , taskë“¤ì´ uniformí•˜ê²Œ ì˜ ë½‘íˆëŠ”ì§€ ì‚´í´ë³´ë©´ ì¢‹ë‹¤ê³ 
* regression ë¬¸ì œì¼ ê²½ìš° ê°™ì€ ìŠ¤ì¼€ì¼ì¸ì§€ ì²´í¬í•˜ì.

## Common Challenges in MTL

* **Negative transfer**: if independent networks work the best
  * Maybe optimization problem.
    * caused by cross-task interference.
    * Tasks may learn at different rates.
  * maybe representational capacity
    * MT networks often need to be much larger than single-task model
  * if nagative transfer problem occurs, share less parameters.
* **Overfitting**
  * Share more paraemters

## Case study

* Recommending What Video to Watch Next: A Multitask Ranking System
  * Recommendataion systems of Youtube
  * conflicting objectives
    * videos that users will rate highly
    * videos that users will share
    * videos that users will watch
    * ìœ„ ì…‹ì¤‘ ì–´ë–¤ê±¸ ì¶”ì²œí•´ì•¼í•˜ë‚˜
  * implicit bias caused by feedback: ëª¨ë¸ì˜ ì¶”ì²œ ê²°ê³¼ê°€ ìœ ì € í–‰ë™ì— ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ, ë¶€ì •ì ì¸ í”¼ë“œë°±ì´ ë  ìˆ˜ë„ ìˆë‹¤.
  * ìì„¸í•œ ë‚´ìš©ì€ ë…¼ë¬¸ì„ ì½ì

## MTL vs Transfer Learning

ìŠ¬ë¼ì´ë“œì—ëŠ” Transfer Learningê³¼ì˜ ë¹„êµê°€ ì¡´ì¬. ë¹„ë””ì˜¤ì—ëŠ” meta learningê³¼ì˜ ë¹„êµê°€ ì¡´ì¬

* MTL: Solve multiple tasks at once
* Transfer Learning: Solve target tasks after solving source task by transferring knowledge learned from source task.
  * Key assumption: Cannot acces source task dataset during transfer
* Transfer learning is a valid solution to MTL (not vice versa)
