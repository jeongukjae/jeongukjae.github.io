---
layout: post
title: í¸ë¦¬í•œ NLPë¥¼ ìœ„í•œ TensorFlow-Textì™€ RaggedTensor
tags:
    - tensorflow
    - nlp
    - conference
featured: true
---

TensorFlow Everywhere Korea([festa](https://festa.io/events/1395), [fb](https://www.facebook.com/groups/TensorFlowKR/permalink/1412985029042551/)) ì—ì„œ ë°œí‘œí•œ "í¸ë¦¬í•œ NLPë¥¼ ìœ„í•œ TensorFlow-Textì™€ RaggedTensor"ë¥¼ ì¤€ë¹„í•˜ë©´ì„œ ì‘ì„±í•œ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

* ë°œí‘œìë£Œ: [ë§í¬](/pdfs/tfekr_tftext.pdf)
* ë°œí‘œì˜ìƒ: [í˜ë¶ ê²Œì‹œê¸€ ë§í¬](https://www.facebook.com/100002683652633/videos/3258341000931990/)
* ë°œí‘œì—ì„œ ì‚¬ìš©í•œ ì½”ë“œì˜ GitHub ë§í¬: <https://github.com/jeongukjae/nsmc-tf-text>

***ì´ í¬ìŠ¤íŠ¸ëŠ” TensorFlow 2.4.1ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.***

ë¨¼ì € ì´ ë°œí‘œë¥¼ í•˜ëŠ” ì´ìœ ë¥¼ ì„¤ëª…ë“œë¦¬ìë©´, ì €ëŠ” PyTorchë¥¼ êµ‰ì¥íˆ ë§ì´ ì‚¬ìš©í–ˆì—ˆì–´ìš”.
PyTorch í–‰ì‚¬ì—ì„œ Contributorë¡œ ì´ë¦„ì´ ì˜¬ë¼ê°ˆ ì •ë„ë¡œ ê¸°ì—¬ë„ ì¼ì •ê¸°ê°„ í–ˆì—ˆê³ ìš”.
êµ‰ì¥íˆ ì¢‹ì€ í”„ë¡œì íŠ¸ë¼ê³  ìƒê°í•˜ê³ , ì—°êµ¬ ë¶„ì•¼ì—ì„œëŠ” PyTorchë§Œí•œ ìœ ì—°ì„±ì„ ê°€ì ¸ê°€ê¸° í˜ë“¤ë‹¤ê³  ìƒê°í•˜ê¸´ í•˜ì§€ë§Œ, í”„ë ˆì„ì›Œí¬ ë‹¨ì—ì„œ ë§ì€ ë¶€ì¡±í•¨ì„ ëŠê¼ˆì–´ìš”.
ê·¸ë¦¬ê³  TensorFlowë¥¼ ê¹Šê²Œ ì‚¬ìš©í•˜ê¸° ì‹œì‘í–ˆëŠ”ë°, ìƒê°ë³´ë‹¤ ëŒ€í˜•ëª¨ë¸ì´ ì•„ë‹Œ ì´ìƒ ì •ë§ ë¹ ë¥¸ ì‹¤í—˜ ì´í„°ë ˆì´ì…˜ì„ ê°€ì ¸ê°ˆ ìˆ˜ ìˆì—ˆê³ , ì§§ìœ¼ë©´ì„œë„ ëª…í™•í•œ ì½”ë“œë¥¼ ë§ì´ ê°€ì ¸ê°ˆ ìˆ˜ ìˆì—ˆì–´ìš”.
ê·¸ë˜ì„œ ê·¸ëŸ° ë©´ì„ NLP ìª½ìœ¼ë¡œ, TensorFlowì˜ String ì—°ì‚°, TensorFlow Textì™€ ê´€ë ¨ì§€ì–´ ì„¤ëª…ë“œë¦¬ë ¤ í•©ë‹ˆë‹¤.

## ëª©ì°¨

1. [NLP ê·¸ë¦¬ê³  TensorFlow Text, RaggedTensor](#1-nlp-ê·¸ë¦¬ê³ -tensorflow-text-raggedtensor)
1. [RaggedTensor](#2-raggedtensor)
    1. [RaggedTensor vs SparseTensor](#21-raggedtensor-vs-sparsetensor)
    1. [ìì—°ì–´ì™€ RaggedTensor](#22-ìì—°ì–´ì™€-raggedtensor)
1. [ìœ ìš©í•œ `tf.strings`, tensorflow-text](#3-ìœ ìš©í•œ-tfstrings-tensorflow-text)
    1. [í•œê¸€ê³¼ Unicode](#31-í•œê¸€ê³¼-unicode)
    1. [í…ìŠ¤íŠ¸ ì²˜ë¦¬ì—ì„œì˜ `tf.strings`](#32-í…ìŠ¤íŠ¸-ì²˜ë¦¬ì—ì„œì˜-tfstrings)
    1. [tensorflow-textì˜ Tokenizer](#33-tensorflow-textì˜-tokenizer)
1. [NSMCë¡œ TensorFlow Textì™€ RaggedTensor ë§›ë³´ê¸°](#4-nsmcë¡œ-tensorflow-textì™€-raggedtensor-ë§›ë³´ê¸°)
    1. [ë°ì´í„° ì²˜ë¦¬](#41-ë°ì´í„°-ì²˜ë¦¬)
    1. [ëª¨ë¸ í•™ìŠµ](#42-ëª¨ë¸-í•™ìŠµ)

## 1. NLP ê·¸ë¦¬ê³  TensorFlow Text, RaggedTensor

NLPì™€ ì¹œìˆ™í•˜ì§€ ì•Šìœ¼ì‹œë‹¤ë©´, TensorFlow-Textì™€ ìµìˆ™í•˜ì§€ ì•Šìœ¼ì‹¤ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
RaggedTensorëŠ” ë¹„ë””ì˜¤ë‚˜ ì˜¤ë””ì˜¤ìª½ì´ì‹œë¼ë©´ ì–´ì©Œë©´ ì¹œìˆ™í•˜ì‹¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
ë‘ ê¸°ëŠ¥ì€ NLPì—ì„œ ë¹ ì ¸ì„œëŠ” ì•ˆë˜ëŠ” ê¸°ëŠ¥ì¸ë°, í•˜ë‚˜ëŠ” ë°ì´í„° ì²˜ë¦¬ë¥¼ ë„ì™€ì£¼ê³ , í•˜ë‚˜ëŠ” ë°ì´í„° í‘œí˜„ì„ ë„ì™€ì¤ë‹ˆë‹¤.

TensorFlow Textë¥¼ ë¨¼ì € ì„¤ëª…ë“œë¦¬ë©´, í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ ëª¨ë¸ì—ì„œ í•„ìš”ë¡œ í•˜ëŠ” ì „ì²˜ë¦¬ë‚˜, ê¸°íƒ€ ìœ ìš©í•œ ê¸°ëŠ¥ë“¤ì„ TensorFlow Graphì•ˆì—ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë„ì™€ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
ë‹¤ë¥¸ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì•¼ì™€ëŠ” ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ NLP ë¶„ì•¼ì˜ ìì—°ì–´ëŠ” ìˆ˜í•™ ì—°ì‚° ë§Œìœ¼ë¡œ ì „ì²˜ë¦¬ë¥¼ í•´ë‚´ê¸°ì— ì¡°ê¸ˆ ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤.
ê·¸ë˜ì„œ TensorFlow Coreê°€ ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ì—°ì‚°ë§Œì„ ì§€ì›í•œë‹¤ë©´ TensorFlow TextëŠ” ë”¥ëŸ¬ë‹ìš©ìœ¼ë¡œ í•„ìš”í•œ í’ë¶€í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ì§€ì›í•˜ëŠ” ê²ƒì´ë¼ ìƒê°í•˜ì‹œë©´ ë©ë‹ˆë‹¤.
ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” TensorFlow Coreì— ìˆëŠ” ì—°ì‚°ë“¤ê³¼ ë”ë¶ˆì–´ TensorFlow Textì˜ ìœ ìš©í•œ ê¸°ëŠ¥ë“¤ì„ ì†Œê°œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

RaggedTensorëŠ” ì–´ë–»ê²Œ ë³´ë©´ ë‹¨ìˆœí•œ ë°ì´í„° í‘œí˜„ ì»¨í…Œì´ë„ˆë¼ê³  ìƒê°í•˜ì‹¤ ìˆ˜ë„ ìˆì§€ë§Œ, TensorFlowì˜ Keras layerì™€ í•¨ê»˜ ì‚¬ìš©í•œë‹¤ë©´ ì •ë§ ì‰½ê²Œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ëª¨ë¸ë§ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë¬¼ë¡  í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ì˜¤ë””ì˜¤ë‚˜ ë¹„ë””ì˜¤ê°™ì€ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì—ë„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ í•´ë‹¹ ë‚´ìš©ì€ ì œ ì „ë¬¸ë¶„ì•¼ê°€ ì•„ë‹ˆê¸°ë„ í•˜ê³ , í¬ìŠ¤íŠ¸ ë‚´ìš©ì„ ë²—ì–´ë‚˜ë‹ˆ ì˜†ìœ¼ë¡œ ì ì‹œ ì œì³ë‘ê³  RaggedTensorì˜ ê¸°ëŠ¥ì„ ì†Œê°œë“œë¦¬ê² ìŠµë‹ˆë‹¤.

## 2. RaggedTensor

TensorFlow Textë¥¼ ë¨¼ì € ì–¸ê¸‰í•˜ê¸´ í–ˆì§€ë§Œ, ì¡°ê¸ˆ ë” ë¶„ëŸ‰ì´ ì ì€ RaggedTensorë¥¼ ë¨¼ì € ì†Œê°œí•´ë“œë¦¬ê³ , ë‚˜ì¤‘ì— TensorFlow Textë¥¼ ì†Œê°œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

### 2.1. RaggedTensor vs SparseTensor

{% include image.html url="/images/2021/02-27-tfeverywhere-tftext/1.png" width=80 %}

ë§ì€ ë¶„ë“¤ì´ SparseTensorì—ëŠ” ìµìˆ™í•˜ì‹¤ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ì—ì„œë„ Sparse Tensorë¼ê³  ë¶€ë¥´ê³ , ìˆ˜í•™ì ì¸ ëª…ì¹­ë„ Sparseì´ê¸° ë•Œë¬¸ì— ë”ìš± ê·¸ë˜ë³´ì…ë‹ˆë‹¤.
SparseTensorëŠ” DenseTensorì´ì§€ë§Œ ë§ì€ ë¶€ë¶„ì´ ë¹„ì–´ìˆì–´ì„œ ê°’ê³¼ ì¸ë±ìŠ¤ë§Œ ì €ì¥í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤.
ê·¸ë˜ì„œ ë”ìš± Sparseí• ìˆ˜ë¡ ê°™ì€ í¬ê¸°ì„ì—ë„ ë” ì ì€ ê³µê°„ë§Œì„ ì°¨ì§€í•˜ë©´ì„œ ë” ë¹ ë¥¸ ì—°ì‚°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ê·¸ì— ë¹„í•´ RaggedTensorëŠ” í¬ê¸°ê°€ ì •í•´ì§€ì§€ ì•Šì€ ë¶ˆê·œì¹™í•œ í¬ê¸°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
SparseTensorê°€ ê·¸ëŸ° ê²ƒì²˜ëŸ¼ ë¬¼ë¡  data agnosticí•©ë‹ˆë‹¤.
RaggedTensorëŠ” ì•„ë˜ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ìƒì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

```python
>>> tf.RaggedTensor.from_value_rowids(
... values=[3, 1, 4, 1, 5, 9, 2, 6],
... value_rowids=[0, 0, 0, 0, 2, 2, 2, 3])
<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6]]>
>>>
>>> tf.RaggedTensor.from_row_lengths(
... values=[3, 1, 4, 1, 5, 9, 2, 6],
... row_lengths=[4, 0, 3, 1])
<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6]]>
>>>
>>> tf.RaggedTensor.from_row_splits(
... values=[3, 1, 4, 1, 5, 9, 2, 6],
... row_splits=[0, 4, 4, 7, 8])
<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6]]>
```

ìœ„ ì„¸ê°€ì§€ ë°©ë²•ì´ ì „ë¶€ ê°™ì€ RaggedTensorë¥¼ ë§Œë“œëŠ” ë°©ë²•ì…ë‹ˆë‹¤.
ê·¼ë° ì´ë ‡ê²Œ ë‹¨ìˆœíˆ "ê¸°ì¡´ê³¼ ë‹¤ë¥¸ ìë£Œí˜•ì´ë‹¤!" ì •ë„ë¼ë©´ êµ³ì´ ì†Œê°œë“œë¦´ ì´ìœ ëŠ” ì—†ê² ì£ ?
RaggedTensorë¥¼ ì‚¬ìš©í•  ë•Œì˜ ì´ì ì€ Keras Layerì™€ ê°™ì´ ì¼ì„ ë•Œ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 2.2. ìì—°ì–´ì™€ RaggedTensor

ë§ì€ ë¶„ë“¤ì´ ê¸°ì¡´ì— ìì—°ì–´ë¥¼ ì „ì²˜ë¦¬ í›„ì— tokenize, index ë³€í™˜(ë•Œë¡œëŠ” tokenizeì™€ í•©ì³ì§€ê¸°ë„ í•˜ì£ ), padding, ê·¸ë¦¬ê³  ë‰´ëŸ´ ë„·ì—ì„œì˜ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ì‹¤ í…ë°ìš”,
Ragged Tensorë¥¼ ì‚¬ìš©í•œë‹¤ë©´, padding ë‹¨ì„ ì—†ì•¨ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì•„ë˜ê°€ ê·¸ ì˜ˆì‹œì…ë‹ˆë‹¤.

```python
>>> model = tf.keras.Sequential([
... tf.keras.layers.Input(shape=[None], dtype=tf.int32, ragged=True),
... tf.keras.layers.Embedding(32, 16),
... tf.keras.layers.LSTM(16),
... tf.keras.layers.Dense(16, activation='relu'),
... tf.keras.layers.Dense(3, activation='softmax'),
... ])
>>> model(tf.ragged.constant([[1, 2, 3, 4], [1, 2, 3], [4, 5, 6, 7, 8, 9]]))
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[0.33290705, 0.33258146, 0.3345115 ],
       [0.3322196 , 0.33285874, 0.3349216 ],
       [0.332826  , 0.33172128, 0.3354527 ]], dtype=float32)>
```

ë§Œì¼ RaggedTensorê°€ ì—†ë‹¤ë©´, ë¯¸ì„¸í•œ ì°¨ì´ì§€ë§Œ íŒ¨ë”©ì˜ ê¸¸ì´ì— ë”°ë¼ í‘œí˜„ê°’ì´ ì¡°ê¸ˆì”© ë‹¬ë¼ì§€ê±°ë‚˜ maskê°’ì„ ì¼ì¼íˆ ì‹ ê²½ì“°ë©´ì„œ ì½”ë“œë¥¼ ì‘ì„±í•´ì•¼ê² ì£ .
ì´ëŸ° ê¸°ëŠ¥ë“¤ì„ ì˜ ì´ìš©í•˜ëŠ” ê²ƒì´ ì½”ë“œë¥¼ ì½ê¸° ì‰½ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ì§€ë¦„ê¸¸ì´ ë˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ë‹¤ë§Œ ì•„ì§ê¹Œì§€ ë‹¨ì ì€ LayerNormalizationê³¼ ê°™ì€ ì»´í¬ë„ŒíŠ¸ë“¤ì´ RaggedTensorë¥¼ ì§€ì›í•˜ì§€ ì•Šì„ ë¿ë”ëŸ¬, Batch Matmulê³¼ ê°™ì€ ì¤‘ìš”í•œ ì—°ì‚°ë“¤ì´ RaggedTensorê°€ ì§€ì›ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê°„ë‹¨í•œ ëª¨ë¸ì— êµ­í•œëœë‹¤ëŠ” ì ì´ ìˆìŠµë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´ Transformer ê²Œì—´ì˜ ëª¨ë¸ì€ ì•„ë¬´ë˜ë„ êµ¬ì„±í•˜ê¸° í˜ë“­ë‹ˆë‹¤.

## 3. ìœ ìš©í•œ `tf.strings`, tensorflow-text

ê·¸ë ‡ë‹¤ë©´ ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ `tf.strings` ì—°ì‚°ê³¼ tensorflow text ì—°ì‚°ì„ ì§šì–´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

### 3.1. í•œê¸€ê³¼ Unicode

{% include image.html url="/images/2021/02-27-tfeverywhere-tftext/2.png" width=80 %}

ë‹¤ë“¤ ì•„ì‹œë‹¤ì‹¶ì´ í•œê¸€ì€ ASCII ì½”ë“œ ìƒì— ì†í•˜ì§€ ì•Šì£ .
íŠ¹íˆ CJK(Chinese, Japanese, Korean)ì— ì†í•˜ëŠ” ìš°ë¦¬ë‚˜ë¼ ê¸€ìëŠ” ìœ ë‹ˆì½”ë“œ ì¤‘ì—ì„œë„ êµ‰ì¥íˆ ë‹¤ë£¨ê¸° í˜ë“  í¸ì…ë‹ˆë‹¤.
ë‹¤í–‰íˆ `tf.strings`ì˜ ì—°ì‚°ë“¤ì€ ìœ ë‹ˆì½”ë“œë¥¼ ì˜ ë‹¤ë£° ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ëŠ”ë°ìš”, ëŒ€í‘œì ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì€ ê²½ìš°ë¥¼ ê¼½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë¨¼ì € ìœ ë‹ˆì½”ë“œ ë¬¸ìëŠ” í•œ ë¬¸ìì˜ ê¸¸ì´ê°€ ê°ì ë‹¬ë¼ì„œ ì˜ëª»ëœ ê¸¸ì´ë¥¼ ì¬ê±°ë‚˜, substr ì—°ì‚°ì´ ì ì ˆíˆ ë™ì‘í•˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
ê·¸ë¥¼ ìœ„í•´ `tf.strings` ë‚´ë¶€ì˜ ì—°ì‚°ì„ ì•„ë˜ì²˜ëŸ¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
>>> string_tensor = tf.constant(["ì•ˆë…•í•˜ì„¸ìš”", "TensorFlow Everywhere Korea!", "ğŸ˜ŠğŸ‘‹ğŸ¤—ğŸ¥•"])
>>>
>>> tf.strings.length(string_tensor)
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([15, 28, 16], dtype=int32)>
>>> tf.strings.length(string_tensor, unit='UTF8_CHAR')
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 5, 28,  4], dtype=int32)>
>>>
>>> tf.strings.substr(string_tensor, pos=0, len=1)
<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'\xec', b'T', b'\xf0'], dtype=object)>
>>> tf.strings.substr(string_tensor, pos=0, len=1, unit="UTF8_CHAR")
<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'\xec\x95\x88', b'T', b'\xf0\x9f\x98\x8a'], dtype=object)>
>>> [s.decode("UTF8") for s in tf.strings.substr(string_tensor, pos=0, len=1, unit="UTF8_CHAR").numpy()]
['ì•ˆ', 'T', 'ğŸ˜Š']
>>>
>>> tf.strings.unicode_split(string_tensor, "UTF-8")
<tf.RaggedTensor [[b'\xec\x95\x88', b'\xeb\x85\x95', b'\xed\x95\x98', b'\xec\x84\xb8', b'\xec\x9a\x94'], [b'T', b'e', b'n', b's', b'o', b'r', b'F', b'l', b'o', b'w', b' ', b'E', b'v', b'e', b'r', b'y', b'w', b'h', b'e', b'r', b'e', b' ', b'K', b'o', b'r', b'e', b'a', b'!'], [b'\xf0\x9f\x98\x8a', b'\xf0\x9f\x91\x8b', b'\xf0\x9f\xa4\x97', b'\xf0\x9f\xa5\x95']]>
>>> [[s.numpy().decode("UTF8") for s in v] for v in tf.strings.unicode_split(string_tensor, "UTF-8")]
[['ì•ˆ', 'ë…•', 'í•˜', 'ì„¸', 'ìš”'], ['T', 'e', 'n', 's', 'o', 'r', 'F', 'l', 'o', 'w', ' ', 'E', 'v', 'e', 'r', 'y', 'w', 'h', 'e', 'r', 'e', ' ', 'K', 'o', 'r', 'e', 'a', '!'], ['ğŸ˜Š', 'ğŸ‘‹', 'ğŸ¤—', 'ğŸ¥•']]
```

ë˜í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ì‚¬ìš©ì ì…ë ¥ì„ ì§ì ‘ì ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤ë©´, normalizationì˜ ë‹ˆì¦ˆê°€ ë§ì´ ìˆê² ì£ .
Unicode Normalization ì—°ì‚°ì€ TensorFlow Textì— ì¡´ì¬í•©ë‹ˆë‹¤.
í•œê¸€ì— ì ìš©í•˜ëŠ” ì ì ˆí•œ ì˜ˆì‹œë¥¼ ìƒê°í•´ë‚´ì§€ ëª»í•´ ì´ ì˜ˆì‹œëŠ” ë‹¤ë¥¸ ì–¸ì–´ë¡œ í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
>>> text.normalize_utf8(['Ã„ffin'])
<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'\xc3\x84ffin'], dtype=object)>
```

### 3.2. í…ìŠ¤íŠ¸ ì²˜ë¦¬ì—ì„œì˜ `tf.strings`

ë³¸ê²©ì ìœ¼ë¡œ ëª¨ë“  stringì„ TensorFlow Graphë¡œ ì²˜ë¦¬í•œë‹¤ë©´ `tf.strings`ì™€ TensorFlow Textì˜ ë§ì€ ê¸°ëŠ¥ì„ í™œìš©í•˜ê²Œ ë í…ë°ìš”, ì œê°€ ì†Œê°œë“œë¦¬ê³  ì‹¶ì€ ê¸°ëŠ¥ì€ ì•„ë˜ ì •ë„ì…ë‹ˆë‹¤.

* `tf.strings.split`
* `tf.strings.to_number`
* `tf.strings.strip`
* `tf.strings.regex_replace`

ë°ì´í„°ì…‹ì´ ë©€í‹°ë¼ì¸ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” tsvì´ê±°ë‚˜, ì§€ì •ëœ í¬ë§·ì„ ê°€ì§€ëŠ” íŒŒì¼ì¼ ê²½ìš° ìœ„ì™€ ê°™ì€ ì—°ì‚°ë“¤ë§Œìœ¼ë¡œ ë§ì€ ê²½ìš°ë¥¼ ì»¤ë²„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´ NSMCì˜ ëª‡í–‰ì„ ê°€ì ¸ì™€ì„œ íŒŒì‹±ì„ ì§„í–‰í•´ë³´ì£ .

```python
>>> tsv_rows = tf.constant([
... "6270596\têµ³ ã…‹\t1",
... "9274899\tGDNTOPCLASSINTHECLUB\t0",
... "8544678\të­ì•¼ ì´ í‰ì ë“¤ì€.... ë‚˜ì˜ì§„ ì•Šì§€ë§Œ 10ì  ì§œë¦¬ëŠ” ë”ë”ìš± ì•„ë‹ˆì–ì•„\t0",
... ])
>>> splits = tf.strings.split(tsv_rows, sep='\t', maxsplit=2).to_tensor()
>>> string_inputs = tf.strings.strip(splits[:,1])
>>> string_inputs
<tf.Tensor: shape=(3,), dtype=string, numpy=
array([b'\xea\xb5\xb3 \xe3\x85\x8b', b'GDNTOPCLASSINTHECLUB',
       b'\xeb\xad\x90\xec\x95\xbc \xec\x9d\xb4 \xed\x8f\x89\xec\xa0\x90\xeb\x93\xa4\xec\x9d\x80.... \xeb\x82\x98\xec\x81\x98\xec\xa7\x84 \xec\x95\x8a\xec\xa7\x80\xeb\xa7\x8c 10\xec\xa0\x90 \xec\xa7\x9c\xeb\xa6\xac\xeb\x8a\x94 \xeb\x8d\x94\xeb\x8d\x94\xec\x9a\xb1 \xec\x95\x84\xeb\x8b\x88\xec\x9e\x96\xec\x95\x84'],
      dtype=object)>
>>> labels = tf.strings.to_number(splits[:,2], out_type=tf.int32)
>>> labels
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 0, 0], dtype=int32)>
```

ìœ„ì²˜ëŸ¼ `tf.strings`ì— ì¡´ì¬í•˜ëŠ” ëª‡ê°€ì§€ ì—°ì‚°ì„ ì¡°í•©í•´ ë§¤ìš° ì‰½ê²Œ ê°„ë‹¨í•œ í¬ë§·ì˜ ë°ì´í„°ì…‹ì„ íŒŒì‹±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë¬¼ë¡  Graph ëª¨ë“œì—ë„ ì ì ˆí•œ ì—°ì‚°ì´ë©´ì„œ, eager executionë„ ì˜ ì§€ì›í•©ë‹ˆë‹¤.

ë˜í•œ `tf.strings.regex_replace`ë¥¼ ì‚¬ìš©í•˜ë©´ ê°„ë‹¨í•œ ì „ì²˜ë¦¬ë„ ì•„ë˜ì²˜ëŸ¼ ì²˜ë¦¬ê°€ëŠ¥í•©ë‹ˆë‹¤.

```python
>>> string_tensor = tf.constant(["ì•ˆë…•í•˜ì„¸ìš” ã…ã…ã…ã…ã…", "ì•ˆë…•í•˜ì„¸ìš”!!!!!!"])
>>> string_tensor = tf.strings.regex_replace(string_tensor, "ã…{2,}", "ã…ã…")
>>> string_tensor = tf.strings.regex_replace(string_tensor, "!{2,}", "!!")
>>> [s.numpy().decode("UTF8") for s in string_tensor]
['ì•ˆë…•í•˜ì„¸ìš” ã…ã…', 'ì•ˆë…•í•˜ì„¸ìš”!!']
```

ìœ„ ì˜ˆì‹œëŠ” `ã…`ì´ë‚˜ `!`ì™€ ê°™ì€ ë°˜ë³µë  ë•Œ í° ì˜ë¯¸ë³€í™”ê°€ ì—†ëŠ” í† í°ì„ ì¤„ì—¬ì£¼ëŠ” ê³¼ì •ì„ `tf.strings.regex_replace`ë¡œ ì‘ì„±í•´ë³¸ ì˜ˆì‹œì…ë‹ˆë‹¤.
regexë¥¼ í™œìš©í•´ì„œ ë§ì€ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— íŠ¹ë³„í•œ ê²½ìš°ê°€ ì•„ë‹Œ ì´ìƒ ì „ì²˜ë¦¬ê¹Œì§€ TensorFlow Graph ëª¨ë“œì—ì„œ ìˆ˜í–‰ê°€ëŠ¥í•©ë‹ˆë‹¤.

### 3.3. tensorflow-textì˜ Tokenizer

ì´ì œ ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ì—°ì‚°ì„ ë²—ì–´ë‚˜ì„œ TensorFlow Textì˜ í•µì‹¬ê¸°ëŠ¥ì¸ Tokenizerë¥¼ ì†Œê°œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

í˜„ì¬ TensorFlow Textì—ëŠ” ì•„ë˜ ì •ë„ì˜ Tokenizerê°€ ì¡´ì¬í•˜ëŠ”ë°ìš”, ë‹¤ì„¯ê°€ì§€ ë‹¤ ì¶©ë¶„íˆ ê°€ì¹˜ìˆëŠ” Tokenizerë¼ ìƒê°í•˜ê¸° ë•Œë¬¸ì— í•˜ë‚˜í•˜ë‚˜ì”© ì†Œê°œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

* `text.WhitespaceTokenizer`
* `text.UnicodeScriptTokenizer`
* `text.SentencepieceTokenizer`
* `text.WordpieceTokenizer`
* `text.BertTokenizer`
* ...

ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” `text.WhitespaceTokenizer`ì€ ê°€ì¥ ê°„ë‹¨í•œ í˜•íƒœì˜ Tokenizerì…ë‹ˆë‹¤.
Whitespace ë‹¨ìœ„ë¡œ í† í°ì„ ë‚˜ëˆ„ì–´ì£¼ëŠ” Tokenizerë¼, í…ìŠ¤íŠ¸ê°€ ì „ì²˜ë¦¬ë˜ì§€ ì•ŠëŠ”ë‹¤ë©´ Unknown Tokenì´ êµ‰ì¥íˆ ë§ì´ ìƒê¹ë‹ˆë‹¤.
ê·¸ë˜ë„ ì „ì²˜ë¦¬ë¥¼ ì˜ í•œë‹¤ë©´ ì¶©ë¶„íˆ ì˜ë¯¸ê°€ ìˆê±°ë‚˜, ë‹¤ë¥¸ ì²˜ë¦¬(ì˜ˆë¥¼ ë“¤ì–´ wordpiece tokenizer)ë¥¼ í•˜ê¸° ì „ì— ë‹¨ìˆœí•˜ê²Œ ë¯¸ë¦¬ ë‚˜ëˆ„ì–´ì£¼ëŠ” ìš©ë„ë¼ë©´ ì¶©ë¶„íˆ ì¢‹ì€ Tokenizerê°€ ë˜ê² ì£ .

```python
>>> tokenizer = text.WhitespaceTokenizer()
>>> tokens = tokenizer.tokenize("ì•ˆë…•í•˜ì„¸ìš”! TensorFlow Everywhere!")
>>> [s.decode("UTF8") for s in tokens.numpy()]
['ì•ˆë…•í•˜ì„¸ìš”!', 'TensorFlow', 'Everywhere!']
```

`text.UnicodeScriptTokenizer`ëŠ” ì˜ì–´ê¶Œ ì–¸ì–´ë¥¼ ì²˜ë¦¬í•  ë•Œ ìœ ìš©í•˜ë‹¤ê³  ìƒê°í•˜ëŠ” Tokenizerì…ë‹ˆë‹¤.
Unicodeì˜ Script codeë¥¼ ê²½ê³„ë¡œ ë‚˜ëˆ„ëŠ” Tokenizerì…ë‹ˆë‹¤.
tensorflow text ë¬¸ì„œì—ì„œëŠ” <https://unicode-org.github.io/icu-docs/>ì—ì„œ ë” ìì„¸í•˜ê²Œ ë³¼ ìˆ˜ ìˆë‹¤ê³  ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤.
ì•„ë˜ì—ì„œëŠ” ëŠë‚Œí‘œì™€ í•œêµ­ì–´, ì˜ì–´ëŠ” ì„œë¡œ ë‹¤ë¥¸ unicode script codeì— í•´ë‹¹í•˜ê¸° ë•Œë¬¸ì— ì•„ë˜ì²˜ëŸ¼ ë‚˜ë‰˜ì–´ì§‘ë‹ˆë‹¤.

```python
>>> tokenizer = text.UnicodeScriptTokenizer()
>>> tokens = tokenizer.tokenize("ì•ˆë…•í•˜ì„¸ìš”! TensorFlow Everywhere!")
>>> [s.decode("UTF8") for s in tokens.numpy()]
['ì•ˆë…•í•˜ì„¸ìš”', '!', 'TensorFlow', 'Everywhere', '!']
```

`text.SentencepieceTokenizer`ëŠ” ë³´ì‹œëŠ” ê·¸ëŒ€ë¡œ [google/sentencepiece](https://github.com/google/sentencepiece) ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ TensorFlow Graphì•ˆì—ì„œ ì“¸ ìˆ˜ ìˆê²Œ ë§Œë“  ê²ƒì…ë‹ˆë‹¤.
sentencepiece ë ˆí¬ì§€í† ë¦¬ì—ì„œë„ ì„¤ëª…í•˜ë“¯ ì•„ë˜ì²˜ëŸ¼ ì‚¬ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
ì§€ê¸ˆê¹Œì§€ ë‚˜ì˜¨ Tokenizerì™€ ë‹¤ë¥´ê²Œ `tf.int32`ì™€ `tf.string` ë‘˜ ë‹¤ ì§€ì›í•©ë‹ˆë‹¤.
sentencepieceëŠ” í•™ìŠµì‹œì— vocab tableì„ ë§Œë“œëŠ”ë§Œí¼, ë³„ë„ë¡œ hash tableì„ ê´€ë¦¬í•  í•„ìš”ë„ ì—†ìŠµë‹ˆë‹¤.
Sentencepiece TokenizerëŠ” ì•„ë˜ì™€ ê°™ì´ ì“¸ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
>>> tokenizer = text.SentencepieceTokenizer(model=open('spm_model.model', 'rb').read())
>>> tokenizer.tokenize(['hello world']) # output type = tf.int32
...
>>> tokenizer = text.SentencepieceTokenizer(model=open('spm_model.model', 'rb').read(), out_type=tf.string)
>>> tokenizer.tokenize(['hello world']) # output type = tf.string
...
```

`text.WordpieceTokenizer`ì™€ `text.BertTokenizer`ëŠ” ê°™ì´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
NLP ë¶„ì•¼ì—ì„œëŠ” Huggingfaceì˜ tokenizerê°€ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ í•´ë³´ê³  ì‹¶ì–´ë„ ì‹œê°„ ë¬¸ì œë•Œë¬¸ì— êº¼ë ¤ì§€ë˜ WordPiece modelì„ tensorflow-textë¥¼ í†µí•´ì„œë„ ë“œë””ì–´ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.
ì—¬ê¸°ì„œ `text.WordpieceTokenizer`ëŠ” ì •ë§ Wordë¥¼ Wordpieceë¡œ ë‚˜ëˆ„ì–´ì£¼ëŠ” tokenizerì´ê¸° ë•Œë¬¸ì— WhitespaceTokenizerì™€ WordpieceTokenizerë¥¼ í†µí•©í•œ `text.BertTokenizer`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ í¸í•©ë‹ˆë‹¤.
ì¦‰, WordpieceTokenizerê°€ `ì•ˆë…•í•˜ì„¸ìš”`ë¥¼ `ì•ˆë…•, ##í•˜ì„¸ìš”`ë¡œ ë°”ê¾¸ì–´ ì¤€ë‹¤ë©´, BertTokenzierëŠ” `ì—¬ëŸ¬ë¶„ ì•ˆë…•í•˜ì„¸ìš”`ë¥¼ `ì—¬ëŸ¬, ##ë¶„, ì•ˆë…•, ##í•˜ì„¸ìš”`ë¡œ ë°”ê¾¸ì–´ì£¼ëŠ” ì‹ì´ì£ .

í˜„ì¬ stable releaseì—ì„œ TokenizerëŠ” ì‚¬ìš©ê°€ëŠ¥í•˜ê³ , vocabì„ ë§Œë“œëŠ” ê¸°ëŠ¥ì€ ì‚¬ìš©í•˜ì§€ ëª»í•˜ì§€ë§Œ, nightly ë²„ì „(2.5.x)ì„ ì„¤ì¹˜í•œë‹¤ë©´ ëª¨ë“  ë²„ì „ì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ë°ìš”, ì•„ë˜ì²˜ëŸ¼ ì‚¬ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

```python
string_tensor_dataset = (
    tf.data ....
    .map( ...
) # string tensorë§Œì„ ë°˜í™˜í•˜ëŠ” dataset

from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab

bert_vocab = bert_vocab.bert_vocab_from_dataset(
    string_tensor_dataset.batch(1000),
    vocab_size=8000,
    reserved_tokens=["<pad>", "<unk>", "<s>", "</s>"],
)

with tf.io.gfile.GFile("vocab.txt", "w") as out_file:
    for token in bert_vocab:
        print(token, file=out_file)
```

í•˜ì§€ë§Œ, ì§ì ‘ ëŒë ¤ë³¸ ê²°ê³¼ ì†ë„ë©´ì—ì„œ í¬ê²Œ ë§Œì¡±ìŠ¤ëŸ½ì§„ ì•Šì€ë°ìš”, nightly ë²„ì „ì´ê¸°ë„ í•˜ê³ , verbosity ì˜µì…˜ë„ ì¡´ì¬í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì‹¤ì œë¡œ ì‚¬ìš©í•˜ê¸°ì—ëŠ” ë¬´ë¦¬ê°€ ìˆì–´ ë³´ì´ì§€ë§Œ, stable releaseê°€ ëœë‹¤ë©´ ê¸°ëŒ€í•´ë³¼ë§Œ í•©ë‹ˆë‹¤.

## 4. NSMCë¡œ TensorFlow Textì™€ RaggedTensor ë§›ë³´ê¸°

ê·¸ëŸ¼ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œë¥¼ ë³´ì—¬ë“œë¦¬ê¸° ìœ„í•´ ì œëª©ì²˜ëŸ¼ tensorflow-textì™€ RaggedTensorì˜ ê¸°ëŠ¥ì„ í™œìš©í•´ ì •ë§ ì§§ì€ ì½”ë“œë¡œ NSMC ë¶„ë¥˜ê¸°ë¥¼ ì‘ì„±í•´ë³´ë ¤ í•©ë‹ˆë‹¤.
NSMCëŠ” Naver Sentiment Movie Corpusì˜ ì•½ì–´ë¡œ ê°ì • ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ì…‹ì´ê³  test, trainì…‹ì„ í•©ì³ 20ë§Œ ë¬¸ì¥ ì •ë„ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.

### 4.1. ë°ì´í„° ì²˜ë¦¬

ë§ì€ ë°ì´í„° ì²˜ë¦¬ê°€ ë“¤ì–´ê°€ë©´ ì¢‹ê² ì§€ë§Œ, ìµœëŒ€í•œ ê°„ë‹¨í•œ ë²„ì „ìœ¼ë¡œ ì‹¤í–‰í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
ì „ì²˜ë¦¬ ì—†ì´ sentencepiece tokenizeë§Œ ì‹¤í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.

ê·¸ ì „ì— ì•½ê°„ì˜ íŒì„ ë“œë¦¬ìë©´ í•œêµ­ì–´ì—ì„œ sentencepieceë‚˜, wordpiece tokenizerë§Œìœ¼ë¡œë„ ìì†Œ ë‹¨ìœ„ì˜ tokenizingì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œë ¤ë“œë¦¬ê³  ì‹¶ì–´ìš”.
ê·¸ë˜ì„œ í•´ë‹¹ ê¸°ëŠ¥ê¹Œì§€ í™œìš©í•´ì„œ ì˜ˆì‹œ ì½”ë“œë¥¼ ë³´ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

ì•„ë˜ëŠ” ê°„ë‹¨í•œ sentencepiece tokenizer í•™ìŠµì½”ë“œì…ë‹ˆë‹¤.
ëŒ€ì‹  normalization ruleì„ ì—†ì• ê³ , ì»¤ìŠ¤í…€í•˜ê²Œ Unicode Normalizationì„ NFD í˜•íƒœë¡œ, ì¦‰ í•œêµ­ì–´ì˜ ì´ˆì„±, ì¤‘ì„±, ì¢…ì„±ì„ ë‚˜ëˆ„ë„ë¡ normalizeí•´ì£¼ì—ˆìŠµë‹ˆë‹¤.

```python
import io
import unicodedata

import sentencepiece as spm
import tensorflow as tf

def _get_nsmc_nfd():
    with open("nsmc/ratings.txt") as f:
        for line in f:
            yield unicodedata.normalize("NFD", line.split("\t")[1])

spm.SentencePieceTrainer.train(
    sentence_iterator=_get_nsmc_nfd(),
    model_prefix="spm",
    vocab_size=5000,
    normalization_rule_name="identity",
    pad_id=0,
    bos_id=1,
    eos_id=2,
    unk_id=3,
)
```

ìœ„ì²˜ëŸ¼ sentencepiece í•™ìŠµì„ ì§„í–‰í•  ê²½ìš° ì•„ë˜ì™€ ê°™ì€ tokenization ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
í•œêµ­ì–´ëŠ” ë™ì‚¬ì˜ ì‹œì œê°€ ë³€í•  ë•Œ `-ã……` ë°›ì¹¨ì´ ë¶™ëŠ” ì‹ìœ¼ë¡œ ë§ì´ ë³€í•´ì„œ ì´ì™€ ê°™ì€ ë°©ë²•ì´ ì˜ë¯¸ê°€ ìˆì„ ë•Œê°€ ë§ìŠµë‹ˆë‹¤.

```text
ì¬ë°‹ëŠ”ë… -> ['<s>', 'â–ì¬ë¯¸', 'á†ºëŠ”ë°', 'á†¼', '</s>']
ì• í‹‹í•œ ì˜í™”ë„¤ìš” -> ['<s>', 'â–', 'ì• í‹‹í•˜', 'á†«', 'â–ì˜í™”ë„¤ìš”', '</s>']
```

### 4.2. ëª¨ë¸ í•™ìŠµ

ì´ì œ tokenizerê°€ ì¤€ë¹„ë˜ì—ˆë‹¤ë©´, ì‹¤ì œë¡œ ëª¨ë¸í•™ìŠµì„ í•´ë³¼ë•Œì¸ë°ìš”, ì œì¼ ê°„ë‹¨í•œ í˜•íƒœë¡œ ë¹ ë¥´ê²Œ êµ¬ì„±í•´ë³´ê² ìŠµë‹ˆë‹¤.

ë¨¼ì € ëª¨ë¸ ì½”ë“œë¥¼ ì¤€ë¹„í•´ì¤ë‹ˆë‹¤. ìœ„ì—ì„œ sentencepiece vocab sizeë¥¼ 5000ìœ¼ë¡œ ì¡ì•˜ê¸° ë•Œë¬¸ì— 5000ì˜ input sizeë¥¼ ê°€ì§€ëŠ” Embedding ë ˆì´ì–´,
ê·¸ë¦¬ê³  ì—°ì†ì ì¸ ë°ì´í„°ë¥¼ ì˜ ëª¨ë¸ë§í•˜ê¸° ìœ„í•´ LSTM, ê·¸ë¦¬ê³  LSTMì˜ ì¶œë ¥ê°’ì„ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ Dense Layerë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ìŒ“ì•˜ìŠµë‹ˆë‹¤.
ë©”íŠ¸ë¦­, Loss, OptimizerëŠ” ê°€ì¥ ê°„ë‹¨í•œ í˜•íƒœë¡œ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤.

```python
model = tf.keras.Sequential(
    [
        tf.keras.layers.Input(shape=[None], dtype=tf.int32, ragged=True),
        tf.keras.layers.Embedding(5000, 256),
        tf.keras.layers.LSTM(256),
        tf.keras.layers.Dense(256, activation="relu"),
        tf.keras.layers.Dense(2, activation="softmax"),
    ]
)
model.summary()
model.compile(optimizer="rmsprop", loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics="acc")
```

ì´ì œ ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ì¤ë‹ˆë‹¤.
ë¨¼ì € tokenizerì™€ string tensorê°€ ë“¤ì–´ì™”ì„ ë•Œ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ë³€í™˜í•´ì¤„ í•¨ìˆ˜ë¶€í„° ì¤€ë¹„í•©ë‹ˆë‹¤.
ë°©ê¸ˆ ì „ sentencepieceë¥¼ `NFD`ë¡œ unicode normalizeí•œ í…ìŠ¤íŠ¸ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ í•™ìŠµì‹œì¼°ê¸° ë–„ë¬¸ì— ì…ë ¥ í…ì„œ xì— ëŒ€í•´ `NFD`ë¡œ normalize í›„ tokenize í•´ì¤ë‹ˆë‹¤.

```python
with open("./spm.model", "rb") as spm_model:
    tokenizer = text.SentencepieceTokenizer(spm_model.read(), add_bos=True, add_eos=True)

def make_model_input(x: tf.Tensor) -> tf.Tensor:
    x = text.normalize_utf8(x, "NFD")
    return tokenizer.tokenize(x)
```

ê·¸ í›„ ì¤€ë¹„ëœ ëª¨ë¸ ì…ë ¥ ë³€í™˜ í•¨ìˆ˜ì™€ `tf.data` ëª¨ë“ˆì„ í™œìš©í•˜ì—¬ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•´ì¤ë‹ˆë‹¤.
TextLine ë°ì´í„°ë¡œ trainì™€ test ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¨ ë’¤ ì ì ˆí•˜ê²Œ íŒŒì‹±í•œ í›„ trainì—ì„œ 100ê°œ ì •ë„ì˜ batchë¥¼ ê°€ì ¸ì™€ devì…‹ë„ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.

```python
def parse_batch_tsv_rows(x: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:
    splits = tf.strings.split(x, sep="\t").to_tensor(shape=[tf.size(x), 3])
    model_inputs = make_model_input(splits[:, 1])
    labels = tf.strings.to_number(splits[:, 2])
    return model_inputs, labels

train_data = (
    tf.data.TextLineDataset("nsmc/ratings_train.txt")
    .skip(1)
    .shuffle(10000, reshuffle_each_iteration=True)
    .batch(64)
    .map(parse_batch_tsv_rows)
)
dev_data = train_data.take(100)
train_data = train_data.skip(100)

test_data = tf.data.TextLineDataset("nsmc/ratings_test.txt").skip(1).batch(256).map(parse_batch_tsv_rows)
```

ì´ì œ ëª¨ë¸ê³¼ ë°ì´í„°ê°€ ì „ë¶€ ì¤€ë¹„ë˜ì—ˆìœ¼ë‹ˆ ë‚¨ì€ ê²ƒì€ í•™ìŠµê³¼ í‰ê°€ë¿ì´ê² ì£ ?
train_data, dev_data, test_dataë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµê³¼ í‰ê°€ë¥¼ ì§„í–‰í•´ì¤ë‹ˆë‹¤.

```python
model.fit(train_data, validation_data=dev_data, epochs=3)
model.evaluate(test_data)
```

ì •ìƒì ìœ¼ë¡œ í•™ìŠµì´ ëœë‹¤ë©´ ì•„ë˜ì •ë„ì˜ ë¡œê·¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

```text
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding (Embedding)        (None, None, 256)         1280000
_________________________________________________________________
lstm (LSTM)                  (None, 256)               525312
_________________________________________________________________
dense (Dense)                (None, 256)               65792
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 514
=================================================================
Total params: 1,871,618
Trainable params: 1,871,618
Non-trainable params: 0
_________________________________________________________________
...
...
2244/2244 [==============================] - 161s 71ms/step - loss: 0.4242 - acc: 0.8019 - val_loss: 0.3179 - val_acc: 0.8612
Epoch 2/3
2244/2244 [==============================] - 160s 71ms/step - loss: 0.3174 - acc: 0.8646 - val_loss: 0.3001 - val_acc: 0.8737
Epoch 3/3
2244/2244 [==============================] - 160s 71ms/step - loss: 0.2935 - acc: 0.8775 - val_loss: 0.3034 - val_acc: 0.8759
196/196 [==============================] - 5s 24ms/step - loss: 0.3290 - acc: 0.8625
```

ì¶”ê°€ì ìœ¼ë¡œ tensorflow-textë¥¼ ì‚¬ìš©í•˜ëŠ” ê²°ì •ì ì¸ ì´ìœ ì¸ preprocess ë¶€í„° model forward ê³¼ì •ê¹Œì§€ë¥¼ tracingí•˜ì—¬ saved modelë¡œ ì €ì¥í•´ë³´ê² ìŠµë‹ˆë‹¤.
ì´ë ‡ê²Œ ì €ì¥í•˜ê²Œ ëœë‹¤ë©´ tensorflow servingì„ í™œìš©í•´ì„œ ë°”ë¡œ ì„œë²„ë¡œ ë§Œë“¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
ì°¸ê³ ë¡œ modelì˜ ì†ì„±ìœ¼ë¡œ tokenizerë¥¼ í• ë‹¹í•´ì£¼ëŠ” ì´ìœ ëŠ” ì €ì¥ë  saved modelì˜ root ì˜¤ë¸Œì íŠ¸(model)ì— ëª¨ë“  ì˜¤ë¸Œì íŠ¸ê°€ í• ë‹¹ë˜ì–´ ìˆì–´ì•¼ í•´ì„œ ê·¸ë ‡ìŠµë‹ˆë‹¤.

```python
@tf.function(input_signature=tf.TensorSpec([None], dtype=tf.string))
def call(x: tf.Tensor) -> tf.Tensor:
    model_input = make_model_input(x)
    return model(model_input)

model.tokenizer = tokenizer
tf.saved_model.save(model, 'nsmc-model/0', call)
```

ì´ì œ ë‹¨ 50ì¤„ ì •ë„ì˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ NSMC ê°ì • ë¶„ë¥˜ ëª¨ë¸ì„ ë§Œë“¤ì—ˆëŠ”ë°ìš”, ìœ„ì—ì„œ ë³´ì‹œë‹¤ì‹¶ì´ ì•„ë˜ì™€ ê°™ì€ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.

* ê°„í¸í•œ ë°ì´í„° ì²˜ë¦¬ê°€ ê°€ëŠ¥í•´ì§€ê³ ,
* TensorFlow Graphì•ˆì— ì „ì²˜ë¦¬, Tokenizer ì½”ë“œë¥¼ í†µí•© ê°€ëŠ¥í•´ì§€ê³ ,
* ê·¸ì— ë”°ë¼ ì„œë¹™ê¹Œì§€ ìì—°ìŠ¤ë ˆ ì´ì–´ì§€ëŠ” ì½”ë“œê°€ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤.

---

## í›„ê¸°

ë°œí‘œí–ˆë˜ í–‰ì‚¬ ì¤‘ì— ì œì¼ í° ê·œëª¨ì¸ë°, ì¤€ë¹„í•˜ëŠ” ê²ƒê³¼ ë°œí‘œí•˜ëŠ” ê³¼ì •ë„ ì¬ë°Œì–´ì„œ ë‹¤í–‰ì´ì—ˆë‹¤.
Gather townì—ì„œ í–‰ì‚¬ë¥¼ ì§„í–‰í–ˆëŠ”ë°, ì˜¤ê±°ë‚˜ì´ì €ë¶„ë“¤ì´ ë§ì€ í–‰ì‚¬ ì´ë²¤íŠ¸(?)ë„ ì¤€ë¹„í•´ì£¼ì…”ì„œ ì¦ê±°ì› ë‹¤!
